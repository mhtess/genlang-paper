---
title             : "Communicating generalizations"
shorttitle        : "Communicating generalizations"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}

author_note: >
  Enter author note here.

abstract: > 
    Generalizations are central to human understanding yet can be difficult to acquire through direct experience (e.g., learning that a plant is poisonous).
    Language provides simple ways to convey generalizations (e.g., *Birds fly.*), and the language of generalizations is ubiquitous in everyday discourse and child-directed speech.
    Yet the meaning of these expressions is philosophically puzzling (e.g., not all birds fly) and has resisted precise formalization.
    The major issue in formalizing *genericity* is in determining the conditions under which such statements are true or false; these conditions are extremely flexible. 
    Using a probabilistic model of pragmatic reasoning, we explore the hypothesis that the meaning of such linguistic expressions is *simple but underspecified*, and that general communicative principles can be used to establish a more precise meaning in context. 
    To test this theory, we examine endorsements of generalizations about three different domains: generalizations about categories (*generic language*, such as *Birds fly*), events (*habitual language*, such as *John runs*), and causes (*causal language*, such as *The block makes the machine play music.*). Across these diverse domains, we find that our model explains the full spectra of endorsements of generalizations in language, while simpler models do not.
    These results suggest that central phenomena of genericity emerge from the interaction of diverse prior beliefs about properties with general communicative principles. 
  
keywords          : "genericity, generalizations, generics, pragmatics, semantics, bayesian modeling"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
<!-- \newcommand{\ndg}[1]{\textcolor{Green}{$[ndg: #1 ]$}} -->
<!-- \newcommand{\mht}[1]{\textcolor{Blue}{$[mht: #1 ]$}} -->
<!-- \newcommand{\red}[1]{\textcolor{Red}{$#1$}} -->

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = F, fig.path='figs/',
                      echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)

```


```{r libraries}
library(papaja)
library(formatR)
library(rwebppl)
library(xtable)
library(tidyverse)
library(forcats)
library(langcog)
# library(dplyr)
library(data.table)
library(coda)
library(ggthemes)
library(ggrepel)
library(jsonlite)
library(gridExtra)
library(lme4)
library(knitr)
library(cowplot)
theme_set(theme_few())
estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
logmeanexp <- function(x){
  x.num <- as.numeric(x)
  xstar = max(x.num)
  return(xstar + log(mean(exp(x.num - xstar))))
}

compute_r2 <- function(df,v1, v2, sigfigs = 3){
  return(round(cor(df[[v1]], df[[v2]])^2, sigfigs))
}

compute_mse <- function(df, v1, v2, sigfigs = 3){
  return(round(mean( (df[[v1]]-df[[v2]])^2), sigfigs))
}

project.path <- "../"
options("scipen"=10) 
```

# Introduction 

Figuring out that an object tends to have a property, an entity tends to exhibit a behavior, or a cause tends to produce an effect is crucial knowledge in an uncertain world.
Such *generalizations* can be acquired by drawing inductive inferences from observations [@Baldwin1993; @Xu2008; @Dewar2010].
Other cooperative agents can facilitate this, and learners can draw stronger inductive inferences in social learning contexts [@Csibra2009; @Tomasello1999; @Butler2012].
At around a child's first birthday, however, the mind gains access to a new, human-unique source of data: language.

<!-- give rise to the productivity of human thought and form of foundations of cultural knowledge. -->
<!-- They allow us to understand the complex world around us and dream up how possible future situations might unfold. -->
<!-- It's been suggested that were psychology to have *general laws* akin to Newtonian laws in physics, the first general law in psychology would be of generalizations [Shepard1987]. -->
<!-- Generalizations are central to human understanding.  -->
<!-- Even before birth, human and non-human animals alike gather statistical information and form generalizations to predict what will come next. -->
<!-- The human learner, perhaps uniquely in the animal kingdom, displays striking sensitivity to the intentional and pedagogical actions of others, leading to distinct and often stronger inferences from data than would otherwise be warranted from random observations. -->
Learning from language is important because observations can be costly (e.g., a plant being poisonous) or statistically unlikely (e.g., lightning).
Minimally, language fosters generalizations by bringing attention to objects being of the same kind through the use of labeling [e.g., "This is a dog"; @Gelman1986; @Gopnik2000; @Graham2004; @Markman1989; @Schulz2008].
Objects have many properties though; when speakers attach kind-labels to properties (e.g., *Dogs bark*), listeners hear a *generalization*.
The above example is a generalization about a category (*generic language*); individual entities can also be talked about in generalization (*habitual language*; e.g., "Mary swims after work.") in addition to causal events (*causal language*; e.g., "Drinking moonshine makes you go blind."), to name just a few.
What these statements have in common is that they convey generalizations [also referred to as generic meaning, or *genericity*; @Carlson1977; @Carlson1995].

<!-- and about people in general [e.g., "You never know what will happen on a blind date."; @Orvell2017]. -->

It is believed that every language can express generic meaning [@Behrens2005; @Carlson1995].
Generic language is ubiquitous in everyday conversation as well as in child-directed speech [@Gelman2008], and around the age of two and a half, children begin to understand that *generic language* refers to kinds and conveys generalizations that extend beyond the here and now [@Cimpian2008].
Because of its powerful implications, generalizations in language are thought to be central to the growth of conceptual knowledge [@Gelman2004].
<!-- *Causal language* (e.g., "The block *makes* it [the machine] go.") faciliates more abstract forms of causal reasoning [@Bonawitz2010]. -->
Additionally, generics are the primary way by which speakers discuss social categories, making them key to propagating stereotypes [@Gelman2004; @Rhodes2012; @Leslie2015] and impacting motivation [@Cimpian2010].

<!-- It's plausible that cultural knowledge accumulates across generations through language's ability to talk about generalizations [@Henrich2015]. -->

<!-- Generic statements convey generalizations about categories, but there are other linguistic devices that look different but seem to similary convey generalization. -->
<!-- *Genericity* can be observed when talking about categories (e.g., *Dogs bark*), people in general , specific people (e.g., *Martha swims after work*), or causal events (e.g., *Drinking moonshines makes you go blind*). -->
<!-- Though this diverse array of utterances all seem to convey something beyond the here and now, what exactly is common to each of them is not obvious.  -->
<!-- Without a formal way of describing generalizations in language, we do not yet know in what ways these large swaths of language are similar.  -->

Probabilistic models are used to describe language understanding in precise, quantitative terms [@Frank2012; @Goodman2013; @Franke2015; @Goodman2016], formalizing complex linguistic phenomena ranging from politeness [@Yoon2016] and hyperbole [@Kao2014] to metaphor [@Kao2014] and vagueness [@Lassiter2013; @Qing2014].
Such models of rational communication describe inferences listeners can draw from language; heretofore, however, this language has been limited to utterances that convey information about a specific entity or situation.
Meanwhile, the language of generalizations --- statements that extend beyond the here-and-now --- has eluded precise formalization.
<!-- The language of generalization is exceedingly difficult to formalize, however. -->
<!-- The cognitive act of generalization is naturally formalized in Bayesian cognitive models [@Tenenbaum2011]. -->
<!-- Bayesian models showcase the role of flexibly structured representations [@Kemp2007; @Kemp2008] and assumptions learners can make about the data-generating process [e.g., learning from pedagogical actions; @Shafto2012]. -->
<!-- Learning from language, however, is more challenging to formalize because of the inherent context-sensitivity of language [@Grice1975; @Clark1996; @Levinson2000]. -->


The major issue in formalizing genericity is determining what makes such statements true or false. 
When we hear "Dogs bark", it feels like a universal statement, like *All* dogs bark.
Yet, Basenji dogs are barkless^[
http://strangesounds.org/2014/09/basenji-dogs-basenjis-dont-bark-howl.html
]; the generalization instead should signal something weaker like *Most* dogs bark.
Indeed, many generalizations seem to have the quantificational force of *most*. 
But *most mosquitos* do not carry malaria, while the statement "Mosquitos carry malaria" is intuitively true.
The only quantifier that would permit such a statement would be "Some" (e.g., *some mosquitos carry malaria*).
But equating the generalization with *some* is too lenient: Some robins are female, but saying *Robins are female* seems odd.
Exceptions could be made for properties that only apply to one gender (like being female), but *Robins lay eggs* is intuitively true despite the property also being only present in females.

In this paper, we combine a probabilistic model of pragmatic reasoning with ideas from formal semantics to articulate a formal theory of the truth conditions of generalizations in language. 
By formalizing this semantic hypothesis in a pragmatic language understanding model, we derive predictions for the endorsements of such statements which we show to depend in systematic ways on relevant prior knowledge.
Across three distinct case studies, we both measure and manipulate prior knowledge, showing that it is strongly and causally related to the endorsement of generalizations in the ways predicted by our model.
<!-- We relate known measurements on factors regarding generic language to each other via our model. -->
<!-- We then go on to manipulate the relevant factors, showing they are causally related via our model to generic endorsement and interpretation.  -->
We do this in three domains: generalizations about categories, events, and causes.
We show that in each domain, our general theoretical framework applies and predicts endorsements about generalizations conveyed in language, when simpler models would fall short. 

The rest of this paper is organized as follows. 
In Section 1, we describe the Bayesian framework for generalization and define the scales along which generalization is measured in our three domains. 
In Section 2, we formalize the hypothesis that the core of meaning of a generalization is *simple but underspecified* and incorporate this *uncertain semantics* into a general framework for pragmatic language understanding -- the Rational Speech Act framework.

To test our model, we examine endorsements of generalizations in three domains: categories, events, and causes.
Our model makes predictions about how endorsements should vary according to (i) the listener's world knowledge prior and (ii) the speaker's intended message (or, belief distribution) that she seeks to communicate. 
In Section 3, we describe experiments the measure both quantities and relate them to the endorsements of generalizations about categories, or *generic language*. 
In Section 4, we describe experiments that manipulate the speaker's intended message in communicating generalizations about events, or *habitual language*.
In Section 5, we describe experiments that manipulate both the listener's relevant prior knowledge and the speaker's intended message in communicating generalizations about causes, or *causal language*. 
Across all of our experimental settings, we find a very strong agreement of our model's predictions to the full spectrum of human elicited endorsement (cumulative $r^2(N) = 0.XX$), where simpler models fall short.
In the final section of the paper, we discuss how our model begins the process of unifying the extant psychological, linguistic, and philosophical literatures on *genericity*.
Our theory also opens up a number of new directions for understanding generalizations in language and building models that learn from language like people.

<!-- ALTERNATIVE INTRO -->

<!-- Yet, generalizations can be hard to acquire. -->
<!-- Consider what it would take to learn that under certain conditions (e.g., when eaten raw) a plant is poisonous, while under other conditions (e.g., when processed appropriately) the plant is nutritious.  -->
<!-- This is a common ecological problem and historical examples suggest that humans are not particularly good at learning such generalizations [e.g., the Australian explorers case; @Henrich2015]. -->
<!-- In this domain, the relationship to be learned (i.e., between different ways of food processing and the poisonousness of the plant) is non-obvious and trial-and-error learning is costly (negative examples entail death). -->

<!-- Crucially, once acquired, such hard-won generalizations can be communicated among humans, something which other animals struggle to accomplish [cite tomasello apes something]. -->
<!-- Generalizable knowledge can be inferred through demonstration, or in pedagogical contexts when ostensive cues are present [@Csibra].  -->
<!-- By and large, however, language provides the most flexible and generative way to communicate generalizations.  -->

<!-- A generalization provides a relation between a category and a property: It tells us to predict that property of future instances of the category [@HumeTHN]. -->
<!-- Generalization can be described as a probability and inductive inference as a probabilistic model [@Tenenbaum2011]. -->
<!-- From a very young age, children make strong inductive inferences from their experience in a way consistent with the Bayesian probability calculus [@Dewar2010; @Gelman1986; @Markman1989]. -->

<!-- If generalizations can be described by predictive probabilities, it would stand to reason that the most direct way to communicate a generalization would be to convey probability.  -->
<!-- Ironically, humans are very bad at describing and interpreting probabilities explicitly [@Tversky1974]. -->
<!-- Rather than be a deficiency, however, the difficulty we have in understanding numerical descriptions of probability might reflect the flexible adaptivity of natural language to make vague or ambiguous that which shared context can be relied upon to guide interpretation [@Piantadosi2012]. -->
<!-- If language has evolved in a way to make efficient the kinds of inductive inferences that are central to human cognition, we would expect the language of generalizations to be simple [c.f., @Leslie2008]. -->

<!-- Indeed, generalizations in language (e.g., *Dogs are friendly*, *Mary smokes cigarettes*, *Yeast makes bread rise*) are among the simplest forms of compositional expressions.  -->
<!-- Generalizations do not come in just one syntactic form: "*Dogs* are friendly", "*A library book* should be treated with respect", "*The contemporary foodie* seeks culinary experinces in menu-less restaurants and grandmothers' kitchens" [@Carlson1995; @NickelBlackwell; @sepGenerics]. -->
<!-- Each of these statements has in common the fact that they predicate a property of a category and the properties themselves apply to instances of the category (unlike, e.g., *Dinosaurs are extinct* because being extinct is not a property that can be true of a particular dinosaur).  -->
<!-- In this way, these statements exhibit *genericity*, or put another way, they convey generalizations. -->

<!-- In this paper, we formalize the language of generalizations with the interest is deciding what makes a generalization in language *true or false*. -->
<!-- How speakers convey generalizations and how listeners interpret them. -->

<!-- We examine three case studies of generalizations in language: Generalizations about agents, objects, and events.  -->
<!-- In Section 1, we introduce an information-theoric communicative theory of how generalizations are conveyed in language and flesh out the model's predictions alongside alternative models. -->
<!-- In order to do so, we provide a unified mathematical framework for dealing with agents, objects, and events, casting each as a category over which generalization can occur. -->
<!-- Our model predicts the endorsements of generalizations should vary with the (A) the probability of an instances having the property and (B) the prior distribution over the property probability (e.g., how likely instances of other categories are to have the property). -->
<!-- In Section 2, we introduce our empirical paradigm which we use to measure endorsement and interpretation of linguisitic generalizations, examining generalizations about *agents* in what is known as *habitual language* (e.g., "Mary smokes cigarettes.").  -->
<!-- In Section 3, we extend our paradigm to generalizations about *events* (i.e., *causal language* e.g., "Yeast makes bread rise.") and experimentally manipulate participants' background knowledge, showing that prior beliefs about probabilities are causally related to endorsement and interpretation of generalizations. -->
<!-- In Section 4, we use our paradigm to address long-standing theoretical and empirical puzzles regarding generalizations about *objects*, so-called *generic language*.  -->
<!-- We show our how model explains three known psychological puzzles surrounding generic language.  -->
<!-- In all case studies, we provide formal comparison to simpler models and in all cases, find that only our communicative theory explains the patterns of empirical data. -->
<!-- We conclude with a general discussion about the relationship of our theory to extant theories in the psychology, linguistics, and philosophy of genericity, and highlight the implications for modeling language acquisition, conceptual development, and other avenues of research that this framework opens. -->


<!-- the truth conditions of generalizations of events (so called *habitual language*) by manipulating the target propensity (A) while surveying a wide variety of events (which should induce variability in B). -->
<!-- In Section 3, we manipulate both (A) and (B) while testing the model on generalizations about causes (*causal language*). -->
<!-- If our theory is truly a theory about communicating generalizations, it should be able to explain the known psychological puzzles surrounding *generic language* (generalizations about categories).  -->



<!-- In stark constrast, a computational understanding of communicating generalizations is not well understood at all.  -->

<!-- Gathering data and observing evidence, however, is costly.  -->
<!-- The human problem of induction even displays a complex sensitivity to not only the type of data, but how the data was generated [@Gweon2010; @Shafto2012]. -->
<!-- A learner cannot form all of her abstract knowledge through direct experience, however. -->
<!-- Life is only so long, and you can only be in one place at a time. -->






<!-- In this paper, we introduce a formal model for understanding the language of generalizations. -->
<!-- <!-- The fact that generalizations in language seem to convey something about the propensity but that there seem to be as many interpretations of generalized statements as there are generalizations have confounded formal models aimed to capture their meaning. -->
<!-- We take as a starting point the idea that the core meaning of a linguistic generalization is simple (i.e., it conveys something about the propensity), but underspecified (i.e., there is uncertainty in the meaning). -->
<!-- We extend a rational model of communication --- the Rational Speech Act theory --- to be able to understand general statements that extend beyond the here-and-now. -->
<!-- We provide a number of stringest tests of this theory to explain generalizations about categories, events, and causal forces.  -->



<!-- How can generalizations in language have such flexible truth conditions while simultaneously carrying intuitive and, at times, very strong implications? -->
<!-- This question has been investigated by psychologists, linguists, and philosophers since @Carlson1977. -->
<!-- Here, we provide a unified formal theory of generalizations conveyed in language. -->
<!-- We test theory on the preeminent case study of linguistic generalizations: generic language, or generalizations about categories. -->
<!--  -->
<!-- To be precise, we develop a computational model that describes pragmatic reasoning about the propensity required to assert the generaization.   -->
<!-- We find that this formalism resolves known philosophical and empirical puzzles. -->
<!--We also provide the first set of experimental results about linguistic generalizations about events and causal forces, so called *habitual* and *causal statements*.-->

<!-- and find extreme flexibility in usage, which our model predicts.  -->

<!-- In Section 2, we conduct a set of experiments to test the *truth conditions* of familiar generalizations about categories (generic statements), and find our model predicts key patterns in human judgments -->
<!-- In Section 3, we conduct a set of experiments to test *interpretations* of novel generic sentences, again finding our model predicts human judgments with high quantitative accuracy. -->
<!-- In Section 4, we begin to test the generality of the theory, examining the *truth conditions* of generalizations about events. -->
<!-- In Section 5, we further probe our theory's generality, investing the *truth conditions* and *interpretations* of novel generalizations about causal forces.  -->
<!-- In all cases, we provide formal comparison to simpler models and in all cases, find that only our communicative theory with an underspecified meaning function explains the pattern of human data. -->
<!-- We also find that the prior beliefs used in our model reflect conceptual structure, and this provides an understanding of the conceptual implications of generics. -->
<!-- Finally, we investigate the underlying semantic scale in more detail, and find that the core meaning of generalizations in language depends in a crucial way on subjective beliefs, not mere frequency. -->

<!--
Most would agree that \emph{Swans are white}, but certainly not every swan is.
This type of utterance conveys a generalization about a category (i.e. \textsc{swans}) and is known as a generic utterance [@Carlson1977; @Leslie2008].
Communicating generically about categories is useful because categories themselves are unobservable [@Markman1989].
Knowledge about categories, while central to human reasoning, is tricky to acquire because categories themselves are unobservable [Markman1989}.
It is believed that every language can express generic meaning [@Behrens2005; @Carlson1995], and that generics are essential to the growth of conceptual knowledge [@Gelman2004] and how kinds are represented in the mind [@Leslie2008].
Generic language is ubiquitous in everyday conversation as well as in child-directed speech [@Gelman2008], and children as young as two or three understand that generics refer to categories and support generalization [@Cimpian2008].
and though English and many other languages do not possess an unambiguous form devoted to generic meaning [@Behrens2000, @AlMalki2014].
Additionally, generics are the primary way by which speakers discuss social categories, making them key to propagating stereotypes [@GelmanEtAl2004; @Rhodes2012; @Leslie2015] and impacting motivation [@Cimpian2010motivation].
Despite their psychological centrality, apparent simplicity, and ubiquity, a formal account of generic meaning remains elusive.


How can generics have such flexible truth conditions while simultaneously carrying strong implications?
In this paper, we explore the idea that the core meaning of a generic statement is simple, but underspecified, and that general principles of communication may be used to resolve precise meaning in context. 
In particular, we develop a mathematical model that describes pragmatic reasoning about the degree of prevalence required to assert the generic.  
We find that this formalism resolves the philosophical and empirical puzzles.
-->



# A Formal View of Generalization

Generalizations are used to make predictions about instances that an agent has yet to experience [@HumeTHN].
From a very young age, children draw strong inferences about nonobvious object properties from just a few examples [@Baldwin1993].
To form a generalization about a category or kind $k$, an observer must be able to individuate an exemplar or instance $x$ as belonging to the category.
This is faciliated through the use of labeling; the linguistic expression of such an individuation would manifest in utterances such as "$x$ is a $k$" or "this $k$ ..."
To generalize a feature $f$ to that category, it is also necessary to be able to determine if the particular instance has the feature.
That is, a speaker must be able to say "This $k$ (i.e., $x$) has $f$".
We call this latter kind of statement a *particular*.

```{r table-generalization, results="asis"}
df.tab <- data.frame(
  Generalization = c("Dogs are friendly", "John smokes", "Drinking moonshine makes you go blind"),
  #x = c("an instance of K in space", "an instance of K in time", "an instance of a "),
  "Generalization over" = c("Category", "Event", "Cause"),
  "Linguistic construct" = c("Generic", "Habitual", "Causal"),
  x = c("a dog", "an instance of John", "an instance of a person drinking moonshine"),
  k = c("DOGS", "JOHN", "DRINKING MOONSHINE"),
  f = c("is friendly", "is smoking / smoked at time t", "caused person to go blind")
) %>% xtable(.,
         caption = c("Decomposition of generalizations into concrete particulars and corresponding properties."),
        label = c("tab:genpart"))

# apa_table(df.tab, 
#           caption = "Decomposition of generalizations into concrete particulars and corresponding properties.", 
#           escape = TRUE, small = TRUE
# )

print(df.tab, type = "latex", 
      tabular.environment = "tabularx", width = "\\textwidth",
      scalebox='0.75',
      include.rownames = FALSE, comment = F)
```

Particular statements can be made about objects, events, and causes (e.g., "My dog is friendly.", "John ran this afternoon.", or "My grandfather drank moonshine and it made him go blind.").
To consider the corresponding generalizations (i.e., "Dogs are friendly", "John runs", "Drinking moonshines makes you go blind"), we must specify exactly how the particular relates to the generalization.
We consider the generalization to be about a *category* and the particular to correspond to *instances* of that category.
Thus, *dog* is an instance of the category \textsc{dogs}, *John at a particular time* is an instance of the category of \textsc{john}, and *drinking moonshine at a particular time* is an instance of the category of \textsc{drinking moonshine} (see Table \ref{tab:genpart}).

Suppose we observe instances $\{x_1, x_2, ..., x_n\}$ of category $k$ and some number of them have feature $f$: What do we learn about possible future instances of $x_{n+1}$, particularly with respect to $f$?
For properties among instances of a category (e.g., being friendly among dogs), we can describe this inductive inference by a probability $h$ $h = P(x \in f \mid x \in k)$^[
With a frequentist interpretation, this can be thought of as the *prevalence* of $f$ among $k$.
].
<!-- In the study of categories and properties, $x$ is an object. -->
For an action exhibited by an agent, the inference concerns a rate --- how frequently (in time) does $x \in f$ occur --- which can be described as a probability mass: $h = \int_{t} P(x \in f_t \mid x \in k_t) \diff t$
<!-- In this case, $x$ is a instance of an agent. -->
Causal learning can be described with the same formalism as for categories, a probability referring to the likelihood of an effect $e$ given a cause $c$, also known as causal power: $h = P(x \in e \mid x \in c)$.
Throughout, we refer to the degree of probability as $h$ because it represents a hypothesis about the underlying causal power or frequency of some instance in a category.
<!-- A Bayesian agent will have prior beliefs about the probability (potentially an uninformed prior) and uses Bayesian inference to learn about the probability from her observations. -->

<!-- Suppose *a priori* we have no informative beliefs about the propensity (i.e., $P(x \in f \mid x \in k) \sim \text{Uniform}(0, 1)$). -->
Observing some number of positive instances of $k$ with $f$ will update a prior belief distribution into a posterior belief distribution.
As a concrete example, consider the feature *is friendly* ($f$) for the category *dog* ($k$): $h = P(x \in \{\text{is friendly}\} \mid x \in \{\text{is a dog}\})$.
If an agent were to encounter some number of individual dogs, note how many of them were friendly ($d$), she could form a posterior belief distribution about the probability that the next dog she encounters will be friendly (in other words, a belief about the friendliness of dogs).

$$
P(h \mid d) = \frac{P(d \mid h) \cdot P(h)}{\int_{h'}P(d \mid h') \cdot P(h')}
$$

Our ability to use this updated belief $P(h \mid d)$ in order to make predictions about unobserved instances is a computational description of generalization [e.g., @Tenenbaum2006; @Tenenbaum2011].

The human capacity to generalize from observations has been studied extensively in cognitive and developmental psychology [@Nisbett1983; @Baldwin1993; @Heit2000].
Inductive inferences show complex sensitivites to the kind of observations, and even how those observations came to be observed in the first place.
For instance, observing that an indigenous person living on a remote island *is obese* provides considerably less information about the weights of other islanders in comparison to inferences about skin color upon learning that one islander's skin color *is brown* [@Nisbett1983].
This subtlety can be modeled using heirarchical models to elaborate the prior in a Bayesian model [@Kemp2008].
Observations that are generated by an intentional agent can similar lead to subtle inferences, like unconfounding otherwise ambiguous causal evidence [@Goodman2009].

Studying generalizations from observational evidence can only take us so far, however, in understanding how the mind understands the world or culture accumulates across generations.
Relying upon observations would be particularly problematic to learn, for example, about properties that are difficult to observe (e.g., *cows have four stomachs*) or events that are statistically unlikely (e.g., *lightning strikes tall objects*).
For abstract, generalizable knowledge to remain in a culture, it must be able to be faithfully transmitted between individuals and across generations [@Tomasello1999], and language provides simple ways to communicate generalizations.


# Formalizing Generalizations in Language

We will discuss communicating generalizations in the context of generalizations about categories (i.e., *generic language*), though the arguments will extend to events and causes as well, following the definitions of $h$ described in the previous section. 
Generics express a relation between a kind K (e.g., \textsc{dogs}) and a property F (e.g., \textsc{is friendly}), such that the property can also be said to be applicable to an individual (i.e., "My dog is friendly"). 
Bare plural statements (e.g., *Dogs are friendly*) strongly tend to yield a generic meaning, though other forms can express such a meaning [e.g., *A mongoose eats snakes.*; @Carlson1977] .

Given that generics express a property that can be applied to individuals, it seems intuitive that what makes the statement true or false would correspond to the number of individuals who hold the property.
At first glance, generics convey that *most* have the property, such as *Most dogs are friendly*.
*Robins lay eggs* is true, though, despite the fact that only female robins lay eggs, which is debatable whether or not that counts as *most robins*. 
If generic statements can be true by restricting themselves to only be describing females, why does *Robins are female* seem like a weird statement to make?
Finally, *Mosquitos carry malaria* is intuitively true despite malaria only being present in a tiny fraction of a percentage of mosquitos.
It seems that any hard condition on the prevalence of the feature in the category (e.g., how many robins lay eggs) would violate intuitions. 

These observations have led to proposals that prevalence or probability is only peripherally related to generic meaning. 
**Conceptual accounts** of generics emphasize the structure of generic knowledge [@Prasada2000], and view generic utterances as a special way of expressing mental relationships between kinds and properties [@Leslie2008; @Prasada2012]. 
From this perspective, the statement *Bishops move diagonally* is true not because most bishops move diagonally, but rather those are the rules of chess.
The most influential account in psychology is credited to @Leslie2007, who proposes that generics express a cognitively primitive, *default generalization*. 

The *generics as default* view argues that general cognitive principles explain why certain generics are true or not.
*Mosquitos carry malaria* conveys a dangerous or striking property, which is useful to know about if you are trying to survive in a hostile world.
This theory thus predicted that generics that convey striking properties would be true even if they were relatively rare (e.g., *Sharks attack swimmers*), which was empirically corroborated [@Cimpian2010; @Prasada2013].
Other generics convey properties that are *characteristic* of a kind, such as *Robins lay eggs*, argued to have a *principled connection* between kind and property [@Prasada2006; @Prasada2013].

Yet, generics are not limited to conveying rich relations. 
Arbitrary relationships can also be expressed (e.g., *Barns are red*; *Ravens are bigger than toasters*) which seem only to be true because those are the statistics of the world [@Nickel2008; @Nickel2016; @Sterken2015].
The *generics as default* view must then be supplemented with the principle that, in certain situations,  statistics can influence generic endorsement.
It is thus argued that there are qualitatively different kinds of generic statements (i.e., "striking", "characteristic", "statistical"), each with their own conditions on what makes them true.
This diversity is interested, but what is common across generics?
Is there some core meaning to *genericity*, a general unified principle that explains what makes generic statements true or false?

We propose that generic language and other forms of genericity information convey information about probabilities that underly generalization and prediction.
We posit that the conceptual factors of the kind discussed by Prasada, Leslie, and colleagues influence the meaning of a generic statement via their influence on probabilities. 
Bayesian cognitive models have been shown to represent flexible, structured knowledge of the world and formalize the inferences that people draw using such conceptual representations [e.g., @Kemp2008; @Goodmanconcepts].
Thee basic currency of inference in these models is probability.
In generalizations in language convey information about probabilties, then a relatively simple semantic theory (e.g., a threshold on probability) may be enough to formalize the core meaning of a generic statement $u_{gen}$:
<!-- In the case of generic language, we posit that conceptual factors may influence the meaning of a generic statement via its influence on subjective probability. -->
<!-- If that's true, a simple, semantic theory  -->


\begin{eqnarray}
\delta_{\denote{u_{gen}}(h, \theta)} & = & P(u_{gen} \mid h, \theta) \\
&\propto  &
\begin{cases}
1 & \text{if } h > \theta \\
0 & \text{otherwise}
\end{cases}
\end{eqnarray}

$\delta_{\denote{u}(h, \theta)}$ is the Kronecker delta function returning $1$ for states $h$ greater than threshold $\theta$ and $0$ otherwise. 
Logical quantifiers can be described using threshold semantics (i.e., *some* is $h > 0$, all is $h = 1$).
What threshold $\theta$ should be used for the generic generalization?

To answer this, we draw a metaphor to Bayesian statistics and data analysis.
When there is a latent parameter the Bayesian scientist doesn't know much about, she puts a prior distribution over the parameter and infers the parameter's likely value by collecting data.
In a similar vein, we posit that the meaning of a generalization in language is *underspecified*; that is, there is not a fixed $\theta$ that corresponds to the generalization, but rather the listener has uncertainty over $\theta$ (in the form of a prior distribution $P(\theta)$) and infers $\theta$ in context (i.e., after having heard the utterance).
Though it's possible different $P(\theta)$ could be learned for different contexts, we assume this prior is identical across context and is completely uninformed: $P(\theta) = \text{Uniform}(0, 1)$.


\begin{eqnarray}
L(h, \theta \mid u) &\propto& {\delta_{\denote{u}(h, \theta)} \cdot P(h) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

Equation \ref{eq:L0} is a model of a listener $L$ who updates her beliefs $h$ according the meaning an utterance $u$.
$P(h)$ represents the listener's prior world knowledge concerning the relevant probabilities $h$.
The particular form of the prior plausibly depends upon the kind of property or event in question and could be structured as a result of deeper conceptual knowledge about properties and events [cf., @Griffiths2005; @Tenenbaum2006; @Kemp2008].
We term this prior distribution the *prevalence prior*.
$\delta_{\denote{u}(h, \theta)}$ is the likelihood term in Bayes' Rule, representing the literal semantics of utterances described above.


<!-- \begin{eqnarray} -->
<!-- L(h \mid u) &\propto& {\delta_{\denote{u}(h, \theta)} \cdot P(h)} \label{eq:L0fixed} -->
<!-- \end{eqnarray} -->


This is a Bayesian model of interpreting a statement conveying a generalization whose meaning is simple (i.e., a threshold function) but underspecified.
Figure \ref{fig:simulations} shows model $L$'s posterior distribution on $h$ upon hearing utterances corresponding to the uncertain threshold (Eq. \ref{eq:L0}) as well as a comparison model using a fixed threshold at 0 (intuitively, corresponding to the quantifier "some").
The differences in the semantics are easily seen when considering a uniform distribution over $h$ (top left facet).
The fixed threshold model rules out only the lowest possible state and returns a posterior very similar to the prior.
The behavior of the generalization is distinct: The higher the $h$, the more likely the state is.

The intuition behind the model's behavior can be gleaned by imagining what the listener $L$ would believe with different thresholds and averaging over those possibilities.
If the threshold $\theta$ is very high, only the highest $h$ would pass the threshold.
If the threshold $\theta$  is slightly lower but still high, the highest $h$ probabilities would still pass the threshold, as would some that are slightly lower.
As the threshold $\theta$  takes on lower and lower values, more and more $h$ probabilities would surpass it.
When $\theta$ is very low, almost all $h$ are consistent with it (akin to "Some").
When the listener averages over these possibilities, higher values of $h$ pass more thresholds; thus, higher $h$ are more likely *a posteriori*.

In this paper, we are interested in explaining the variable endorsements of generalizations (e.g., *Birds lay eggs* is good; *Birds are female* is not) based on some known or believed probability $h$ (e.g., only female birds lay eggs).^[A more general version of this model can relax the assumption that the speaker has access to a degree of propensity $h$ that she is trying to communicate.
Rather, the speaker may have a belief distribution over $h$ for the category. $k$, corresponding to the speaker's beliefs about the prevalence of the property in the category. 
In this situation, we would define the speaker's utility to be the expected value of the informativity, which integrates over her belief distribution:
$S(u \mid k) \propto \exp{(\lambda \cdot {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L(h, \theta \mid u)} \diff \theta )}$
]
Following the Rational Speech Act (RSA) framework [@Goodman2016], we describe an endorsement model as a Bayesian decision of whether or not to say the generalization to the listener $L$ (Eq. \ref{eq:L0}).
\begin{equation} 
S(u \mid h) \propto \exp{(\lambda \cdot \ln{ \int_{\theta} L_0(h, \theta \mid u)}  \diff \theta  }
\label{eq:S1}
\end{equation}

In RSA, language production and comprehension are defined by recursive Bayesian models describing speaker and listener.
Following RSA, this speaker is assumed to be an approximately rational Bayesian agent with degree with rationality governed by parameter $\lambda$.
Since the speaker $S$ doesn't know the threshold $\theta$, she must integrate over the likely values that the listener will infer. 

The speaker's decision to say an utterance (e.g., the generalization) is determined with respect to how well it would communicate $h$ to the listener $L$, in comparison to alternative utterances.
To model endorsement, this decision is evaluated in comparison to the alternative of *not saying the generalization*;
with just two alternatives (say the utterance vs. not), the model can be interpreted as a model of felicity judgments [@Degen2014]. 
Formally, we consider the negative alternative as the option of staying quiet, which carries no information content^[
This alternative can be realized in at least two other ways: the speaker could have said the negation of the utterance (i.e., *The generalization is not true.*) or the negative generalization (i.e., *The opposite is true.*, e.g., *Ks do not have F*).
All results reported are similar for these two alternatives, and we use the alternative of the *silent* utterance for simplicity.
]:

\begin{equation*}
\delta_{\denote{u_{silence}}(h, \theta)} \propto \begin{cases}
1 &\text{for all h}
\end{cases}
\end{equation*}


The pair of speaker-listener models presented above correspond to the simplest possible communicative models with an underspecified threshold-meaning for a generalization. 
The *simple but underspecified* hypothesis defines a *family* of models, however, with potentially more levels of recursion (e.g., a listener who thinks about a speaker who thinks about a listener).
These are not of primary theoretical interest to this paper, as *any* formal model that explains the very flexible endorsement patterns of generalizations in language will be a substantial contribution.
However, there are interesting theoretical differences to be found within this family of models (e.g., if the inference about the threshold can be thought of as a *pragmatic inference* or just a mere *literal inference*).
We discuss these alternative formulations and provide formal model comparison between these RSA models in Appendix.

<!-- This decision is based on whether or not the generalization would accurately convey the $h$ -->
<!-- For addressing the question of whether or not a generalization is "true" or "felicitous", we consider how a Gricean speaker would act, given the goal of being informative to the literal listener (Eq. \ref{eq:L0}).  -->

<!-- S_{1}(u \mid k) \propto \exp{(\lambda \cdot {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L_0(x, \theta \mid u)}  \diff \theta ) } -->

<!-- Inspired by Bayesian decision theory, the speaker takes actions (i.e., produces utterances) soft-max optimally in accord with his utility function \cite{Baker2009}. -->
<!-- The utility function of a speaker is based on the informativeness of the utterance, which is computed with respect to what a naive listener $L_0$ would believe after hearing the utterance, taking into account the cost of the utterance $C(u)$.  -->
<!-- \begin{equation} -->
<!-- U(u; h) = \ln{ \int_{\theta} L_0(h \mid u)}  \diff \theta - C(u) -->
<!-- \label{eq:utilS1marginalization} -->
<!-- \end{equation} -->
<!-- Since the speaker doesn't know the threshold $\theta$, he must integrate over the likely values that the listener will infer. ^[ -->
<!-- A more general version of this model could relax the assumption that the speaker has access to a degree of propensity $h$ that he is trying to communicate. -->
<!-- Rather, the speaker may have a belief distribution over $h$, corresponding to his beliefs about the rate with which an event happens or the prevalence of the property in the kind.  -->
<!-- In this situation, we would define the utility function to be the expected value of the informativity, which integrates over his belief distribution:  -->
<!-- $U(u; k) = {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L_0(h, \theta \mid u)}  \diff \theta$ -->
<!-- ] -->
<!-- $S_1$ thus seeks to minimize the surprisal of $s$ given $u$ for the naive listener, while bearing in the mind the utterance cost $C(u)$. -->


<!-- No fixed value of the threshold, $\theta$, would allow for the extreme flexibility generics exhibit (e.g., *Mosquitos carry malaria*; *Robins lay eggs* v. *Robins are female*). -->


<!-- Abstract mental representations, then, may thus be influencing generic endorsement via its influence on subjective probability. -->


<!-- To understand how generalizations are conveyed in language, we start by considering general purpose language understanding principles. -->
<!-- Foremost, understanding language depends upon assumptions interlocutors make about each other and can exhibit a complex sensitivity to context [@Clark1996; @Grice1975; @Levinson2000]. -->
<!-- Though once viewed as a wastebasket in which to dump unexplained loose ends in service of more rigorous formal analysis, *pragmatics* --- or language understanding in context --- has seen an emergence of formal frameworks that connect closely with behavioral data [@Franke2015]. -->

<!-- One dominant framework --- the Rational Speech Act theory --- has had considerable success in providing a formal account of pragmatic language phenomena [i.e., interactions between language and context; @Goodman2016]. -->
<!-- This framework has been used to explain, in precise mathematical terms, how it is that utterances like "some of the kids failed the test" are interpreted to mean "some (but not all) of the kids failed the test" [@Goodman2013], how "I waited a million years to get a table" is interpreted as "I waited an obnoxiously long time to get a table" [@Kao2014], how "John is tall" can mean John is 5'4" is John if a 12-year-old but means John is 6'2" if he is 24-years-old [@Lassiter2015], and how "Your talk was fine" can be a polite way of saying your talk was less than fine [@Yoon2016].  -->
<!-- Computational modeling has pushed our understanding of these complex linguistic phenomena, but so far, rational models of language use have focused on *specific* statements: utterances that convey information about a particular entity or situation.  -->
<!-- A formal theory of statements that convey generalizations about entities or situations ---  statements that extend beyond the here-and-now --- has remained elusive. -->

<!-- Formalizing the meaning of linguistic expressions requires specifying *truth conditions*, or the states of the world in which the expression is true vs. false. -->
<!-- Using the tools of formal semantics, we specify a *truth function*: A function from states of the world to truth-values.  -->
<!-- A simple truth function for a generalization could be a threshold on an agent's posterior probability: $\denote{u_{generalization}}(h, \theta): \{ h: h > \theta\}$, where $\denote{u_{generalization}}$ represents a function from a state of the world $h$ and a threshold variable $\theta$ to the set of the states of the world where $h > \theta$.  -->
<!-- Applying this function to a set of worlds returns a set of worlds that are consistent with $h > \theta$. -->

<!-- Though intuitively appealing and simple, this threshold-based truth function for a generalization is problematic. -->
<!-- <!-- fixed value of $\theta$ seems unlikely. --> -->
<!-- The generalization *Mary smokes cigarettes* suggests a daily-habit (e.g., $h > \text{once a day}$), while *Bill writes poems* implies something much weaker, perhaps a poem every few weeks (e.g., $h > \text{once a month}$. -->
<!-- *Drinking moonshine makes you go blind* and so does *staring at the sun*, but neither causal statement tells you to precisely the increased relative risk. -->
<!-- *Mosquitos carry malaria* is a true statement despite the prevalence of carrying malaria among mosquitos being a just a fraction of 1\%, while *Ducks are female* might seem like a weird thing to say, even though the rate is much higher than the malaria-carrying mosquitos. -->
<!-- Finally, *Ducks lay eggs* is a good generalization, though the rate is the same as *Ducks are female*. -->
<!-- The meaning of generalizations, it seems, cannot reduce to a single, precise quantitative statement; this is the major stumbling block to providing a formal account of genericity [@Carlson1977; @Leslie2008; @NickelBlackwell]. -->

<!-- We hypothesize the simple threshold-based truth function for lingusitic generalizations can be rescued by positing an *underspecified* meaning.  -->
<!-- That is, we adopt a threshold semantics as described above, but imagine that $\theta$ is not a fixed property of language but rather is drawn from a prior probability distribution. -->
<!-- In this formulation, listeners infer thresholds along with speakers' intended meanings in context. -->
<!-- We formalize this hypothesis with a "literal listener" model from the Rational Speech Act framework [@Frank2012; @Goodman2013; @Goodman2016]. -->

<!-- *Bills writes poems* might true if Bill does it a few times a year, while we might be hesitant to say *Mary smokes cigarettes* if she only does it once every few months. -->
<!-- *Ducks lay eggs* is true despite only the female ducks laying eggs, while *Ducks are female* is strange to say (even though only the females are female). -->

<!-- The language of generalizations is so simple that it appears everywhere in human conversation (*Movie theaters are cold.*), pedagogy (*The blicket makes the machine play music.*), political and scientific discourse (*The President peddles falsehoods.*, *Psychology experiments dont replicate.*), stereotypes (*Boys are good at math*), motivation (*Big girls eat their broccoli!*), emotion-regulation  (*Accidents happen.*), child-directed (*Mommy works late*) and child-produced speech (*Slides aren't for grown-ups!*), and many other facets of experience [@GelmanEtAl2004; @Gelman2008; @Cimpian2010motivation; @Rhodes2012]. -->

<!-- Though the language of generalizations is found everywhere and in every language [@Behrens2005; @Carlson1995], the meaning of such linguistic expressions is philosophically puzzling and has resisted precise formalization. -->
<!-- Intuitively, generalizations should convey something about the probability of an instance of the category to have a property, but exactly what is conveyed is unclear. -->

<!-- What if we leave the $\theta$ underspecified in the semantics? -->
<!-- That is, what would a listener believe if she drew $\theta$ from an uninformed prior belief distribution: $P(\theta)$?^[ -->
<!-- Previous attempts have been made to salvage this threshold semantics, using a semantic a technique called "domain restriction", wherein the scope of generalization is restricted to a smaller subset of entities or instances [e.g., "only female ducks"; @Cohen1999].  -->
<!-- Though effective at dealing with at least some of these semantic puzzles, this approach leaves unresolved why the domain should be restricted in some cases and not others. -->
<!-- We return to this more fully in the General Discussion. -->
<!-- ] -->

<!-- [MHT: talk about soft semantics here?] -->

<!-- \begin{eqnarray} -->
<!-- L_{0}(h, \theta \mid u) &\propto& {\delta_{\denote{u}(h, \theta)} \cdot P(h) \cdot P(\theta)} \label{eq:L0} -->
<!-- \end{eqnarray} -->


<!-- The threshold $\theta$ is assumed to be drawn by an uninformative prior distribution, defined on the same scale as the probability $h$: $\theta \sim \text{Uniform}(0, 1)$.^[ -->
<!-- Though it's plausible that different thresholds could be learned for different domains, we assume an uninformed prior distribution in our model to show how background knowledge about how the world works drives inferences. -->
<!-- ] -->
<!-- Finally, the prior distribution $P(h)$ is a distribution over the degrees of probability. -->
<!-- The particular form of the prior plausibly depends upon the kind of property or event in question and could be structured as a result of deeper conceptual knowledge about properties and events [cf., @Griffiths2005; @Tenenabum2006; @Kemp2008]. -->
<!-- This is a simple Bayesian model of interpreting a statement conveying a generalization. -->


```{r simluationsModel, eval = T, cache = T}
l0.model <- '
var probability = function(Dist, x) {
    return Math.exp(Dist.score(x));
}
var targetUtterance = "generic";

var utterancePrior = Infer({model: function(){
  return uniformDraw([targetUtterance,"silence"])
}});

var thetaPrior = Infer({model: function(){
 return uniformDraw([
   0.01, 0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
   0.5, 0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95
 ])}
});

var bins = [
  0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
  0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99
];

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=="silence"? true :
         utt=="some"? state>0.01:
         utt=="most"? state> 0.5:
         utt=="all"? state >= 0.99:
         true
}

var mixture = data.prior.mix[0];
var priorParams = data.prior.params[0];

var statePrior = Infer({model: function(){
  var component = flip(mixture);
  return component ?
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta(priorParams), b) + Number.EPSILON
      }, bins )
    }) :
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta({a:1,b:50}), b) + Number.EPSILON
      }, bins )
    })
}});

var listener0 = cache(function(utterance) {
  Infer({model: function(){
    var state = sample(statePrior)
    var state_prior = sample(statePrior)
    var theta = utterance == "generic" ? sample(thetaPrior) : -99
    condition(meaning(utterance, state, theta))
    return {
      state_Posterior: state, 
      state_Prior: state_prior
  }
 }})}, 10000)


var continuousListener0 = function(utterance) {
  Infer({model: function(){
   var state = sample(Beta( flip(mixture) ? priorParams : {a:1,b:100}))
   var state_prior = sample(Beta( flip(mixture) ? priorParams : {a:1,b:100}))
//     var state = sample(flip(mixture) ? Beta( priorParams) : Delta({v:0}))
//     var state_prior = sample(flip(mixture) ? Beta( priorParams) : Delta({v:0}))
//    var theta = uniform(0,1)
    factor(
    (utterance == "generic") ? 
      Math.log(state) : 
    (state > 0.017) ? 0 : -Infinity
    )
    return {
      state_Posterior: state, 
      state_Prior: state_prior
  }
 }, method: "rejection", samples: 20000, burn:5000, verbose: T})}



//var out = {generic: listener0("generic"), some: listener0("some")}
//out
continuousListener0(data.utt[0])
'

priorNames <- c(
  "uniform", #"uniformRare", 
  "isFemale", 
  "laysEggs", 
  #"eatsFood", 
  "barks",
  "carriesMalaria"#,
 # "areSick"
  )
priorShapes <- list(
  uniform =  list(params = data.frame(a = 1, b = 1), mix = 1), 
  #uniformRare =  list(params = data.frame(a = 1, b = 1), mix = 0.4), 
  isFemale =  list(params = data.frame(a = 10, b = 10), mix = 1), 
  laysEggs =  list(params = data.frame(a = 10, b = 10), mix = 0.4), 
  #eatsFood =  list(params = data.frame(a = 50, b = 1), mix = 1), 
  barks =  list(params = data.frame(a = 5, b = 1), mix = 0.4), 
  #areSick =  list(params = data.frame(a = 1, b = 5), mix = 1), 
  carriesMalaria =  list(params = data.frame(a = 2, b = 10), mix = 0.4)
)

all.simulations <- data.frame()
for (p in priorNames){
  for (u in c("generic", "some")){
    
    inputData = list(prior = priorShapes[[p]], utt = u)
    l0.rs <- webppl(l0.model, data = inputData, data_var = "data")
    
    # all.simulations <- bind_rows(all.simulations,
    #                         bind_rows(
    #                           data.frame(l0.rs$generic) %>%
    #                             gather(key, val, -probs) %>%
    #                              group_by(key, val) %>%
    #                              summarize(prob = sum(probs)) %>%
    #                             ungroup() %>%
    #                               mutate(key = gsub("support.", "", key)) %>%
    #                              separate(key, into = c("Parameter","Distribution")) %>%
    #                               mutate(Distribution = gsub("Posterior", "Posterior_Generalization", Distribution)),
    #                           data.frame(l0.rs$some) %>%
    #                             gather(key, val, -probs) %>%
    #                             group_by(key, val) %>%
    #                             summarize(prob = sum(probs)) %>%
    #                             ungroup() %>%
    #                             mutate(key = gsub("support.", "", key)) %>%
    #                             separate(key, into = c("Parameter","Distribution")) %>%
    #                             mutate(Distribution = gsub("Posterior", "Posterior_0", Distribution)) %>%
    #                             filter(Distribution != "Prior"),
    #                           data.frame(l0.rs$most) %>%
    #                             gather(key, val, -probs) %>%
    #                             group_by(key, val) %>%
    #                             summarize(prob = sum(probs)) %>%
    #                             ungroup() %>%
    #                             mutate(key = gsub("support.", "", key)) %>%
    #                             separate(key, into = c("Parameter","Distribution")) %>%
    #                             mutate(Distribution = gsub("Posterior", "Posterior_05", Distribution)) %>%
    #                             filter(Distribution != "Prior")
    #                         ) %>%
    #                           mutate(PriorShape = p)
    # )
      all.simulations <- bind_rows(all.simulations,
                                   l0.rs %>%
                                  separate(Parameter, into = c("Parameter","Distribution")) %>%
                                  mutate(Distribution = gsub("Posterior", "Posterior_Generalization", Distribution))
                                  %>%
                              mutate(PriorShape = p, utterance = u)
    )
  }
  #print(p)
}
```

```{r simulations, fig.width = 9, fig.height=5}
all.simulations %>%
  mutate(Distribution = paste(Distribution, utterance, sep = "_")) %>%
  filter(Distribution != "Some", Distribution != "Most", Distribution != "Prior_generic") %>%
  mutate(Distribution = factor(Distribution, levels = 
                             c("Prior_some",  "Posterior_Generalization_generic", "Posterior_Generalization_some"),
                                labels = c("Prior/Silence",  "Generalization (Uncertain threshold)", "Fixed threshold")),
         PriorShape = factor(PriorShape, levels = c("uniform", "barks","laysEggs","carriesMalaria","isFemale"),
                             labels = c("uniform", "bark","lay eggs","carry malaria","are female")
                                  #"eatsFood","areSick", "uniformRare", 
                                  #"carriesMalaria"
                                  )
         ) %>%
  ggplot(., aes(x = value, #y = prob, 
                #lty = Distribution, 
                color = Distribution, fill = Distribution))+
    geom_density(#stat= 'identity', 
                aes(y = ..scaled..),
                 size = 0.6, alpha = 0.7, adjust = 1.5)+
    #geom_bar(stat = 'identity', position = position_dodge(), color = 'black')+
    theme_few() +
    #scale_color_solarized()+
    #scale_fill_solarized()+
    #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
   scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #scale_linetype_manual(values = c(3, 4, 2, 1))+
    scale_linetype_manual(values = c(3, 4, 1))+
    scale_x_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Degree of probability h") +
    ylab("Scaled probability density") +
  facet_grid(PriorShape~Distribution, scales = 'free')+
  guides(fill = F, color = F)+
  theme(strip.text.y = element_text(angle = 0))
```



<!-- For addressing the question of whether or not a generalization is "true" or "felicitous", we consider how a Gricean speaker would act, given the goal of being informative to the literal listener (Eq. \ref{eq:L0}).  -->

<!-- <!-- S_{1}(u \mid k) \propto \exp{(\lambda \cdot {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L_0(x, \theta \mid u)}  \diff \theta ) } --> 
<!-- \begin{equation}  -->
<!-- S_{1}(u \mid h) \propto \exp{(\lambda \cdot U(u; h)  } -->
<!-- \label{eq:S1} -->
<!-- \end{equation} -->

<!-- Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda$. -->
<!-- Inspired by Bayesian decision theory, the speaker takes actions (i.e., produces utterances) soft-max optimally in accord with his utility function \cite{Baker2009}. -->
<!-- The utility function of a speaker is based on the informativeness of the utterance, which is computed with respect to what a naive listener $L_0$ would believe after hearing the utterance, taking into account the cost of the utterance $C(u)$.  -->
<!-- \begin{equation} -->
<!-- U(u; h) = \ln{ \int_{\theta} L_0(h \mid u)}  \diff \theta - C(u) -->
<!-- \label{eq:utilS1marginalization} -->
<!-- \end{equation} -->
<!-- Since the speaker doesn't know the threshold $\theta$, he must integrate over the likely values that the listener will infer. ^[ -->
<!-- A more general version of this model could relax the assumption that the speaker has access to a degree of propensity $h$ that he is trying to communicate. -->
<!-- Rather, the speaker may have a belief distribution over $h$, corresponding to his beliefs about the rate with which an event happens or the prevalence of the property in the kind.  -->
<!-- In this situation, we would define the utility function to be the expected value of the informativity, which integrates over his belief distribution:  -->
<!-- $U(u; k) = {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L_0(h, \theta \mid u)}  \diff \theta$ -->
<!-- ] -->
<!-- $S_1$ thus seeks to minimize the surprisal of $s$ given $u$ for the naive listener, while bearing in the mind the utterance cost $C(u)$. -->


## Model simulations

All models were implemented in the probabilisitic programming language WebPPL [@dippl].
The following simulations were implemented in the RMarkdown document that generated this paper text and figures, using the R package RWebPPL.
That document, as well as the other models, analysis, data and links to experiments used in this paper can be found at \url{https://mhtess.github.io}.

The RSA model defined in the previous section predicts that the endorsement of a generalization depends upon a listener's *a priori* beliefs about the probabilty in question $P(h)$.
For purposes of illustration, we present present the listener $L$ interpretations of the generalization for the underspecified threshold model as well as a fixed threshold model where the threhsold is set to the lowest possible value, corresponding intuitively to the quantifier *some* (Figure \ref{fig:simulations}).

Above, we described the intuition for how an underspecified threshold interacts with a uniformly uncertain belief about the probability $h$.
Here, we consider some canonical cases from the literature on generics to demonstrate the kind of inferences our listener model would draw from different generic statements.
Note that our model of endorsements is a model of a speaker who decides whether or not to say the generic to the listener or to stay silent. 
Qualitatively speaking, model predictions for endorsement can be read off Figure \ref{fig:simulations} by comparing the posterior distribution over the probability $h$ upon hearing the generalization with the prior distribution over $h$ (which is the same as the listener's posterior upon hearing "silence").
The speaker decides, for the degree of target probability $h$ that she seeks to communicate, will the generalization be better than nothing?

In Figure \ref{fig:simulations} (row 2), we show a schematic prior distribution over a feature "barks".
Here, we posit the prior is bimodal with peaks near 0\% and some high prevalence values, corresponding intuitively to the fact that many kinds of creatures do not bark (i.e., have $h \sim 0$; e.g., cats) and for those that do have members that bark, many of them bark (i.e., have some high $h$).
<!--  -->
We see that the fixed threshold (middle row) struggles to differentiate those that generally do not have the feature from those that usually have the feature, returning a posterior distribution very similar to the prior.^[
We model "those that do not generally have the feature" a Beta distribution with an expected value (mean) close to zero. 
The fact that this component of the prior is not just a Delta function at 0\% reflects the intuitive possibility that some members of a kind who do not generally have the feature (e.g., a cat who barks) could somehow acquire the feature from accidental or transient causes (e.g., a strange genetic mutation).
]
Hearing the generalization ("Dogs bark"), however, strongly signals the property is widespread in the category.

The strength of a listener's inference about $h$ (i.e., how prevalent is the feature?) depends in systematic ways on the prior belief distribution over $h$.
Consider a prior intuitively corresponding to the feature "lays eggs" (Figure \ref{fig:simulations}, row 3). 
The listener's posterior upon hearing the generalization (e.g., "Robins lay eggs") is much more conservative than the "barks" prior: The listener thinks that about 50% of robins lay eggs.
Indeed, this is the same inference as if a listener heard "Robins are female" (Figure \ref{fig:simulations}, row 4, right panel). 
Differences in the endorsements of these two statements would come from the speaker's comparison to what a listener would believe if she stayed silent (i.e., the listener's prior).
In the case of "are female", the posterior is nearly identical with the prior; thus, the utterance has very litle information content. 
In the case of "lays eggs", the utterance carries a lot of information (relative to the prior).

Finally, we look at how a listener would interpret the statement "Mosquitos carry malaria", which intuitively should correspond to "Some mosquitos carry malaria".
Indeed, we see that for the prior over "carries malaria"---which *a priori* assumes that most creatures do not carry malaria and among those that do, the feature is not very widespread---the listener's posterior upon hearing the generalization is similar to what she learns about hearing "Some mosquitos carry malaria". 
Thus, a speaker who believes "some but very few" mosquitos carry malaria will still endorse the generalization.

In sum, these simulations show that the behavior of the uncertain-threshold semantics is distinct from both the prior distribution over probability $h$ as well as the inferences derived from a fixed-threshold semantics.
In the rest of this paper, we examine the behavior of the speaker model as a model of endorsements through three case studies: generalizations about categories, events, and causes.


<!-- Consider how an uniformly underspecified threshold would interact with a uniformly uncertain belief about a probability (recall the thresholds defines what probabilities or states of the world should be ruled out).  -->
<!-- If the threshold is very high, only the highest probabilities would be left (i.e., pass the threshold). -->
<!-- If the threshold is slightly lower but still high, the highest probabilities would still pass the threshold, as would some that are slightly lower. -->
<!-- As the threshold takes on lower and lower values, more and more probabilities would surpass it. -->
<!-- When the threshold is very low, almost all probabilities are consistent with it. -->
<!-- But since we don't know *a priori* what the threshold is likely to be, each of theses situations is plausible. -->
<!-- The posterior distribution on probabilities or states of the world will favor higher probabilities (because they pass more thresholds), but will still include probabilities that are quite low [though they are *a posteriori* unlikely; see Figure \ref{fig:simluations} ].  -->
<!-- Speakers, thus, will produce generalizations with some probability in each state of the world, though the speaker will say the generalization more often when the propensities associated with the states of the world are higher [see Figure \ref{fig:simluations}]. -->

<!-- - Figure -->
  <!-- - Subfigure 1 [width = 0.2] (Uniform priors) [two rows, left subfigure] -->
  <!--   -- Basic L0 posteriors on degrees for uncertain, fixed at 0 and fixed at 0.5 -->
  <!--   -- Basic S1 posteriors on utterances, for prevalence 0 - 1 (and for fixed thresholds) -->
  <!-- - Subfigure 2 [width = 0.8] (L0 vs. L1 lifted vs. L1 unlifted x 4 priors x {speaker, listener} ) [model as color, lty; priors as columns x {speaker, listeners} as rows]  -->
  <!--   -- priors = [uniform, n-shaped, u-shaped, mixture model / maybe put more in appendix: rare, accidental, female, eggs] -->

<!--
mht: is it possible to construct priors in causals that would reflect the qualitative prediction shown here?
-->






<!-- $P(x)$ is a distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across animal categories.  -->
<!-- In Figure \ref{fig:schematic-unif}, are $L_1$ (Eq.~\ref{eq:L1}) posterior prevalence distributions on $x$ (red solid line) for several example prior prevalence distributions $P(x)$ (blue dashed line), as well as the $S_2$ generic endorsement probability for different levels of prevalence. -->
<!-- We name these example prior distributions to suggest properties that might be associated with such priors.  -->
<!-- (Later, we will empirically measure these priors for properties of interest.) -->
<!-- First, we explore generic interpretation and endorsement for priors of three different shapes (left column).  -->
<!-- For each of these, the $L_1$ posterior distribution (red solid) over prevalence is heavily driven by the prior (blue dashed). -->
<!-- $S_2$ endorsement probabilities for the generic (black solid) increase as a function of prevalence, and what counts as "true" (in terms of the prevalence) depends on the prior.  -->

<!-- \begin{figure} -->
<!-- \centering -->
<!--     \includegraphics[width=\columnwidth]{figs/schematics_s2.pdf} -->
<!--     \caption{Schematic prior distributions for prevalence $x$ (blue dashed), the pragmatic listener $L_1$ model's posterior distribution over prevalence upon hearing a generic utterance (red), and speaker $S_2$ model's endorsement of a generic utterance for different levels of prevalence (black). -->
<!--     The names given to these priors are meant to be suggestive of what kinds of properties these distributions might correspond to. -->
<!--     The left column uses prevalence priors modeled as Beta(15,15), Beta(4,1), and Beta(4,16) distributions. -->
<!--       The right column uses a prior distribution that is a mixture of the distribution to the left of it with a second component, modeled as Beta(0.5, 4.5), reflecting categories with 0\% property prevalence. -->
<!--     Horizontal dashed line at 0.5 is for convenience of comparing the point at which an utterance becomes judged as more true than false for $S_2$. -->
<!-- 	Note that the prior distribution over prevalence will be the same as $L_1$'s posterior distribution upon hearing the "null" utterance. -->
<!--     } -->
<!--   \label{fig:schematic-unif} -->
<!-- \end{figure} -->


<!-- To take one example, consider the distribution over what might be the property "are female" (top-left facet). -->
<!-- The \emph{a priori} prevalence is centered around 0.5. -->
<!-- Because of pragmatics, the pressure to be \emph{truthful} will drive likely threshold values below 0.5 (lower values are more likely to be true).  -->
<!-- At the same time, the pressure to be \emph{informative} will drive the threshold values up: Higher values are more likely to be informative.  -->
<!-- The result is a posterior over prevalence that is only marginally greater than the prior, making higher prevalence values more likely after hearing the generic.  -->
<!-- But the relative information gain is very little (the posterior is not very different from the prior), and thus the $S_2$ model is reluctant to endorse the generic unless the property is exceedingly prevalent.  -->
<!-- The same basic phenomenon can be observed for the other two example priors (left column): The posterior over prevalence heavily depends upon the prior, but is also not very different from the prior. -->

<!-- We now imagine what would happen if there were some kinds where the property was completely absent, while being present at some rate in other kinds. -->
<!-- Figure \ref{fig:schematic-unif} (right column) shows this possibility: mixing the distribution to the left of it with a second component at 0\% prevalence. -->
<!-- Consider the schematic prior over "lay eggs". -->
<!-- We see the pragmatic listener $L_1$'s posterior prevalence distribution is not very different from the interpretation that doesn't include the second component in the prior (here, "are female"), but it is dramatically different from the prior with two components (compare red with blue dashed line for the right column). -->
<!-- This suggests that when many categories have 0\% prevalence, lower thresholds are informative.  -->
<!-- Indeed, the $S_2$ model predicts that the generic becomes felicitous at lower prevalence levels (compare black line of left v. right column). -->
<!-- For "lay eggs", when the property is prevalent in 50\% of the kind (e.g., 50\% of birds lay eggs), endorsement of the generic (e.g., \emph{Birds lay eggs}) by the $S_2$ is roughly 0.85; for "are female", with the same prevalence (50\% of birds are female), endorsement for \emph{Birds are female} is only 0.50: It is judged to be neither true nor false.  -->
<!-- The important result is the asymmetry: the first generic can be endorsed more strongly than the second, at the same prevalence level; the exact endorsement rates depend on quantitative aspects of the priors, which must be determined empirically. -->

<!-- The model thus predicts differences in truth judgments depending on the prevalence prior.  -->
<!-- The first test of our theory, then, will be to see if these predictions correspond to human truth judgments of familiar generic sentences.  -->




<!-- Next is a hypothetical speaker, who reasons about the literal listener when deciding which utterance to produce. -->
<!-- \begin{eqnarray} -->
<!-- S_{1}(u \mid x, \theta) &\propto& \exp{(\lambda_1 \cdot \ln {L_{0}(x \mid u, \theta)} ) }\label{eq:S1} -->
<!-- \end{eqnarray} -->
<!-- This speaker $S_1$ also knows the threshold $\theta$ and chooses utterances to convey information to the literal listener $L_0$ . -->
<!-- Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda_1$. -->


<!-- For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly). -->
<!-- In this model, a threshold $\theta$ is established in context by a Bayesian listener reasoning about what threshold would make this utterance true? -->



<!-- The model begins with a simple Bayesian agent---a literal listener---who updates her prior beliefs $P(x)$ according to the truth-functional meaning of the utterance heard $\denote{u}$. -->
<!-- \begin{eqnarray} -->
<!-- L_{0}(x \mid u, \theta) &\propto& {\delta_{\denote{u}(x, \theta)} \cdot P(x)} \label{eq:L0} -->
<!-- \end{eqnarray} -->
<!-- For the generic sentence, we assign the simplest possible meaning, a threshold on the prevalence: $\denote{u_{generic}} = P(F\mid K)>\theta$. -->

<!-- The likelihood $\delta_{\denote{u}(x)}$ is the Kronecker delta function returning $1$ for states $x$ compatible with utterance $u$ (i.e., where the prevalence $x$ is above the threshold $\theta$), and $0$ otherwise. -->
<!-- The literal listener is a hypothetical agent who knows the threshold $\theta$. -->

<!-- Next is a hypothetical speaker, who reasons about the literal listener when deciding which utterance to produce. -->
<!-- \begin{eqnarray} -->
<!-- S_{1}(u \mid x, \theta) &\propto& \exp{(\lambda_1 \cdot \ln {L_{0}(x \mid u, \theta)} ) }\label{eq:S1} -->
<!-- \end{eqnarray} -->
<!-- This speaker $S_1$ also knows the threshold $\theta$ and chooses utterances to convey information to the literal listener $L_0$ . -->
<!-- Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda_1$. -->



<!-- For the generic sentence, we assign the simplest possible meaning, a threshold on the propensity: $\denote{u_{generic}} = P(F\mid K)>\theta$. -->
<!-- The prior distribution $P(h)$ is a distribution over likely degrees of propensity.  -->
<!-- For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly). -->

<!-- Life is only so long, however, and a person can only be in one place at a time. -->
<!-- The amount of data we can gather about any particular category, event, or causal force is extremely limited by these physical constraints. -->
<!-- Thus, it would be useful if language provided a simple way to communicate these generalizations to one another. -->





<!-- # A Formal Theory of Generalizations in Language -->

<!-- We find both the statistical and conceptual accounts compelling, but an important perspective is missing. -->
<!-- Linguistic generalizations are not unique in their flexibility. -->
<!-- Understanding language in general depends upon assumptions interlocutors make about each other and what content is under discussion.  -->
<!-- Viewing language understanding as a social reasoning process reveals that utterances carry a mosaic of interpretations with a complex sensitivity to context [@Clark1996; @Grice1975; @Levinson2000].  -->
<!-- Can the puzzles of generic language be understood as effects of pragmatic reasoning? -->
<!-- If so, we may be able to get away with a relatively simple, statistical,  semantic theory. -->
<!-- Abstract mental representations, then, may be the conceptual backdrop against which generic language is interpreted.  -->

<!-- We pursue such a line of inquiry, assuming the simplest truth-conditional meaning: a threshold on prevalence $P(F\mid K)>\theta$ [c.f., @Cohen1999]. -->
<!-- No fixed value of the threshold, $\theta$, would allow for the extreme flexibility generics exhibit (e.g. \emph{Mosquitos carry malaria}; \emph{Robins lay eggs} v. \emph{Robins are female}),  -->
<!-- so instead we allow this threshold to be established in context through pragmatic reasoning. -->
<!-- Such an inference would depend on background knowledge about properties and categories---potentially structured, conceptual knowledge. -->
<!-- This inference, nonetheless, is a general mechanism of understanding language, not specific to interpreting generic statements. -->
<!-- We formalize this hypothesis in the Rational Speech Act (RSA) theory---a formal, probabilistic theory of language understanding. -->
<!-- RSA is derived from social reasoning, formalized as recursive Bayesian inference between speaker and listener [@Frank2012; @Goodman2013; @Goodman2016; see also, @Franke2009; @Franke2015]. -->
<!-- @Goodman2016 provides a good introduction to the RSA framework and Appendix A presents a brief tutorial on RSA for the reader unfamiliar. -->


<!-- The model begins with a simple Bayesian agent---a literal listener---who updates her prior beliefs $P(x)$ according to the truth-functional meaning of the utterance heard $\denote{u}$. -->
<!-- \begin{eqnarray} -->
<!-- L_{0}(x \mid u, \theta) &\propto& {\delta_{\denote{u}(x, \theta)} \cdot P(x)} \label{eq:L0} -->
<!-- \end{eqnarray} -->
<!-- For the generic sentence, we assign the simplest possible meaning, a threshold on the prevalence: $\denote{u_{generic}} = P(F\mid K)>\theta$. -->
<!-- The prior distribution $P(x)$ is a distribution over likely states of the world.  -->
<!-- For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly). -->
<!-- The likelihood $\delta_{\denote{u}(x)}$ is the Kronecker delta function returning $1$ for states $x$ compatible with utterance $u$ (i.e., where the prevalence $x$ is above the threshold $\theta$), and $0$ otherwise. -->
<!-- The literal listener is a hypothetical agent who knows the threshold $\theta$. -->


<!-- To understand how language is interpreted pragmatically, we model a pragmatic listener $L_1$ who updates her prior beliefs $P(x)$ by reasoning about the generative process of the utterance, the speaker model $S_1$. -->
<!-- \begin{eqnarray} -->
<!-- L_{1}(x , \theta \mid u) &\propto& S_{1}(u \mid x, \theta) \cdot P(x) \cdot P(\theta) \label{eq:L1} -->
<!-- \end{eqnarray} -->
<!-- Following @Lassiter2013, we model the pragmatic listener as having uncertainty about the threshold: $P(\theta)$. -->
<!-- Though the listener doesn't know the threshold *a priori*, she resolves the threshold (i.e., the meaning) in context by integrating her prior beliefs $P(x)$ with the basic principles of communication to be truthful and informative, instantiated in her model of the speaker $S_1$ [@Lassiter2015; @GoodmanLassiter]. -->
<!-- In principle, thresholds could be learned over time for different contexts, but here we assume the listener has no informative knowledge about the semantic variable $P(\theta) = \text{Uniform([0, 1])}$. -->
<!-- The pragmatic listener $L_1$ (Eq. \ref{eq:L1}) is thus a model of *generic interpretation*: Upon hearing a generic, how should a listener update her beliefs? -->



# Case Study 1: Generic Language

Generalizations about categories (i.e., *generic language*; e.g., *Birds fly.*) have received substantial interest in psychology for their role in concept and theory formation [e.g., @Gelman2004], stereotype propogation [@Rhodes2012], and motivation [@Cimpian2007], as well as many other facets of everyday cognition.
It is the premier case study of generalizations in language because of its close relation to quantified statements (e.g., *Most birds fly*) and has received the most attention from philosophers, linguists, and psychologists.

Any theory of generic language must explain its puzzling flexibility of usage with respect to prevalence.
That is, \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} are reasonable things to say, but \emph{Birds are female} is not.
As a first test of our theory, we wish to explain the flexibility in endorsements of generics (e.g., "Birds lay eggs" vs. "Birds are female"; "Mosquitos carry malaria") in terms of the relevant prior knowledge about properties, as explored through simulation in the **Model predictions** section.
We validate these simulations empirically by measuring the prior belief distribution over the relevant probabilities $P(h)$ used in Eq. \ref{eq:L0} as well as the target-category prevalence $h$ that the speaker model seeks to convey in Eq. \ref{eq:S1} (Expt. 1a).

<!-- The pragmatic speaker model $S_2$, Eq.~\ref{eq:S2}, is a model of truth judgments.  -->
We test our model on thirty generic sentences that cover a range of conceptual distinctions discussed in the literature [@Prasada2013]: characteristic (e.g. \emph{Ducks have wings.}), minority (e.g. \emph{Robins lay eggs.}), striking (e.g. \emph{Mosquitos carry malaria.}), false generalization (e.g. \emph{Robins are female.}), and false (e.g. \emph{Lions lay eggs.}).
In additional to the canonical cases from the linguistics literature, we selected sentences to elicit the full range of acceptability judgments (intuitively: "acceptable", "unacceptable", and "indeterminate") with low-, medium-, and high-prevalence properties. 

To compare the model to empirical truth judgments, we measure the prior distribution over prevalence of these properties (Expt. 1a).
We hypothesize that these prior distributions encode probabilities associated with conceptual distinctions [@Prasada2013].
In particular, we hypothesize that different kind--property relations [e.g., *principled* v. *accidental relations*^[This is sometimes referred to as "statistical" relations.]; @Prasada2013] can be observed in the prevalence prior. 
If, in fact, the prevalence prior is a manifestation of deeper, conceptual knowledge, we would expect the prevalence prior data to follow a multi-modal, mixture distribution where there exist qualitatively distinct modes or components to the prior, as probabilistic traces of the different types of kinds with respect to the property.
This is important because it is one way in which our formal theory connects to other psychological accounts of generic language [cf., @Prasada2013; @Gelman2003; @Leslie2007].
As a simple test of this hypothesis---that the prevalence prior encodes conceptual knowledge---we compare a structured (i.e., mixture) model of the prior data to an unstructured one.

After having validated a measure of the prevalence prior, we consider the issue of primary theoretical interest: How well our *uncertain threshold* hypothesis accounts for endorsements of generic sentences (Expt. 1b).
In order to better understand the contribution of our theoretical framework, we evaluate alternative models that have either been previously analyzed in the generics literature or that are simpler forms of our *uncertain threshold* RSA model. 
To preview our results, we find that only our formulation of an uncertain threshold RSA model is able to account for the flexibility in generic endorsement.


<!-- At first glance, generics feel like universally-quantified statements as in *All* birds fly.  -->
<!-- Unlike universals, however, generics are resilient to counter-examples (e.g., *Birds fly* seems true even in the face of flightless birds like kiwis or penguins). -->
<!-- *Most* birds fly seems more reasonable, and yet most mosquitos *do not* carry malaria while *Mosquitos carry malaria* seems like a true statement. -->
<!-- Finally, the probability of a bird being female is much higher than the probability of a mosquito carrying malaria, but *Birds are female* seems weird to say. -->
<!-- *Birds lay eggs*, however, is fine, even though the rate is the same as *are female* (only females lay eggs). -->

<!-- Empirical work done on category generalizations has elucidated another puzzling phenomenon. -->
<!-- Generic statements often imply the property is widespread in the category [e.g., *Swans have hollow bones* implies something much stronger than *Some swans have hollow bones*; @Gelman2002; @Cimpian2010; @Brandone2014]. -->
<!-- This characteristically strong interpretation interacts with the indeterminacy in truth conditions, resulting in an *asymmetry* between endorsement and interpretation. -->
<!-- For a variety of features, both children and adults infer that a property is *more* prevalent when told the generalizations than when judging it true or not [@Cimpian2010; @Brandone2014]. -->







<!-- Interpreting the generic as meaning "most" (i.e. \emph{Most swans are white}) captures many cases, but cannot explain why \emph{Robins lay eggs} and \emph{Mosquitos carry malaria} are so intuitively compelling: Only adult female robins lay eggs and a very tiny fraction of mosquitos actually carry malaria. -->
<!-- Indeed, it appears that any \emph{truth condition} stated in terms of how common the property is within the kind violates intuitions. -->
<!-- Consider the birds: for a bird, being female practically implies you will lay eggs (the properties are present in the same proportion), yet we say things like \emph{Birds lay eggs} and we do not say things like \emph{Birds are female}. -->


<!-- One case study in the language of generalizations is about category generalizations. -->
<!-- Category generalizations, called *generic statement*, has been studied extensively in cognitive and developmental psychology: *generic* statements, or generalizations about categories. -->


<!-- These *strong implications* might underly stereotype formation in social categories [@Rhodes2012]. -->
<!-- In addition to the flexibility in truth conditions, two phenomena have been noted.  -->
<!-- The case of *Mosquitos carry malaria* suggests the utterance must in some way be analogous to the quantifier "some" (e.g., *Some mosquitos carry malaria*).  -->
<!-- Generic language, however, is often interpreted as implying the property is widespread: *Swans have hollow bones* means something different than *Some swans have hollow bones* [@Gelman2002]. -->

<!-- These three phenomena---flexible truth, strong interpretations, and evidence exaggeration---have confounded all formal models aimed to capture the meaning of generalizations in language. -->
<!-- Informal theories have argued that the nature of concepts and their relations are inextricably tied to their linguistic expression [@Leslie2007; @Prasada2013]. -->
<!-- Formal theories have limited themselves to a measurable quantitative degree (e.g., how prevalent a property is in a category), which, in isolation, is difficult to reconcile with the extreme flexibility of generic language. -->
<!-- Here, we present a theory formalizing the inuition that the core meaning of a linguistic generalization is simple, but underspecified, and that general principles of communication may be used to resolve the precise meaning in context.  -->
<!-- This theory inherently relies upon measurable quantities (i.e., prevalence) but situates it within a Bayesian agent with potentially structured prior knowledge, providing a bridge to conceptual theories.  -->
<!-- We find that this formalism (and not simpler ones) explains all three empirical puzzles. -->

<!-- When interpretations are compared directly against truth conditions (i.e., how prevalent a property would need to be for the generic to be true),  -->

<!-- Three empirical phenomena make generalizations in language difficult to formalize. -->
<!-- The conditions under which a generalizations becomes "true" are variable: -->


<!-- Existing work in both adults and children suggests that choosing to communicate a generalization can be used to exagerate evidence [@Cimpian2010; @Brandone2014]. -->

<!-- In addition to having manifold interpretations, what makes linguistic generalizations true or false (i.e., their truth conditions) is also a mystery. -->
<!-- The statement *Birds lay eggs* is true even though only about half of them do (namely, the females). -->
<!-- Half of birds are also female, and yet *Birds are female* sounds like a strange thing to say. -->
<!--  while *Canadians are right-handed* is also weird, even though the vast majority of them are.  -->

## Experiment 1a: Measuring the Prevalence Prior for Categories


The prior $P(h)$ in Eq. \ref{eq:L0} describes the belief distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across relevant categories. 
To get an intuition for the kind of knowledge encoded in this belief distribution, consider the following thought experiment: You are on an alien planet and you come across your favorite kind of animal from Earth. Now try to predict whether or not that animal is female. To get an intuition for your subjective probability, ask yourself "What percentage of that kind of animal is female?".
The answer is probably 50\%. 
Now try to predict whether or not that animal lays eggs (or, "What percentage of that kind of animal lays eggs?").
Depending on what your favorite animal is, the answer is probably either 50\% or 0\% (e.g., if your favorite animal is a dog, there is a 0\% chance that it lays eggs)^[
The subjective probability may, in fact, be non-zero but very small. This would allow for the intuitive possibility of a dog that lays eggs potentially due to some strange generic mutation or the interaction of dog genetics with the alien environment. 
].

This decomposition of the prevalence prior $P(h)$ into a prior on kinds $P(k)$ and then a conditional probability of the prevalence of the feature given the kind $P(h \mid k)$ is one way to measure the prevalence prior $P(h) = \int_{k} P(h \mid k) \diff k$.
We measured this distribution by employing this technique empirically for the set of properties (e.g. \textsc{lays eggs, carries malaria}; 21 in total) used in our target sentences. 
We then ask whether a structured, prior model accounts better for this data than an unstructured model.
 
### Method

#### Participants

We recruited 60 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).  
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating (the same criteria apply to all experiments reported). 
3 participants were accidentally allowed to complete the experiment for a second time; we excluded their second responses (resulting in $n=57$).
2 participants self-reported a native language other than English; removing their data ($n=55$) has no effect on the results reported. 
The experiment took about 10 minutes and participants were compensated \$1.00.

#### Procedure and materials

On each trial of the experiment, participants filled out a table where each row was an animal category and each column was a property. 
Participants first were shown six animal categories randomly sampled from a set corresponding to the generic sentences used in Expt. 1b (e.g. \textsc{robins, mosquitos}) and asked to generate five of their own.

A column then appeared to the right of the animal names with a property in the header (e.g., "lays eggs").
Participants were asked to fill in each row with the percentage of members of each of the species that had the property by giving a number (e.g., "50\%").
Eight property--columns appeared in the table, and this whole procedure was repeated 2 times.
In total, each participant generated 10 animal names and reported on the prevalence of sixteen properties for 22 animals (their own 10 and the experimentally-supplied 12). 
<!-- For a full list of the properties, and generic sentences used in Expt. 1b, see Table 2 (Appendix). -->

### Bayesian analysis

To create a larger set of properties, we reverse-code responses for five properties to create their corresponding negative properties (e.g., we create a property "doesn't have beautiful feathers" by subtracting from 100\% the responses for "has beautiful feathers").
In analyzing the prevalence prior data, we are primarily interested in exploring the hypothesis that the prior is structured as a result of deeper conceptual knowledge.
We formalize this hypothesis using a simple Bayesian mixture model. 
We test the simplest "structural" hypothesis, where there are two, qualitatively different components to the prior [e.g., if participants believe that some kinds have a causal mechanism that *could* give rise to the property, while kinds others do not; cf., @Griffiths2005].
The "unstructured" alternative hypothesis is that there is just a single distribution that gives rise to the prevalence prior data.

For the structural hypothesis, we assume the data was generated from one of two distributions: a distribution corresponding to those kinds with a stable causal mechanism that *could* give rise to the property ($\mathcal{D_{stable}}$) and a "transient cause" distribution corresponding to those kinds without a stable mechanism ($\mathcal{D_{transient}}$).
The "transient" distribution intuitively corresponds to accidental causes of the feature (e.g., a lion, who through some genetic mutation, reproduces by laying eggs).
We model this distribution as a Beta distribuition that heavily favors probabilities near 0: $\text{Beta}(\gamma = 0.01, \delta = 100)$.^[
Note that we use the alterantive mean $\gamma$ and concentration $\xi$ (or, inverse-variance) parameterization of the Beta distribution rather than the canonical shape (or pseudocount) parameterization for ease of posterior inference. The shape parameterization can be recovered using: $\alpha = \gamma \cdot \xi; \beta = (1 - \gamma) \cdot \xi$.
]
The "stable" distribution is modeled as a Beta distribution with unknown parameters $\text{Beta}(\gamma, \xi)$.^[
Because the Beta distribution is not defined at the points 0 and 1, we add $\epsilon$ to the 0 responses and round 1 to 0.99.
Adjusting 1 to $1- \epsilon$ leads to improper inferences for this model, as $1 - \epsilon$ is only likely under a highly right-skewed distribution; this would disproportionately influence the shape of $D_{stable}$. 
This problem does not appear for 0 being adjusted to $\epsilon$ because the "transient" distribution already expects such low values.
Similar results can be obtained by rounding 0 to 0.01. 
Alternatively, the "transient" distribution could be defined as a Delta distribution at 0, and 0 responses could remain in their raw form.
]
Finally, we assume that these two components combine with mixture weighting $\phi$ such that the data we observe is $$P(d) = \phi\cdot \text{Beta} (d \mid \gamma, \xi) + (1 -  \phi) \cdot \text{Beta}(d \mid \gamma = 0.01, \xi = 100) $$.
The unstructured alternative hypothesis assumes the data was generated by a single component in the prior distribution with unknown parameteres $P(d) = \text{Beta} (d \mid \gamma, \xi)$.

We test which hypothesis better accounts for the data by computing a Bayes Factor, which compares the relative explanatory power of each model while taking into account model flexibility [@LW2014].
We put the following priors over the latent parameters of each model (note that the unstructured model does not use $\phi$):

\begin{eqnarray}
\phi & \sim & \text{Uniform}(0, 1) \\
\gamma & \sim & \text{Uniform}(0, 1) \\
\xi & \sim & \text{Uniform}(0, 100) \\
\end{eqnarray}

Each property (e.g., "lays eggs", "carries malaria") is modeled independently (i.e., each property's prevalence prior has its own parameters). 

### Results

```{r generic-endorsement-priors}
# this data set already excludes the 2nd attempt of 3 participants who completed the experiment a second time

d.gen.endorse.priors <- read.csv(paste(project.path, 
                                       "data/generics/endorsement/",
                                       "naturalGenerics-prior-trials-n57.csv", sep = ""))
gen.endorse.properties <- levels(d.gen.endorse.priors$Property)

genericEndorsementPriorModelHelpers <- '
var eps = 0.01;//Number.EPSILON;
var log = function(x){ return Math.log(x) }
var exp = function(x){ return Math.exp(x) }

var avoidEndPoints = function(x){
  x == 0 ? eps : 
  x == 1 ? 1 - eps : 
  x
}

var betaShape = function(params){
  return {
      a: params.g * params.d,
      b : (1-params.g) * params.d
  }
}

var preprocessedResponses = map(function(d){
  return avoidEndPoints(d / 100)
}, data)
'


structuredPriorModel <- '
var model = function(){
  var phi = uniformDrift({a:0, b: 1, width:0.2});
  var g = uniformDrift({a:0, b: 1, width:0.2});
  var d = uniformDrift({a:0, b: 100, width:5});
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  var transientComponent = Beta({a:1, b:100})
  mapData({data: preprocessedResponses}, function(d){
    factor( log(
      phi * exp(stableComponent.score(d)) +
      (1 - phi) * exp(transientComponent.score(d))
    ))
  })
  return {g, d, phi}
}
'

unstructuredPriorModel <- '
var model = function(){
  var g = uniformDrift({a:0, b: 1, width:0.2});
  var d = uniformDrift({a:0, b: 100, width:5});
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  mapData({data: preprocessedResponses}, function(d){
    factor(stableComponent.score(d))
  })
  return {g, d}
}
'
genericEndorsementPriorModel <- paste(genericEndorsementPriorModelHelpers, structuredPriorModel, sep = "\n")

```

```{r generic-endorsement-priors-bayesFactors, eval = F}
genericEndorsementPriorModelPriorLikelihoodHelpers <- '
var properties = _.uniqBy(_.map(data, "Property"));

var eps = 0.01;//Number.EPSILON;
var log = function(x){ return Math.log(x) }
var exp = function(x){ return Math.exp(x) }

var avoidEndPoints = function(x){
  x == 0 ? eps : 
  x == 1 ? 1 - eps : 
  x
}

var betaShape = function(params){
  return {
      a: params.g * params.d,
      b : (1-params.g) * params.d
  }
}
'

genericEndorsementPriorModelsPriorLikelihood <- '
var transientComponent = Beta({a:1, b:100})

var model = function(){

  var propertyLogLikes = map(function(property){

    var propertyData = _.map(_.filter(data, {Property: property}), "prevalence");
    var preprocessedResponses = map(function(d){
        return avoidEndPoints(d / 100)
    }, propertyData);

    var phi = uniformDrift({a:0, b: 1, width:0.2});
    var g = uniformDrift({a:0, b: 1, width:0.2});
    var d = uniformDrift({a:0, b: 100, width:5});
    var stableParams = betaShape({g, d})
    var stableComponent = Beta(stableParams)

    var logLikes = map(function(d){
      var structuredLL = log(
        phi * exp(stableComponent.score(d)) +
        (1 - phi) * exp(transientComponent.score(d))
      )

      var unstructuredLL = stableComponent.score(d);

      return {structuredLL, unstructuredLL}
    }, preprocessedResponses)

    return {
      structuredLL: sum(_.map(logLikes, "structuredLL")), 
      unstructuredLL: sum(_.map(logLikes, "unstructuredLL"))
    }

  }, properties)

  return {
    structuredLL: sum(_.map(propertyLogLikes, "structuredLL")), 
    unstructuredLL: sum(_.map(propertyLogLikes, "unstructuredLL"))
  }
}
'

```

```{r generic-endorsement-priors-bayesFactorsRun, eval = F}
marginal.prior.likelihoods <- data.frame()

# 100 samples takes 30s
m.gen.endorse.priors.logLike <- webppl(
    paste(genericEndorsementPriorModelPriorLikelihoodHelpers,
          genericEndorsementPriorModelsPriorLikelihood,
          sep = '\n'), 
    data = d.gen.endorse.priors, data_var = "data",
    model_var = "model", 
    inference_opts = list(method = "forward", samples = 1000),
    chains = 3, cores = 3
) %>% 
  mutate(value = as.numeric(value))

  
min.ll.1 <- min(
  m.gen.endorse.priors.logLike$value[
    is.finite(
      m.gen.endorse.priors.logLike$value
      )
    ]
  )

m.gen.endorse.priors.logLike <- m.gen.endorse.priors.logLike %>% 
  rowwise() %>%
  mutate(value = ifelse(value == -Inf, min.ll.1, value))
  
save(m.gen.endorse.priors.logLike, file = '../analysis/generics-priors-loglike.Rdata')
```

```{r generic-endorsement-priors-bayesFactorsLoad}
load(file = '../analysis/generics-priors-loglike.Rdata')
marginal.prior.likelihoods <- m.gen.endorse.priors.logLike %>%
  group_by(Chain, Parameter) %>%
  summarize(mll  = logmeanexp(value)) 

marginal.prior.likelihoods <- marginal.prior.likelihoods %>% 
  spread(Parameter, mll) %>%
  mutate(logBF = structuredLL - unstructuredLL, BF = exp(logBF))

roundedBF1 = round(mean(marginal.prior.likelihoods$logBF)/ 10000)*10000
```

The Bayes Factor indicates the extent to which the data support one model over another.
The overall Bayes Factor in terms of structured model (S) over the unstructured model (U) for the full prevalence prior data set is approximately $\log BF_{SU} = `r roundedBF1`$, meaning that the structured hypothesis is approximately `r roundedBF1` orders of magnitude more likely than the unstructured hypothesis to account for the prevalence prior data.
Indeed, the data strongly suggests the prevalence prior is structured.

It is possible that the data is, in fact, more structured than our simple two-component model would allow. 
To see how well the structured model fits the prevalence prior data, we use Bayesian inference to infer the likely values of the parameters and then use the inferred parameters to generate new data.
This is called the *posterior predictive distribution* and is an important step in model validation. 
If the model is a good representation of the data, the posterior predictive data will align with the observed experimental data. 
We construct a posterior predictive distribution by "forward sampling" the model.^[
This can be described by the following algorithm: First, flip a coin weighted by $\phi$.
If it comes up heads, we then sample from the "stable" component: $\text{Beta}(\gamma, \xi)$.
If it comes up tails, we sample from the "transient" component: $\text{Beta}(0.01, 100)$.
We do this many times using the posterior distibution to generate a distribution over predicted prevalence ratings.
]

We implemented this statistical model using the probabilistic programming language WebPPL [@dippl].
To learn about the credible values of the parameters, we ran separate MCMC chains for each item, collecting 75,000 samples, removing the first 25,000 for burn-in.
Eight example prior distributions inferred using both the structured and unstructured models, together with the raw empirical distributions, are shown in Figure \ref{fig:generic-endorsement-priors-figure}.
There is a lot of diversity in the shapes of the distributions, which the structured model accounts for quite well.
One item that the structured model doesn't perfectly capture is the prior distribution over "lays eggs".
The empirical distribution is tri-modal, with reliable modes at 0\%, 50\%, and 100\%^[
Similar phenomena have been observed in other prevalence elicitation tasks when the target category is fixed [e.g., estimating only the percentage of Robins that lay eggs; @Prasada2013]
]; our simple two-component mixture model has no way to account for such a tri-modal distribution. 
A more complex model (i.e., one with three mixture components) is necessary to perfectly account for this item's data; however, we retain the imperfect simpler model for data-analytic ease going forward. 

```{r generic-endorsement-priors-savageDickeyModel, eval = F}
savageDickeyModel <-'
var model = function(){
  var phi = uniformDrift({a:0, b: 1, width:0.2});
  var g = uniformDrift({a:0, b: 1, width:0.2});
  var d = uniformDrift({a:0, b: 100, width:5});
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  var transientComponent = Beta({a:1, b:100})
  mapData({data: preprocessedResponses}, function(d){
    factor( log(
      phi * exp(stableComponent.score(d)) +
      (1 - phi) * exp(transientComponent.score(d))
    ))
  })
  return {phi}
}

//var samples = data.samples[0], burn = data.burn[0], lag = data.lag[0];
// var samples = 2000, burn = burn / 2, lag = 0;
var modelPosterior = Infer({method: "MCMC", samples:10000, burn:5000, lag:0, 
verbose: true}, model)
var modelPrior = Infer({method: "forward", samples:10000}, model)

var pointNull = 1;

var logBFs = map(function(diff){
  var savageDickeyDenomenator = expectation(modelPrior, function(x){return Math.abs(x.phi-pointNull)<diff})
  var savageDickeyNumerator = expectation(modelPosterior, function(x){return Math.abs(x.phi-pointNull)<diff})
  var savageDickeyRatio = savageDickeyNumerator / savageDickeyDenomenator
  return [diff, Math.log(savageDickeyRatio)]
}, [0.01, 0.02, 0.03, 0.04, 0.05])

var returnVals = {
  prior: modelPrior, posterior: modelPosterior, logBFs: logBFs
}
returnVals
'
```

```{r generic-endorsement-priors-bayesFactors-supermodel, eval= F}
genericEndorsementPriorSupermodel <- '
var model = function(){
  var alpha = uniformDrift({a:0, b: 1, width:0.2});
  //var structured = flip();
  var phi = uniformDrift({a:0, b: 1, width:0.2});
  var g1 = uniformDrift({a:0, b: 1, width:0.2});
  var d1 = uniformDrift({a:0, b: 100, width:5});
  // var g0 = uniformDrift({a:0, b: 1, width:0.2});
  // var d0 = uniformDrift({a:0, b: 100, width:5});
  var stableParams1 = betaShape({g: g1, d: d1})//, 
      //stableParams0 = betaShape({g: g0, d: d0});
  var stableComponent1 = Beta(stableParams1)//,
      //stableComponent0 = Beta(stableParams0);

  var transientComponent = Beta({a:1, b:100})
  mapData({data:preprocessedResponses},
    function(d){
    var scr1 = log(phi * exp(stableComponent1.score(d)) +
                    (1 - phi) * exp(transientComponent.score(d)))
    var scr0 = stableComponent1.score(d);
    // display(exp(scr1) + " " + exp(scr0));
    var combined_ll = log(alpha * exp(scr1) + (1 - alpha) * exp(scr0));
    // factor(structured ? scr1 : scr0)
    factor(combined_ll)
  })
  return alpha
}
'
```

```{r generic-endorsement-priors-bayesFactor-supermodelRun, cache = T, eval = F}
for (p in gen.endorse.properties){
  priorData <- filter(d.gen.endorse.priors, Property == p)$prevalence
  
  m.gen.endorse.priors.supermodel.posterior <- webppl(
    paste(genericEndorsementPriorModelHelpers, genericEndorsementPriorSupermodel, sep = '\n'), 
    data = priorData, data_var = "data",
    model_var = "model",  chains = 2,
                                 cores = 2,
                                 inference_opts = 
                                   list(method = "MCMC",
                                        samples = 2000, 
                                        burn = 2000, 
                                        lag = 2, 
                                        verbose = T)
    )
  
  qplot(m.gen.endorse.priors.supermodel.posterior$value)
}
```

```{r generic-endorsement-priors-bayesFactors-enumerate, eval = F}
genericEndorsementPriorModelPriorLikelihoodEnumerate <- '
var eps = 0.00000001;

var model = function(){
  var phi = uniformDraw(_.range(eps, 1-eps, 0.1))
  var g = uniformDraw(_.range(eps, 1-eps, 0.05))
  var d = uniformDraw(_.range(eps, 100, 2))
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  var transientComponent = Beta({a:1, b:100})
  var logLike = sum(map(function(d){
    var scr = log(
      phi * exp(stableComponent.score(d)) +
      (1 - phi) * exp(transientComponent.score(d))
    )
    return scr
  }, preprocessedResponses))
  return logLike
}
'
genericEndorsementUnstructuredModelPriorLikelihoodEnumerate <- '
var eps = 0.00000001;

var model = function(){
  var g = uniformDraw(_.range(eps, 1-eps, 0.2))
  var d = uniformDraw(_.range(eps, 100, 2))
  var stableParams = betaShape({g, d});
  var stableComponent = Beta(stableParams);
  var logLike = sum(map(function(d){
    return stableComponent.score(d)
  }, preprocessedResponses))
  return logLike
}
'
```

```{r generic-endorsement-priors-bayesFactorsRunEnumerate, cache = T, eval =F}
marginal.prior.likelihoods <- data.frame()

for (p in gen.endorse.properties){
  priorData <- filter(d.gen.endorse.priors, Property == p)$prevalence
  
  m.gen.endorse.priors.logLike.enum1 <- webppl(
    paste(genericEndorsementPriorModelHelpers,
          genericEndorsementPriorModelPriorLikelihoodEnumerate, sep = '\n'), 
    data = priorData, data_var = "data",
    model_var = "model", 
    inference_opts = list(method = "enumerate")
    )
  
  m.gen.endorse.priors.logLike.enum0 <- webppl(
    paste(genericEndorsementPriorModelHelpers,
          genericEndorsementUnstructuredModelPriorLikelihoodEnumerate, sep = '\n'), 
    data = priorData, data_var = "data",
    model_var = "model", 
    inference_opts = list(method = "enumerate")
    )

  marginal.prior.likelihood1 <- logmeanexp(m.gen.endorse.priors.logLike.enum1$support)
  marginal.prior.likelihood0 <- logmeanexp(m.gen.endorse.priors.logLike.enum0$support)

  marginal.prior.likelihoods <- bind_rows(
    marginal.prior.likelihoods, 
    data.frame(Property = p, structuredLL = marginal.prior.likelihood1, unstructuredLL = marginal.prior.likelihood0)
  )
}

marginal.prior.likelihoods <- marginal.prior.likelihoods %>% mutate(logBF = structuredLL - unstructuredLL, BF = exp(logBF))

```

```{r generic-endorsement-priors-model-run, eval = F}
m.gen.endorse.priors.posteriorPredictive <- data.frame()
n_samples <- 1000
lg = 0
example.generics.prior.properties <- c(
  "dont eat people", "have beautiful feathers",
  "have wings", "are red",
  "carry malaria", "lay eggs", 
  "are female", "have spots"
  )


for (p in example.generics.prior.properties ) {
  priorData <- filter(d.gen.endorse.priors, Property == p)$prevalence
  
    m.gen.endorse.priors.structured <- webppl(
      paste(genericEndorsementPriorModelHelpers, structuredPriorModel, sep = ""),
                                 data = priorData, data_var = "data",
                                 model_var = "model",
                                 chains = 2,
                                 cores = 2,
                                 inference_opts =
                                   list(method = "MCMC",
                                        samples = n_samples,
                                        burn = n_samples / 2,
                                        lag = lg,
                                        verbose = T))
    
    m.gen.endorse.priors.structured.predictive <- m.gen.endorse.priors.structured %>%
      group_by(Iteration, Chain) %>%
      spread(Parameter, value) %>%
      rowwise() %>%
      mutate(
        a = g*d,
        b = (1-g)*d,
        isPresent = rbinom(1, 1, prob = phi),
        prevalence = ifelse(isPresent == 1, 
                           rbeta(n = 1, 
                                 shape1 = a,
                                 shape2 = b),
                           rbeta(n = 1,
                                 shape1 = 1,
                                 shape2 = 100)),
        Property = p, src = "Structured"
      ) %>% ungroup()
    
    m.gen.endorse.priors.unstructured <- webppl(
      paste(genericEndorsementPriorModelHelpers, unstructuredPriorModel, sep = ""),
                                 data = priorData, data_var = "data",
                                 model_var = "model",
                                 chains = 2,
                                 cores = 2,
                                 inference_opts =
                                   list(method = "MCMC",
                                        samples = n_samples,
                                        burn = n_samples / 2,
                                        lag = lg,
                                        verbose = T))
    
    m.gen.endorse.priors.unstructured.predictive <- m.gen.endorse.priors.unstructured %>%
      group_by(Iteration, Chain) %>%
      spread(Parameter, value) %>%
      rowwise() %>%
      mutate(
        a = g*d,
        b = (1-g)*d,
        prevalence = rbeta(n = 1, 
                                 shape1 = a,
                                 shape2 = b),
        Property = p, src = "Unstructured"
      ) %>% ungroup()
    
    
    m.gen.endorse.priors.posteriorPredictive <- bind_rows(
      m.gen.endorse.priors.posteriorPredictive,
      bind_rows(
       m.gen.endorse.priors.structured.predictive %>%
          select( src, Property, prevalence),
      m.gen.endorse.priors.unstructured.predictive %>%
          select( src, Property, prevalence),
      data.frame(Property = p,
                 src = "Data",
                 prevalence = priorData/100)
      )
    )
    
    
  # for (c in 1:2){
  #     m.gen.endorse.priors.savage <- webppl(
  #   paste(genericEndorsementPriorModelHelpers, 
  #         savageDickeyModel, sep = '\n'), 
  #   data = priorData, 
  #   data_var = "data"
  #   )
  
  # m.inferred.prior <- get_samples(
  #   data.frame(m.gen.endorse.priors.savage$prior) %>%
  #     rename(prob = probs), n_samples)
  # 
  # m.inferred.posterior <- get_samples(
  #   data.frame(m.gen.endorse.priors.savage$posterior) %>%
  #     rename(prob = probs), n_samples)
  # 
  # posterior.density <- density(m.inferred.posterior$phi,
  #         from = 0, to = 1)
  # 
  # prior.density <- density(m.inferred.prior$phi,
  #         from = 0, to = 1)
  # 
  # logBF01 = log(approxfun(posterior.density)(1)) - 
  #   log(approxfun(prior.density)(1))



  
  # m.gen.endorse.priors.savage.summary <- bind_rows(
  #   m.gen.endorse.priors.savage.summary,
  #     bind_rows(
  #     data.frame(Property = p, 
  #                Chain = c, 
  #                diff = NA,
  #                logBF01 = logBF01,
  #                BF01= exp(logBF01)),
  #     data.frame(m.gen.endorse.priors.savage$logBFs) %>%
  #       rename(diff = X1, logBF01 = X2) %>%
  #       mutate(Property = p, Chain = c, BF01 = exp(logBF01))
  #     )
  # )
  # 
  # }



    
    #for (c in 1:3){

      # posterior.logspline <-  polspline::logspline(filter(
      #   m.gen.endorse.priors, Parameter == 'phi', Chain == c)$value,
      #   lbound = 0, ubound = 1)
      # 
      #   posterior <- dlogspline(1, posterior.logspline)
      # prior <- dunif(1, min = 0, max = 1)
      # BF01 <- posterior/prior
      # 
      # print(log(BF01))

  #}



  #print(p)
  #print(priorData)
}
save(m.gen.endorse.priors.posteriorPredictive, file = '../analysis/generics-priors-posteriorPredictives.Rdata')

```

```{r generic-endorsement-priors-figure, fig.width = 7, fig.height = 5, fig.cap="Cumulative density plots for prior distributions representing the prevalence of eight different features. Distributions are the posterior predictive distributions for the Structured and Unstructured prior models, as well as the raw empirical distribution. A completely uniform distribution would be represented as the y = x line."}
load('../analysis/generics-priors-posteriorPredictives.Rdata')
ggplot(m.gen.endorse.priors.posteriorPredictive, 
           aes( x = prevalence, color = src, lty = src))+
      #geom_density(aes(y = ..scaled..), adjust = 1, size = 1)+
    #scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    stat_ecdf(size = 1)+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_alpha_manual(values = c(0.6, 0.4, 0))+
    facet_wrap(~Property, nrow = 2)+
    scale_linetype_manual(values = c(1, 4, 3))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
  coord_fixed()+
    xlab("Prevalence") +
    ylab("Cumulative probability density")+
    theme(legend.position = "bottom", legend.title = element_blank())

```


```{r generic-endorsement-priors-figure2, eval = F}
ggplot(m.gen.endorse.priors.summary %>% filter(Parameter == 'phi') %>%
         mutate(Property = factor(Property, levels = Property[order(MAP)]) ),
       aes( x = Property, y = MAP, ymin = cred_lower, ymax = cred_upper))+
  geom_bar(stat = 'identity', position = position_dodge())+
  geom_errorbar(position = position_dodge())+
  coord_flip()+
  ylab("MAP estimate and 95% HDI of Mixture parameter")
```

### Discussion

The prevalence prior distribution $P_f(h)$ encodes beliefs about the prevalence (or, probabilities) of a feature $f$ across different kinds.
These distributions are structured and must be represented by a model with at least two distinct generative components: one corresponding to those kinds that only accidentally or transiently have the feature and one corresponding to those kinds with a stable causal mechanism that gives rise to the feature with some probability.

<!-- The knowledge encoded by the prevalence prior distribution is similar to another theoretical construct discussed in the generics literature: cue validity. -->

If prevalence is the "forward probability" (e.g., one's predictions about whether or not an entity will lay eggs after learning that the entity is a robin), the inverse probability---$P(k \mid f)$ (e.g., one's predictions about whether or not the entity is a robin, upon learning that it lays eggs)---is known as *cue validity*.
For example, if one learns that an entity carries malaria, that entity is probably a mosquito because only mosquitos carry malaria; hence "carrying malaria" has high cue validity for mosquitos. 

A natural proposal is that endorsements for generic language have to do with cue validity.
This possibility has been investigated empirically, and in fact, cue validity is highly correlated with generic endorsement [@Khemlani2012].
However, cue validity has some empirical and theoretical issues.
Empirically, cue validity is not a necessary criteria for generic endorsement (e.g., *Dogs have four legs*).
Theoretically, it's not obvious why cue validity would be part of the semantic content of generics.
If generics are for conveying generalizations and supporting prediction, listeners would be far better suited to have information that supports prediction about features given categories, not categories given features.
<!-- , since knowledge of cue validity requires knowledge of the kind whereas the forward-probability (prevalence)  can be computed only based on the feature. -->
<!-- ^[ -->
<!-- As a simple demonstration, imagine we've just discovered a new kind of animal called a fep. -->
<!-- It should be easy to estimate the prevalence of being female among feps (probably 50\%) even without any knowledge of feps.  -->
<!-- However, if you learn in another context that a particular entity is green, it's impossible to estimate the cue validity of being green for feps -->
<!-- that it's easy to estimate the prevalence of being female among  -->
<!-- ] -->

However, we shouldn't throw away the possibility that distributional information about the property is relevant. 
The mixture parameter $\phi$ in the prevalence prior distribution can be seen as a generalization of cue validity. 
Recall that $\phi$ is the weighting between the two generative components of the prevalence prior (i.e., the stable cause distribution $\mathcal{D_{stable}}$ and the transient cause distribution $\mathcal{D_{transient}}$).
If there is a finite number of kinds in the prevalence prior for a feature, and one knows the prevalence of the feature for each kind, then the cue validity of that feature for a particular kind is encoded in the prevalence prior, in that it can be derived via Bayes' Rule:
$$ P(k \mid f) = \frac{P(f \mid k) \cdot P(k)}{\sum_{k' \in K} P(f \mid k') \cdot P(k')} $$

Alternatively, if the number of kinds in prior is not fixed (e.g., if it is the product of a generative model of kinds), cue validity is not longer defined (or, it is zero for all kinds). 
This is for the same reason that for a continuous probability distribution, the probability of any particular number is zero. 
For example, consider the cue validity of "having wings" for \textsc{robins}: $P(\text{robin} \mid \text{has wings})$.
The feature of "having wings" is somewhat diagnostic for the category of \textsc{birds}, but there are many kinds of birds and the feature is not particularly diagnostic for anyone of them. 
At the same time, the mixture parameter for the distribution over the prevalence of "having wings" is still relatively small, since it represents information about the distribution and not one particular kind.
This is an important point of departure in our work from previous work.

<!-- The prevalence prior distribution, on the other hand, does not require that the set of categories be fixed.  -->
<!-- It in fact may be one dimension of a higher-dimesional generative model of kinds and properties. -->
<!-- Cue validity is conceptually related to the mixture parameter $\phi$ of the prevalence prior. -->
<!-- However, cue validity requires knowledge of the target category $k$ -->

<!-- For example, suppose there are 10 possible kinds and 3 of them belong the stable cause distribution and 7 belong to the transient cause distribution.  -->
<!-- Say we observe the feature and want to know what the probability that the entity is of a particular kind that we know to be of the stable distribution.  -->
<!-- If we know the prevalence given each distribution (i.e, $P(f \mid k_{stable})$ and $P(f \mid k_{transient})$). -->


<!-- For simplicity, assume that each component corresponds to a single probability (i.e., a single prevalence value) as opposed to a distribution. -->
<!-- Thus, $\mathcal{D_{stable}} = P(f \mid \mathcal{D_{stable}}) = p_{stable}$ -->

<!-- $n_{stable} n_{transient}$ -->



<!-- Previous studies have found cue validity to be correlated with generic endorsement [@Khemlani2012], though it's role in generic meaning is controversial [@Leslie2007; @Leslie2008]. -->
<!-- In empirical studies of generic language, however, it is often empirically measured [@Khemlani2012]. -->
<!-- For this analysis, we measured cue validity using a free production paradigm, following @Cree2006. -->
<!-- Participants ($n = 50$) were supplied with a feature (e.g., "X lays eggs") and asked to generate a kind (e.g., "what do you think X is ?"). -->
<!-- For a detailed discussion of the results of this experiment and comparison to other measurements of cue validity, see Appendix B. -->
<!-- To process the priors data, we discretize the prevalence judgments to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$, and look at the counts within each bin, after doing add-1 Laplace smoothing, as the relative probability of that prevalence.  -->
<!-- Do a BDA to describe the data in a more compact form. -->
<!-- We can explore how the underspecified-threshold listener model $L_{0}(x , \theta \mid u)$ (Eq. \ref{eq:L0}) interprets generic utterances with these priors on prevalence (Figure \ref{fig:commongenerics}a, insets).  -->
<!-- The prior beliefs over the prevalence of the property, $P(h)$, can also be interpreted as the pragmatic listener's posterior upon hearing the null utterance, because the null utterance has no information content. -->
<!-- We see the interpretation of the generic is quite variable across our empirically measured priors. -->
<!-- For instance, in the case of \textsc{carries malaria}, the prior is very left-skewed; with this prior, the threshold $\theta$ can plausibly be very low because high prevalence is unlikely. -->
<!-- Properties like \textsc{doesn't attack swimmers} are very right-skewed; with this prior, not many thresholds lead to a substantial update in beliefs, and so the generic is unlikely to be used by speaker $S_1$ unless the property is practically-universal within the target category.  -->
<!-- Some properties have priors that are unimodal with low variance (e.g. \textsc{is female}); these properties are present in every kind in almost exactly the same proportion. The model  and thus are too obvious and certain to allow for an informative generic utterance: The posterior is not very different from the prior.  -->
<!-- With $P(x)$ now empirically established, we can test if our speaker model predicts human truth judgments of generic statements about these properties. -->


## Experiment 1b: Endorsing generic statements

In Expt. 1a, we measured and modeled the prevalence prior distribution (e.g., the prevalence of laying eggs for different kinds of animals) for different features.
This data also includes informaiton about the target-category prevalence (e.g., how many robins lay eggs).
Using the target-category prevalence as $h$ in our speaker model (Eq. \ref{eq:S1}), the speaker model will predict endorsements for the corresponding generic statements (e.g., "Robins lay eggs").
To test these predictions, we collect human endorsements for thirty generic statements taken from the linguistic and psychological literature on generics [@Prasada2013].

#### Method
```{r eval = F}
d1 <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "truth-judgments-n100.csv", sep = ""))
d2 <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "naturalGenerics-trials-formatted.csv", sep = ""))

# write.csv(
#   left_join(
#   d1 %>%
#     mutate(sentence = as.character(sentence),
#            sentence = gsub('&quotechar', '', sentence),
#          sentence = gsub('lyme', 'Lyme', sentence)),
#   unique(d2 %>%
#     mutate(sentence = paste(Category, " ", Property, ".", sep = "")) %>%
#     select(Category, Property, sentence))
# ), file = paste(project.path, "data/generics/endorsement/",
#                     "truth-judgments-n100.csv", sep = ""), row.names = F)
```

```{r generic-endorsement}
d.gen.endorse.catch <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "truth-judgments_catch-trials.csv", sep = ""))

d.gen.endorse <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "truth-judgments-n100.csv", sep = ""))



d.gen.endorse.summary <- left_join(
  d.gen.endorse, 
  d.gen.endorse.catch %>% select(workerid, pass)
  ) %>%
  filter(pass == 1) %>%
  rowwise() %>%
  mutate(response = ifelse(response == "agree-key", 1, 0)) %>%
  group_by(sentence) %>%
  multi_boot_standard(column = "response") %>%
  ungroup() %>%
  mutate(sentence = factor(sentence, levels = sentence[order(mean)]))

d.gen.endorse.bayes <- left_join(
  d.gen.endorse, 
  d.gen.endorse.catch %>% select(workerid, pass)
  ) %>%
  filter(pass == 1) %>%
  rowwise() %>%
  mutate(response = ifelse(response == "agree-key", 1, 0),
         sentence = gsub('&quotechar', '', sentence),
         sentence = gsub('lyme', 'Lyme', sentence)) %>%
  group_by(sentence) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b)) %>% 
  ungroup() %>%
  mutate(sentence = factor(sentence, levels = sentence[order(MAP_h)]))


# d.gen.endorse.summary %>%
#   ggplot(., aes(x = sentence, y = mean, ymin = ci_lower, ymax = ci_upper))+
generics.endorsement.spectrum <- d.gen.endorse.bayes %>%
  #ggplot(., aes(x = sentence, y = MAP_h, ymin = low, ymax = high))+
  ggplot(., aes(x = sentence, y = MAP_h - 0.5, ymin = low - 0.5, ymax = high - 0.5))+
  geom_bar(stat = 'identity', position = position_dodge(),fill = 'grey')+
  geom_linerange(position = position_dodge())+
  #geom_errorbar(position = position_dodge())+
  coord_flip()+
  ylab("Proportion Endorsement")+
  xlab("")+
  #scale_y_continuous(limits = c(0,1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(-0.5,0.5), breaks = c(-0.5, 0, 0.5), labels = c(0, 0.5, 1))#+
  #geom_hline(yintercept = -0.25, lty = 3)+
  #geom_hline(yintercept = 0.25, lty = 3)
```

##### Participants

We recruited 100 participants over MTurk. 
4 participants were excluded for failing to pass a catch trial.
5 participants self-reported a native language other than English; removing their data has no effect on the results reported. 
The experiment took about 3 minutes and participants were compensated \$0.35.

##### Procedure and materials

Participants were shown thirty generic sentences in a randomized order.
They were asked to press one of two buttons (randomized between-participants) to signify whether they agreed or disagreed with the sentence (see Table 2 in Appendix for complete list). 
The thirty sentences were presented in a random order between participants and covered a range of conceptual categories described above.
Approximately 10 true, 10 false, and 10 uncertain truth-value generics were selected.

As an attention check, participants were asked at the end of the trials which button corresponded to "Agree".
4 participants were excluded for failing this trial.

#### Data and model analysis

```{r generic-endorsement-manipulationcheck}
# MHT's truth judgments of sentences
# t --> true; f --> false; i --> indeterminate/uncertain
all.sentences <- list("Cardinals are red." = "t",
 "Ducks have wings." = "t",
 "Kangaroos have pouches." = "t",
 "Kangaroos have spots." = "f",
 "Leopards are juvenile." = "i",
 "Leopards have spots." = "t",
 "Leopards have wings." = "f",
 "Lions are male." = "i",
 "Lions have manes." = "t",
 "Lions lay eggs." = "f",
 "Mosquitos attack swimmers." = "i",
 "Mosquitos carry malaria." = "t",
 "Mosquitos dont carry malaria." = "f",
 "Peacocks dont have beautiful feathers." = "f",
 "Peacocks have beautiful feathers." = "t",
 "Robins are female." = "i",
 "Robins carry malaria." = "f",
 "Robins lay eggs." = "t",
 "Sharks are white." = "i",
 "Sharks attack swimmers." = "t",
 "Sharks dont attack swimmers." = "f",
 "Sharks have manes." = "f",
 "Sharks lay eggs." = "i",
 "Swans are full-grown." = "i",
 "Swans are white." = "t",
 "Ticks carry Lyme disease." = "t",
 "Ticks dont carry Lyme disease."= "f",
 "Tigers dont eat people."= "f",
 "Tigers eat people." = "t",
 "Tigers have pouches."= "f")

mht.truth.judgments <- data.frame(sentence = names(all.sentences),
           truth_judgment = as.vector(unlist(all.sentences)))

d.gen.endorse.manipulation.check.bayes <- left_join(
  left_join(
    d.gen.endorse, 
    d.gen.endorse.catch %>% select(workerid, pass)
    ),
  mht.truth.judgments ) %>%
  filter(pass == 1) %>%
  rowwise() %>%
  mutate(response = ifelse(response == "agree-key", 1, 0),
         sentence = gsub('&quotechar', '', sentence),
         sentence = gsub('lyme', 'Lyme', sentence)) %>%
  group_by(truth_judgment) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b)) %>% 
  ungroup() %>%
  mutate(truth_judgment = factor(truth_judgment, levels = truth_judgment[order(MAP_h)]))

true_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"MAP_h"]]
true_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"low"]]
true_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"high"]]

false_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"MAP_h"]]
false_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"low"]]
false_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"high"]]

uncertain_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"MAP_h"]]
uncertain_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"low"]]
uncertain_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"high"]]

```


As a manipulation check, the first author assigned an \emph{a priori} truth-judgment (true/false/indeterminate) to each stimulus item.
As one would expect, there were substantial differencesin empirical endorsements: true generics were almost universally endorsed (Maximum A-Posteriori estimate and 95\% credible interval of endorsement probability: $`r true_MAP`$ $[`r true_low`, `r true_high`])$;
indeterminate generics were agreed with \emph{less} likely than chance ($`r uncertain_MAP`$ $[`r uncertain_low`, `r uncertain_high`])$) but substantially more than false generics  ($`r false_MAP`$ $[`r false_low`, `r false_high`])$).

In addition to these categorical differences between our thirty generic statements, we find a contiuum of endorsement values (Figure \ref{fig:generics-endorsement-figure} top left).
Such a continuum of judgments is already evidence against any theory that only predicts whether a generic statement is true or false.
Ideally, a complete theory of genericity should be able to explain statements that are endorsed completely, unendorsed completely, and all shades in between.

To try to understand this endorsement data, we articulate a set of simple models that have been proposed in the literature and a reduced-form of our proposed RSA model. 
As the simplest baseline hypothesis, we first explore whether prevalence itself predicts generic endorsement (e.g., does the fraction of \textsc{robins} that \textsc{lay eggs} predict the felicity of *Robins lay eggs*?).
As a secondary alternative hypothesis, we explore whether prevalence and *cue validity* (the probability of a kind given the feature; described in more detail below) predicts generic endorsement of these items.
We fit these models using standard maximum-likelihood techniques; we model uncertainty in the input measurements (e.g., prevalence, cue validity) by bootstrapping those data.

Our underspecified threshold model has access to the full prior distribution on prevalence $P(h)$, from which it infers likely thresholds for the generic. 
The endorsement model is a model of a speaker who is deciding whether or not to say the generic.
There are thus three unique components to this model: (i) the prior distribution $P(h)$, (ii) the speaker model deciding whether or not to produce the generic, and (iii) the uncertain threshold.
As a control model, we use the same infrastructure but assign a fixed semantics to the utterance (i.e., analgous to the quantifier "some", which only rules out the lowest possible degree). 
This control model has both features (i) and (ii) but lacks the uncertainty over the threshold.
This provides a strict test of our *simple but underspecified* hypothesis.

```{r generic-regression-models, cache = T}
d.cv <- read.csv(paste(project.path, "data/generics/endorsement/", "cue-validity-2-freeProduction-trials.csv", sep = ""))

d.target.items <- fromJSON(paste(project.path, "data/generics/endorsement/", "originalStims_wrongDeployment.json", sep = "")) %>%
  mutate(sentence = paste(category, property))


### Preprocessing
# - Force all responses to lower case. 
# - Remove spaces.
# - Fix mosquito mispellings.
# - Count "deertick" as "tick".
# - Remove plural "s".
# 
# Mark if the produced animal matches the generic of interest.

mosquito.mispellings <- c("mosqu", "mesqu", "misqu", "mosiq")

d.target.items <- fromJSON(paste(project.path, "data/generics/endorsement/", "originalStims.json", sep = "")) %>%
  mutate(
    property = gsub("'", "", property),
    Property = gsub("'", "", Property),
    sentence = paste(Category, Property),
    item = paste(category, property))


d.prev <- read.csv(paste(project.path, "data/generics/endorsement/",
                       "naturalGenerics-prior-trials-n57.csv", sep = ""))


d.prev.summary <- d.prev %>%
  mutate(item = paste(Category, Property)) %>%
  filter(item %in% d.target.items$sentence) %>%
  mutate(prevalence = prevalence / 100) %>%
  group_by(Category, Property, item) %>%
  multi_boot_standard(column = "prevalence")

d.cv.bootstrapped <- data.frame()
resample_n <- length(levels(factor(d.cv$workerid)))
for (i in 1:1000){
  
  d.cv.bsample <- d.cv %>% 
    select(workerid, property, response) %>% 
    spread(property, response) %>% 
    sample_n(resample_n, replace = TRUE) %>%
    gather(property, category, -workerid) %>% 
        group_by(property) %>%
    mutate(n = n(),
           item = paste(category, property)) %>%
    filter(item %in% d.target.items$item) %>%
    group_by(category, property) %>%
    summarize(mentions = n(),
              trials = mean(n), # mean(n) == n, because it's just the number of subjects
              prop = mentions / trials,
              prop = ifelse(is.na(prop), 0.01, prop),
              iteration = i)
  
  d.cv.bootstrapped <- bind_rows(d.cv.bootstrapped, d.cv.bsample)
}

empiricalLower = function(dist){
  xi <- quantile(dist, 0.025)
  return (xi[["2.5%"]])
}
empiricalUpper = function(dist){
  xi <- quantile(dist, 0.975)
  return (xi[["97.5%"]])
}
empiricalMean = function(dist){
  xi <- quantile(dist, 0.5)
  return (xi[["50%"]])
}


d.gen.cv.summary <- left_join(
    d.target.items %>% select(-sentence),
    d.cv.bootstrapped %>%
      group_by(property) %>%
      summarize(cv_mean = empiricalMean(prop),
                cv_ci_lower = empiricalLower(prop),
                cv_ci_upper = empiricalUpper(prop))
) %>%
  mutate(
    cv_ci_lower = ifelse(is.na(cv_mean), 0.01, cv_ci_lower),
    cv_ci_upper = ifelse(is.na(cv_mean), 0.01, cv_ci_upper),
    cv_mean = ifelse(is.na(cv_mean), 0.01, cv_mean)
  )
  
# quantile(filter(d.cv.bootstrapped, property == "is red")$prop, c(0.025, 0.5, 0.975))
# 
# d.cv.bootstrapped %>%
#   group_b

# need to filter by items specified with endorsement
# d.gen.cv.summary <- left_join(
#   d.target.items %>% select(-sentence),
#   d.cv %>%
#     rename(category = response) %>%
#     group_by(property) %>%
#     mutate(n = n(),
#            item = paste(category, property)) %>%
#     filter(item %in% d.target.items$item) %>%
#     group_by(category, property) %>%
#     summarize(mentions = n(),
#               trials = mean(n), # mean(n) == n, because it's just the number of subjects
#               prop = mentions / trials)
#   ) %>%
#   mutate(prop = ifelse(is.na(prop), 0.01, prop))

d.gen.endorse.prev.cue <- left_join(
  left_join(
    left_join(
      d.gen.endorse, 
      d.gen.endorse.catch %>% select(workerid, pass)
      ) %>%
      filter(pass == 1) %>%
    rowwise() %>%
    mutate(response = ifelse(response == "agree-key", 1, 0),
         sentence = gsub('&quotechar', '', sentence),
         sentence = gsub('lyme', 'Lyme', sentence),
          sentence = gsub('[.]', '', sentence)
    ),
    d.prev.summary %>% 
      rename(prev_mean = mean,
             sentence = item,
             prev_ci_lower = ci_lower,
             prev_ci_upper = ci_upper)
    ),
  d.gen.cv.summary %>% select(-item))

glm.gen.endorse.prev <- glm(response ~ prev_mean, 
              data = d.gen.endorse.prev.cue, family = 'binomial')

glm.gen.endorse.prev.cv <- glm(response ~ prev_mean + cv_mean, 
              data = d.gen.endorse.prev.cue, family = 'binomial')

d.gen.endorse.prev.cue.uniq <- unique(select(
    d.gen.endorse.prev.cue, Property, Category, sentence, 
    prev_mean, prev_ci_lower, prev_ci_upper, cv_mean, cv_ci_lower, cv_ci_upper
  ))

d.gen.endorse.prev.cue.uniq.lower <- d.gen.endorse.prev.cue.uniq %>% 
  select(-prev_mean, -cv_mean) %>% 
  rename(prev_mean = prev_ci_lower,
         cv_mean = cv_ci_lower)
d.gen.endorse.prev.cue.uniq.upper <- d.gen.endorse.prev.cue.uniq %>% 
  select(-prev_mean, -cv_mean) %>% 
  rename(prev_mean = prev_ci_upper,
         cv_mean = cv_ci_upper)

d.gen.endorse.regression.prevalence <- left_join(
  d.gen.endorse.bayes,
  d.gen.endorse.prev.cue.uniq %>%
    mutate(
      prediction_mean = predict(glm.gen.endorse.prev, ., type = "response"),
      prediction_ci_lower = predict(glm.gen.endorse.prev, d.gen.endorse.prev.cue.uniq.lower, type = "response"),
      prediction_ci_upper = predict(glm.gen.endorse.prev, d.gen.endorse.prev.cue.uniq.upper, type = "response")
    ) %>%
    mutate(sentence = paste(Category, " ", Property, ".", sep = ""))
) %>% mutate(
  sqErr = (MAP_h - prediction_mean) ^ 2
)

d.gen.endorse.regression.prevalence.cuevalidity <- left_join(
  d.gen.endorse.bayes,
  d.gen.endorse.prev.cue.uniq %>%
    mutate(
      prediction_mean = predict(glm.gen.endorse.prev.cv, ., type = "response"),
      prediction_ci_lower = predict(glm.gen.endorse.prev.cv, d.gen.endorse.prev.cue.uniq.lower, type = "response"),
      prediction_ci_upper = predict(glm.gen.endorse.prev.cv, d.gen.endorse.prev.cue.uniq.upper, type = "response"),
      sentence = paste(Category, " ", Property, ".", sep = "")
      )
) %>% mutate(sqErr = (MAP_h - prediction_mean) ^ 2)

d.gen.endorse.regression <- bind_rows(
  d.gen.endorse.regression.prevalence.cuevalidity %>%
    mutate(src = "regression_Prev_Cuevalidity"),
  d.gen.endorse.regression.prevalence %>% 
    mutate(src = "regression_Prev")
  )
# ggplot(d.gen.endorse.regression, 
#        aes (x = prediction, y = MAP_h, ymin = low, ymax = high, color = cuevalidity))+
#   geom_errorbar(alpha = 0.3)+
#   geom_abline(intercept = 0, slope = 1, lty = 3)+
#   # geom_text_repel(data = d.gen.endorse.regression.prevalence.cuevalidity %>% 
#   #                   rename(cuevalidity = prop) %>% 
#   #                   filter(sqErr > 0.05), 
#   #                 aes(label = sentence, color = cuevalidity), force = 1, size = 3)+
#   geom_point()+
#   xlim(0,1)+
#   ylim(0,1)+
#   coord_fixed()+
#   xlab("Logistic model prediction")+
#   ylab("Human generic endorsement")+
#   facet_wrap(~src)

r2.gen.n <- length(d.gen.endorse.bayes$sentence)
r2.gen.regression.prev <- round(with(d.gen.endorse.regression.prevalence, cor(MAP_h, prediction_mean))^2,2)
mse.gen.regression.prev <-  round(mean(d.gen.endorse.regression.prevalence$sqErr), 3)

r2.gen.regression.prev.cv <- round(with(d.gen.endorse.regression.prevalence.cuevalidity, cor(MAP_h, prediction_mean))^2, 3)
mse.gen.regression.prev.cv <- round(mean(d.gen.endorse.regression.prevalence.cuevalidity$sqErr), 3)

intermediate.prev.quantiles <-  quantile(d.gen.endorse.regression.prevalence$prev_mean, c(0.25, 0.76))


d.gen.endorse.regression.prevalence.intermediateprev <- d.gen.endorse.regression.prevalence %>%
  filter(
    (prev_mean > intermediate.prev.quantiles[["25%"]]) &
    (prev_mean < intermediate.prev.quantiles[["76%"]])
)
r2.gen.n.intermedprev <- length(d.gen.endorse.regression.prevalence.intermediateprev$sentence)

r2.gen.regression.prev.intermedprev <- round(with(d.gen.endorse.regression.prevalence.intermediateprev, cor(MAP_h, prediction_mean))^2,2)
mse.gen.regression.prev.intermedprev <-  round(mean(d.gen.endorse.regression.prevalence.intermediateprev$sqErr), 3)
```


##### Prevalence baseline

From the prevalence prior data (Expt. 1a), we estimate participants' beliefs about the prevalence of a property for the target categories (e.g., the percentage of \textsc{robins} that \textsc{lay eggs}.
We find a little over half of the variance in the endorsement data is explained this way ($r^2(`r r2.gen.n`) = `r r2.gen.regression.prev`$; MSE=$`r mse.gen.regression.prev`$; Figure \ref{fig:generics-endorsement-figure}, upper-right: lower-left facet). 
This is because our stimulus set includes generics that are true with high-prevalence properties (e.g., *Leopards have spots.*) and generics that are false with low prevalence properties (e.g., *Leopards have wings.*). 
However, large deviations from an account based purely on target-category prevalence remain: Generics in which the target-category has intermediate prevalence (prevalence quartiles 2 and 3: $`r round(100*intermediate.prev.quantiles[["25%"]])`\% < \text{prevalence} < `r  round(100*intermediate.prev.quantiles[["76%"]])`\%$), are not at all explained by prevalence within those categories ($r_{Q2,3}^2(`r r2.gen.n.intermedprev`) = `r r2.gen.regression.prev.intermedprev`$; MSE = $`r mse.gen.regression.prev.intermedprev`$).

##### Prevalence and cue validity

<!-- As a secondary alternative hypothesis, we explore whether prevalence and *cue validity* predicts generic endorsement of these items. -->
If prevalence is the "forward probability" (e.g., one's predictions about whether or not an entity will lay eggs after learning that the entity is a robin), *cue validity* is the inverse probability: $P(k \mid f)$ (e.g., one's predictions about whether or not the entity is a robin, upon learning that it lays eggs).
*Cue validity* measures the salience of a category given a feature but also captures the distinctiveness of the feature. 
For example, if one learns that an entity carries malaria, that entity is probably a mosquito because only mosquitos carry malaria. 
Previous studies have found cue validity to be correlated with generic endorsement [@Khemlani2012], though it's role in generic meaning is controversial [@Leslie2007; @Leslie2008].

For a prevalence distribution with a fixed set of categories, the cue validity of a kind given a feature can be calculated from the prevalence prior distribution.
In empirical studies of generic language, however, it is often empirically measured [@Khemlani2012].
For this analysis, we measured cue validity using a free production paradigm, following @Cree2006.
Participants ($n = 50$) were supplied with a feature (e.g., "X lays eggs") and asked to generate a kind (e.g., "what do you think X is ?").
For a detailed discussion of the results of this experiment and comparison to other measurements of cue validity, see Appendix B.

A linear model that uses predictors for both prevalence and cue validity does a better job at explaning the endorsement data than just prevalence alone ($r^2(`r r2.gen.n`) = `r r2.gen.regression.prev.cv`$; MSE=$`r mse.gen.regression.prev.cv`$).
This model is able to account for the endorsements of examples like "Mosquitos carry malaria" and "Lions have manes", as these features are very diagnostic of the kind.
Deviations, however, still remain.
For example, "Robins lay eggs" still receives only intermediate endorsement by this model, and "Mosquitos don't carry malaria" is judged to be a good statement.

These highlight a shortcoming of a simple model based on cue validity.
"Lays eggs" is a somewhat diagnostic feature for *birds*, but there are many kinds of birds, and the feature is not itself diagnostic for "Robins".
A single metric like cue validity is too blunt to capture this subtlety.
Additionally, negative properties (like, "not carrying malaria") are completely undiagnostic for almost every category, and the fitted regression model doesn't know how to penalize "Mosquitos don't carry malaria" because the prevalence is high.

##### Fixed threshold RSA model

```{r generic-RSAmodels, fig.width = 8}
n_chains <- 3
n_samples <- 100000
burn <- n_samples / 2
lg <- 20

model_prefix <- "results-generics-jointModel-S1-smntcs_"

# m.samp <- data.frame()
# m.samp.fixed <- data.frame()
# for (i in seq(1, n_chains)){
#   mi.gen <- fread(paste(project.path,  "models/generics/results/", model_prefix, "generic-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.samp <- bind_rows(m.samp, mi.gen %>% mutate(chain = i))
#   
#   mi.fixed <- fread(paste(project.path,  "models/generics/results/", model_prefix,"some-",
#                     n_samples, "_burn", burn, "_lag", lg ,"_chain", i, ".csv", sep = ""))
#   m.samp.fixed <- bind_rows(m.samp.fixed, mi.fixed %>% mutate(chain = i))
#   }
# 
# save(m.samp,
#      file = paste(project.path,  "models/generics/results/", model_prefix, "generic-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))
# 
# save(m.samp.fixed,
#      file = paste(project.path,  "models/generics/results/", model_prefix, "some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))


load(file = paste(project.path,  "models/generics/results/", model_prefix, "generic-",
                    n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))

load(file = paste(project.path,  "models/generics/results/", model_prefix, "some-",
                    n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))


m.gen.somemodel.endorsement <- m.samp.fixed %>%
  filter(type == 'endorsement') %>%
  group_by(type, param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.gen.fullmodel.endorsement <- m.samp %>%
  filter(type == 'endorsement') %>%
  group_by(type, param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.gen.endorse.rsa <- bind_rows(
  left_join(
    d.gen.endorse.bayes,
    m.gen.fullmodel.endorsement %>%
      mutate(sentence = paste(category, " ", property, ".", sep = "")) %>%
      ungroup() %>%
      select(-type, -param)
  ) %>%
    mutate(src = "generics_model"),
  left_join(
    d.gen.endorse.bayes,
    m.gen.somemodel.endorsement %>%
      mutate(sentence = paste(category, " ", property, ".", sep = "")) %>%
      ungroup() %>%
      select(-type, -param)
  ) %>%
    mutate(src = "some_model")
)


r2.gen.rsa.generics <- compute_r2(
  m.gen.endorse.rsa %>% filter(src == "generics_model"), 
  "MAP", "MAP_h", sigfigs = 3
)
  
r2.gen.rsa.fixed <- compute_r2(
  m.gen.endorse.rsa %>% filter(src == "some_model"),  
  "MAP", "MAP_h", sigfigs = 3
)

mse.gen.rsa.generics <- compute_mse(
  m.gen.endorse.rsa %>% filter(src == "generics_model"), 
  "MAP", "MAP_h", sigfigs = 4
)

mse.gen.rsa.fixed <- compute_mse(
  m.gen.endorse.rsa %>% filter(src == "some_model"),  
  "MAP", "MAP_h", sigfigs = 4
)


generics.endorsement.models <- bind_rows(
  d.gen.endorse.regression,
  left_join(
    m.gen.endorse.rsa %>% 
      rename(prediction_mean = MAP, prediction_ci_lower = cred_lower, prediction_ci_upper = cred_upper),
    d.gen.endorse.regression %>% 
      select(sentence, Property, Category, prev_mean, cv_mean)
  )
) %>%
  mutate(src = factor(src, levels = c( "some_model",
                                      "generics_model",
                                      
                                      "regression_Prev", 
                                      "regression_Prev_Cuevalidity"
                                     ),
                      labels = c("Fixed semantics model",
                                 "Uncertain semantics model",
                                 
                                 "Prevalence alone",
                                 "Prevalence + Cue validity"
                                 ))) %>%
  ggplot(., aes ( x = prediction_mean, xmin = prediction_ci_lower, xmax = prediction_ci_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = prev_mean ))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.7)+
  geom_errorbarh(alpha = 0.7)+
  geom_point(shape = 21)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human generic endorsement")+
  facet_wrap(~src, nrow = 2)+
  theme(legend.position = "bottom")
```


```{r generic-model-insets, fig.width = 4.75, fig.height = 1.5}
example.generics.properties <- c("dont eat people", 
                      "carry malaria", 
                      "lay eggs", 
                      "are female",
                      "have spots")

example.generics <- c("Tigers dont eat people", 
                      "Mosquitos carry malaria", 
                      "Robins lay eggs", 
                      "Robins are female",
                      "Leopards have spots")

m.gen.fullmodel.prior.parameters <- m.samp %>%
  filter(type == "prior", property %in% example.generics.properties) %>%
  group_by(param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

# use MAP estimates to generate L(h | generic) & L(h | silence) predictions

m.gen.fullmodel.prior.parameters.tidy <- m.gen.fullmodel.prior.parameters %>%
  ungroup() %>%
  select(param, property, category, MAP) %>%
  mutate(param = paste(param, category, sep = "_")) %>%
  select(-category) %>%
  spread(param, MAP) %>%
  rename(mix = mixture_NA, 
         stable_mean = stableFreq_mean, 
         stable_concentration = stableFreq_sampleSize) %>%
  mutate( a = stable_mean * stable_concentration, 
          b = (1 - stable_mean) * stable_concentration)

gen.listener.predictions <- data.frame()
  
for (p in example.generics.properties){
 priorParams <- m.gen.fullmodel.prior.parameters.tidy %>% filter(property == p) 
 inputData = list(prior = list(params = data.frame(a = priorParams[["a"]],
                                                   b = priorParams[["b"]]),
                               mix = priorParams[["mix"]]), 
                  utt = "generic")
 l0.rs <- webppl(l0.model, data = inputData, data_var = "data")
 gen.listener.predictions <- bind_rows(
   gen.listener.predictions, 
   l0.rs %>% select(Parameter,value) %>% mutate(property = p)
   )
}

m.gen.fullmodel.target.prevalence <- m.samp %>%
  filter(type == "targetPrevalence") %>%
  group_by(param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))



m.gen.speakerBeliefs <- m.samp %>%
  filter(type == "targetPrevalence") %>%
  mutate(Sentence = paste(category, property)) %>%
  filter(Sentence %in% example.generics) 

m.samp.prev.params <- m.samp %>%
    filter(type == "targetPrevalence") %>%
    mutate(Sentence = paste(category, property)) %>%
    filter(Sentence %in% example.generics) %>%
    mutate(parameter = paste(param, property, category, sep = "_")) %>%
    select(-param, -property, -category, -chain, -type) %>%
    group_by(parameter) %>%
    mutate(iteration = ave(parameter==parameter, parameter, FUN=cumsum)) %>%
    ungroup() %>%
    separate(parameter, into = c("parameter", "property", "category"), sep= "_") %>%
    group_by(category, property, iteration) %>%
    spread(parameter, val) %>%
    rowwise() %>%
    mutate(
      a = mean*sampleSize,
      b = (1-mean)*sampleSize,
      val = rbeta(n = 1, shape1 = a, shape2 = b)
      ) %>%
    ungroup()

gen.inset.distributions <- bind_rows(
  gen.listener.predictions %>%
    mutate(category = NA),
  m.gen.speakerBeliefs %>%
    select(property, val) %>%
    rename(value = val) %>%
    mutate(Parameter = "speakerBeliefs")
)


category.text.labels <- data.frame(property = c("dont eat people", 
                      "carry malaria", "lay eggs",  "are female", "have spots"),
             category = c("Tigers", "Mosquitos", "Robins", "Robins", "Leopards"),
             x = c(0.3, 0.3, 0.47, 0.05, 0.6),
            y = c(0.45, 0.5, 0.45, 0.5, 0.26))

generics.endorsement.insets <- gen.inset.distributions %>%
  mutate(Parameter = factor(Parameter, levels = c("state_Prior",
                                                  "state_Posterior",
                                                  "speakerBeliefs"),
                            labels = c("Listener Prior (Posterior given Silence)", 
                                       "Listener Posterior given Generic", 
                                       "Speaker Beliefs")),
         property = fct_relevel(property,
                                "have spots", "lay eggs","carry malaria", 
                                "are female", "dont eat people") ) %>%
  ggplot(., aes( x = value, fill = Parameter, color = Parameter, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 4, size = 1)+
  facet_wrap(~property, nrow = 1)+
  geom_text_repel(data = category.text.labels,
                  aes(label = category, x = x , y = y),
                  inherit.aes = F, color = "#2b83ba")+
    #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_alpha_manual(values = c(0.6, 0.4, 0))+
    #scale_linetype_manual(values = c(3, 4, 2, 1))+
    scale_linetype_manual(values = c(3, 4, 1))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Prevalence") +
    ylab("Scaled probability density")+
    theme(legend.position = "bottom", legend.title = element_blank())
```



```{r generics-endorsement-figure, fig.width=11, fig.height=8}
grid.arrange(generics.endorsement.spectrum, 
             generics.endorsement.models,
             generics.endorsement.insets, ncol = 2,
             layout_matrix = cbind(c(1,1,3), c(2,2,3)))
```

The RSA model we propose for generic endorsement is a speaker model that decides whether or not to say the generic to a naive listener.
Unlike the simple regression models based on point-estimates of target-category prevalence and cue validity, this cognitive model has access to the full prior distribution on prevalence $P(h)$.
This model has two other main architectural differences: an information-theoretic decision between producing the generic vs. an information-less alternative and uncertainty about the semantic threshold that corresponds to the theoretical, core meaning of a generic statement.

We first test a model that has two of the the above three main architectural features.
This model has access to the full prior distribution on prevalence as well as models an information-theoretic decision of producing the generic vs. an alternative.
The only difference is that this model uses a fixed-threshold to correspond to the core meaning of a generic statements.
For this model, we use $\theta = 0$ (corresponding intutively to the semantics of the quantified "Some" statements).^[
Other fixed-threshold models are possible (e.g., generic means "Most" $\theta = 0.5$) but require the further specification of a noise-model for participants' responses in the cases where participants endorse the statement when it is literally false (e.g., "Mosquitos carry malaria").
]
This is a strong alterantive model to our uncertain-threshold model; the only difference concerns the threshold.


<!-- Our pragmatic speaker model, $S_1$ in Eq. \ref{eq:S1}, reasons about whether it would be useful to say to the generic to this kind of listener so that the listener may acquire the same beliefs as the speaker (represented as the prevalence of the property within the target category $h'$). -->
We use the empirically estimated target-category prevalence as the $h$ that the speaker $S_1$ is trying to communicate, and use the empirically measured priors from Expt. 1a as the listener's prevalence prior $P(h)$.^[
Rather than trying to communicate a particular $h$, the speaker could plausibly try to communicate her full belief distribution over $h$ to the listener. 
This corresponds to the more general form of the model described in a footnote in the model section. 
With these data, the predictions of these two models are nearly identical.
]
The $S_1$ model is then fully specified after setting the speaker optimality parameter $\lambda_1$ (in Eq. \ref{eq:S1}).


\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{figs/generics_bayesnetmodel.pdf}
    \caption{Caption
    }
  \label{fig:genericsModelDiagram}
\end{figure}

To fit this model, we incorporate the RSA model into the Bayesian data analysis model of the prevalence prior data (Expt. 1a) to create a single, joint-inference model where the speaker optimality parameter $\lambda_1$ is inferred jointly with the target prevalence $h_{k, f}$ for each category $k$ and property $f$ and the parameters of the prevalence priors $P_f(h)$ for each property $f$ using data from Expt. 1a \& b (see Figure \ref{fig:genericsModelDiagram}).
For the parameters of the prevalence distributions, we use the same priors described in Expt. 1a; for the speaker optimality parameter, we use a prior with a range consistent with previous literature using the same model class: $\lambda_1 \sim \text{Uniform}(0,5)$.
We learn about the \emph{a posteriori} credible values of this joint inference model using an incrementalized-version of the  by collecting samples from 3 MCMC chains of 100,000 iterations removing the first 50,000 iterations for burn-in, using an incrementalized version of the Metropolis-Hastings algorithm [@Ritchie2016]. 

The posterior distribution of this joint inference model tells us what we should believe about the latent parameters of the model (prevalences, prevalence priors, and the speaker optimality parameter) given the observed data, as well as what predictions the model makes given these parameter values.
Importantly, the model recapitulates the prevalence prior data (e.g., the prevalence of laying eggs among various species) and the target-category prevalence data (e.g., the prevalence of laying eggs among robins; see Appendix A for full posteriors) as it did when the Bayesian data analysis model only included these data and their associated model (results section of Expt. 1a).
This means the theoretically-interesting predictionso of this model --- predictions of generic endorsement --- are based on intuitively meaningful model components (i.e., the details of the prevalence distributions). 

We compare the fixed-threshold model's posterior predictive distribution of generic endorsement to the empirical truth judgments.
(Again the posterior predictive distribution marginalizes over the inferred parameter values to produce predictions about what the data \emph{should look like} given the pragmatics model and the observed data.)
Figure \ref{fig:generic-endorsement-figure} (top right; top left subplot) shows the fixed-threshold model's ability to predict the generic endorsement data ($r^2(`r r2.gen.n`) = `r r2.gen.rsa.fixed`$; MSE = $`r mse.gen.rsa.fixed`$).
For almost all items, it assigns an intermediate endorsement.
This is because almost all items get endorsed by some participants; the model then infers that the prevalence must be at least a little greater than 0\%; but, if the prevalence is greater than 0\%, the statement is literally true. The speaker model then must decide is it better to say the 0\%-threshold meaning or stay silent. 
For some items, a fixed-threshold meaning carries some information-gain over silence (i.e., some model predictions \> 0.5). For many items, however, the 0\%-threshold is almost as meaning-less as silence; the model, thus, has a hard time strongly endorsing some statements (most model predictions around 0.5\%).
Consistent with intuition, assigning a fixed-semantics is too lenient a specification. 

MHT: Outfit fixed-threshold model with noise parameter?

##### Uncertain threshold RSA model

Our underspecified threshold model is the same as the fixed-threshold RSA model described above, except that rather than having a fixed threshold at $\theta = 0$, it has uncertainty over the threshold.
We construct the same joint-inference Bayesian data analytic model as we did for the fixed-threshold analysis.
As we see in Figure \ref{fig:generics-endorsement-figure} (top right, top right subplot), the speaker model that relies upon an uncertain-threshold semantics explains nearly all of the variance in human endorsements ($r^2(`r r2.gen.n`) = `r r2.gen.rsa.generics`$; MSE = $`r mse.gen.rsa.generics`$).


To gain some intuition for why that is the case, examine the bottom row of Figure \ref{fig:generics-endorsement-figure}.
The blue, solid lined distribution corresponds to the speaker's beliefs about the prevalence of the feature for the target category (also in blue).
The grey distribution shows the listener's prior distribution over prevalence of the feature (these are the corresponding Probability Density Functions of the Cumulative Density Functions shown in Figure \ref{fig:generic-endorsement-priors-figure}).
It is also the listener $L_0$'s posterior upon hearing the silent utterance.
The red distribution shows the $L_0$ posterior upon hearing the generic utterance that corresponding to an uncertain threshold-semantics. 
The speaker model decides when the generic utterance conveys information that would bring the listener's distribution more in line with the speaker's own distrubtion. 
Thus, it finds that "Leopards have spots", "Robins lay eggs", and "Mosquitos carry malaria" are also useful things to say.
"Robins are female" is neither good nor bad, as the information-content of the utterance is very small relative to silence.
"Tigers don't eat people" is rather misleading, however; it doesn't bring the listener closer to the speaker's distribution, despite the speaker believing that the prevalence is more than half. 



<!-- Generics that received definitive agreement or disagreement are predicted to be judged as such by the model (corners of Figure \ref{fig:commongenerics}b), including items for which target-category prevalence is not a good indicator of the acceptability (e.g. \emph{Mosquitos carry malaria}, for prevalence quartiles 2 and 3, $r_{Q2,3}^2=0.955$; MSE=0.005; Figure \ref{fig:commongenerics}b, intermediate shades). -->
<!-- We also see the generics truth judgment model predicts uncertain truth judgments: for instance, \emph{Robins are female} is judged by both the model and human participants to be neither true nor false. -->
<!-- \emph{Sharks don't attack swimmers}, while true of most sharks, is judged to be not a good thing to say by both participants and the model. -->
<!-- This is strong evidence that the puzzling flexibility of generic truth conditions can be understood with a simple but underspecified semantics for which the precise meaning is resolved with respect to diverse prior beliefs in context. -->




<!-- The above analysis provides a single estimate for model predictions, but it is based on noisy empirical measurements of $P(x)$. In order to estimate the impact of this empirical noise on our model predictions, we resampled the prior data (with replacement) for 57 participants worth of data, discretizing and binning as we did above and then inferring parameters. -->
<!-- This procedure (re-sample prior, discretize and bin, infer parameters) was repeated 500 times to bootstrap the model predictions. -->
<!-- The Maximum A-Posteriori (MAP) estimate and 95\% Highest Probability Density (HPD) interval for $\lambda_1$ is 0.5 [0.004, 12.7] and $\lambda_2$ is 1.7 [1.3, 2.1]. -->
<!-- ^[ -->
<!-- The fact that $\lambda_1$ is credibly less than 1 suggests that the generic utterance may be more costly than "staying silent" (our model assumes equal cost). -->
<!-- We maintain using only the two $\lambda$ parameters in the model for simplicity. -->
<!-- ] -->


<!-- This is akin to fitting the parameters and is the critical step in model validation: It shows what data is actually predicted by the model.)  -->


<!-- basic communicative principles (\emph{be truthful}, \emph{be informative}) operating over diverse prior beliefs about the properties, all of which are at play in understanding language.  -->

<!-- \begin{figure} -->
<!-- \centering -->
<!--     \includegraphics[width=\columnwidth]{figs/generics-prior-prevalence-tj.pdf} -->
<!--     \caption{Endorsing familiar generics. (a)  -->
<!--     Prevalence prior distributions empirically elicited for twenty-one animal properties. -->
<!--     Prior distributions summarized by two parameters of a structured Bayesian model: $\phi$----a property's potential to be present in a category----and $\gamma$----the mean prevalence when it is possible for the property to be present in a category. -->
<!--     Inset plots display example empirical prior distributions over prevalence and corresponding $L_1$ model predictions: the posterior after hearing a generic utterance.  -->
<!--     Intervals on the top of insets show human judgments about the prevalence of the property within a target category. -->
<!--     (b) -->
<!--     Human acceptability judgments compared with model predictions (left) and the target-category prevalence (right) for thirty generic utterances about familiar animals and properties.  -->
<!--     Color denotes target-category prevalence of the property, with lighter colors indicating higher prevalence.  -->
<!--      Error bars denote 95\% Bayesian credible intervals. -->
<!--     } -->
<!--   \label{fig:commongenerics} -->

<!-- \end{figure} -->


## Discussion

Generic language is the premier case study for generalizations in language. 
Generics have been studied extensively in the cognitive and developmental psychological literatures and have been shown to have implications for wide ranging phenomena from stereotype propogataion [@Rhodes2012] to motivation [@Cimpian2007].
However, no formal model has been able to make precise quantitative predictions about the basic phonemena of generics, and classic counterexamples to theories based on probability (*Birds lay eggs* v. *are female*; *Mosquitos carry malaria*) have stifled the development of formal models.

By measuring listeners' prior beliefs about the prevalence of the property across kinds and the prevalence of the target kind (e.g., the percentage of birds that lay eggs), we were able to show how a semantics based on probability is tenable inspite of these seeming counterexamples.
The key theoretical contribution is that the threshold must be *underspecified* in the semantics and inferred in context by the listener.
The decision of whether or not to endorse the generic comes down to whether or not the generic would make the listener's belief distribution more in line with the speaker's.

In explaining the variable endorsements of generics, we measured the prior distribution over the prevalence of features across categories in addition to the prevalence of the feature in the target category (e.g., the percentage of robins that lay eggs) to model the endorsement data.
Thus, we have shown how these measurements are related to generic endorsement via our model. 
Yet, this evidence is still correlational in nature.
We do not yet know if the variables that we are measuring are causally related to endorsement.
In our second case study, we extend our theory to a new domain and provide the first causal evidence for our theory by manipulating the target propensity that the speaker aims to communicate.

<!-- In addition, why generics often imply the property is widespread has also been puzzling [@Gelman2004]. -->
<!-- We show that this is the result of the shape of listener's prior knowledge and that when that shape differs, the implied prevalence is much weaker (e.g., with accidental properties). -->
<!-- Even more puzzling has been the basic asymmetry between endorsements and interpretations [@Cimpian2010]. -->
<!-- We show that this asymmetry arises from the underspecified-threshold interacting with diverse prior beliefs.  -->
<!-- While implied prevalences vary wildly for different kinds of properties under discussion, the endorsements are relatively less sensitive to these distinctions. -->
<!-- Thus, the basic asymmetry is largely being driven by the listener's interpretation of a generic. -->


# Case Study 2: Habitual Language

*Habitual* language (e.g,. "Mary smokes."; "My car won't start.") conveys generalizations about events. 
In this case study, we focus on a particular class of events: people doing actions.
We focus on present-tense habitual sentences of the form \textsc{singular noun phrase} $+$ \textsc{present tense simple verb phrase} (e.g., \emph{Mary smokes cigarettes.}). 

Habituals about people are of particular importance because they are likely central to intuitive theories of personality and behavior. 
<!-- Young children can use actors' behaviors to make inferences about what the actors are like more generally [e.g., @Repacholi1997; @Seiver2013]. -->
@McGuire1986 explored 5th to 12th graders' responses to open ended probes such as "Tell us about your school / family". 
A surprisingly large number of utterances (over 85\% in their study) were about people, and roughly two-thirds of those utterances used action verbs (e.g., "My brother works part-time at the restaurant.").
Habitual language may be a more conservative form of trait language (e.g., \emph{Bill is a smoker.}) and convey that behaviors are relatively enduring [@Gelman1999; @Gelman2004].

From a methodological standpoint, habituals are a domain particularly amenable to manipulating the target propensity that a speaker aims to communicate.^[
While target probability can be manipulated for generalizations about categories (generic language) (e.g., by stating the prevalence of feature in a kind), observers may have strongly constraining domain knowledge that heavily guides what prevalence values are likely (e.g., it's *a priori* very unlikely that 90% of a hypothetical novel kind---lorches---have broken wings).
]
First of all, because of the sociality of our species, we are often in the situation of being introduced to new people and learning new information about them.
In addition, propensity information of people doing actions can be expressed in an easily-interpretable frequency (e.g., "Last month, Mary smoked cigarettes 3 times.").

We follow the same general experimental structure as in Experiment 1. 
First, we measure the prior distribution over the frequency of events, for different kinds of events (Expt. 2a).
We do this using a novel data-analytic approach that takes advantage of the structured representation discovered in Expt. 1a.
Then, we test endorsements of habitual sentences, manipulating the actual frequency with which a person does an action (Expt. 2b).
In addition to directly manipulating the target propensity, these experiments provide the first test of the generality of our theory: It can apply to generalizations about events as well as categories.

Finally, if habituals (and generics) are truly language for conveying generalizations, it would be useful for them to reflect speakers' expectations, not merely her observations. 
Up to this point, we have been relatively agnostic as to the interpretation of $h$, the target propensity that the speaker aims to convey.
In two follow-up experiments, we ask whether $h$ represents the actual, past frequency of the event (e.g., how often Mary has actually smoked in the past week) or the predictive probability that this event will occur in the near future (e.g., how likely it is that Mary will smoke next week)? 
We do this by measuring particpants predictions about future frequency in situations where there are causal forces enabling or preventing the action (Expt. 2c). 
We separately measure endorsements under the same contexts (Expt. 2d) and use the prediction measurements (Expt. 2c) as the target propensity $h$ that the speaker model is trying to communicate in order to predict habitual endorsements.
To anticipate our results, we find that only a model that uses *predictive frequency* as the object of communication is able to predict the endorsement data, thus tying the phenomenology of *genericity* even more to subjective beliefs.


<!-- This set of experiments is organized as follows. -->
<!-- In the **Event Prior Elicitation** study (Expt. 1), -->
<!-- In the **Endorsing Habitual Language** experiment (Expt. 2), we introduce a separate participants to different characters, each of whom have done some action with some frequeny (e.g., *In the last month, John ran 3 times*), and ask them to judge the corresponding generalization (e.g., *John runs*). -->
<!-- In the **Interpreting Habitual Language** experiment (Expt. 3), we invert this paradigm, supplying participants with the generalization (e.g., *John runs*) and ask them to juge how often the person does some action (e.g., answer the question: *How often do you think John runs?*). -->

<!-- We evaluate our family of RSA models, in addition to some ad-hoc non-Bayesian models, on these experimental data. -->
<!-- Because the ad-hoc models do not comprise a formalized alternative theory of endorsement and interpretation, but rather statistical descriptions of the data, we compare the ad-hoc models to the simplest RSA model on endorsement and interpretation, separately. -->
<!-- After presenting the results of Expts. 2 \& 3, we compare RSA models on their ability to jointly predict both endorsement and interpretation data simultaneously.  -->
<!-- Finally, having validated a communicative theory of genericity, we probe the theory in more detail in Experiment 4 by asking whether past frequency or predictive probability is what is being communicated. -->


<!-- The internal structure of particular events can differ in substantial ways. -->
<!-- For example, events differ in their expected duration. -->
<!-- For example, the event of *smoking a cigarette* plausibly takes between 5 - 10 minutes, while the event of *wearing socks* probably lasts for several hours. -->
<!-- We take as a starting point the fact that events can be segmented and aggregated; the relevant dimension becomes the frequency with which the event occurs. -->

<!-- In our main test of the hypothesis, we collect felicity or truth judgments for habitual statements about people's actions. -->
<!-- The computational model described in Eq. \ref{eq:S1} is fully specified except for the prior distribution over the frequency of an event $P(h)$, which plausibly varies across different kinds of events. -->
<!-- First, we measure the prior distribution, and use it and the computational model to predict human endorsement of habitual statements. -->

## Experiment 2a: Measuring the propensity prior for event knowledge

In this experiment we elicit the prior $P(h)$ for different events in order to generate model predictions for corresponding habitual statements (Expt. 2b \& 2d).
For the case of statements about the behaviors of people, $P(h)$ can be conceived as a distribution over *different people*, each of whom do the behavior with some frequency.
To better understand what frequencies should be expected for different actions, we took advantage of the 
structured representation of the prior discovered in Expt. 1.

In Expt. 1a, we discovered that prior distribution over the prevalence of a feature across different categories can be modeled by a mixture model with two components.
The first component (the "stable cause" component) represents the normal, stable frequencies of the property, among those that normally have the property.
The second component (the "unstable" or "transient cause" component) represents the frequencies of the property among those categories who do not typically have the property. This second component will usually result in frequencies of 0\% (e.g., 0\% of lions have lay eggs), but could produce non-zero frequencies (e.g., in some hypothetical world, a lion who has undergone weird genetic mutations may lay eggs). 

For the case of background knowledge over the frequency with which people do actions, we consider again two components.^[
In the case of event knowledge about people, we might expect there to be several more than just these two possibilities represented in the prior, corresponding to individuals with different traits or demographics.
We assume here a simple structure so as to not make the specification of the prior overly complex.
]
To a first approximation, the first component represents the frequencies of the event among the people who have done the action before (e.g., how often a person hikes, given that they have hiked before).
The second component represents the frequencies of the event among the people who have not done the action before. 
We will assume for these experiments, that those people who have never done the action before will almost never do the action. 
In this experiment, we measure the relative contribution of these two components to the prior, as well as the expected frequency among those people who have done the action before.

### Method

#### Participants

We recruited 40 participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average 12 minutes and participants were compensated \$1.25 for their work.

#### Materials

We created thirty-one events organized into pairs or triplets from 5 different conceptual categories:
<!-- We created twenty-seven events organized into pairs or triplets from 5 different conceptual categories:  -->
food and drug (e.g. \emph{eats caviar}, \emph{eats peanut butter}), 
work (e.g. \emph{sells things on eBay}, \emph{sells companies}), 
clothing (e.g. \emph{wears a suit}, \emph{wears a bra}), 
entertainment (e.g. \emph{watches professional football}, \emph{watches space launches}), and 
hobbies (e.g. \emph{runs}, \emph{hikes}). 
Items were chosen to intuitively cover a range of likely frequencies of action, as well as to provide a minimal comparison to another item by having a common superordinate action (e.g. \emph{eating} caviar vs. peanut butter).

#### Procedure

For each event, participants were asked two questions, with associated dependent measures:

\begin{enumerate}
\item ``How many \{men, women\} have \textsc{done action} before?'' \\

Participants responded ``N out of every J.'' by entering a number for N and choosing J from a drop-down menu (options: \{1000 - 10 million\}; default: 1000).

\item ``For a typical \{man, woman\} who has \textsc{done action}  before, how frequently does he or she \textsc{do action}?''\\  

Participants responded ``M times in K.'' by entering a number for M and choosing K from a drop-down menu (options: \{week, month, year, 5 years\}; default: year).
\end{enumerate}

For example, one set of prompts read: ``How many men have smoked cigarettes before?''; ``For a typical man who has smoked cigarettes before, how frequently does he smoke cigarettes?''

The meaning of these questions were explained to participants on an instructions page before the experimental trials. 
After reading the instructions, participants were asked to recall the two types of questions asked in the experiment.
They responded to this attention check by selecting an option from a drop-down menu consisting of four options. 
They did this to test their understanding and attention for each of the two questions.

For the experimental trials, we anticipated there might be different beliefs about the frequency of events depending on whether the actor is male or female, so we asked about both genders. Participants answered both questions for both genders on each slide (4 questions total per slide, order of male / female randomized between-subjects), and every participant completed all 31 items in a randomized order.




<!-- The experiment began by having participants input the names of eight friends, family members, or people they knew well that they could answer some questions about. -->
<!-- Participants were told that these names would only be used to assist them in reasoning through the second part of the experiment.  -->

<!-- In the second phase of the experiment, participants were asked: "For each of the following people, how often does he or she \textsc{do action}?" -->
<!-- Participants had to fill in the blanks in sentences of the form: "\text{friend does action} ___ times per ___". -->
<!-- The first blank was a text box where participants could enter a number. -->
<!-- The second blank was a drop-down menu where participants could select a time interval from the following options: \{week, 2 weeks, month, 2 months, 6 months, year, 2 years, 5 years\} -->
<!-- They completed this task for each of their eight listed friends or family members. -->
<!-- To make the task slightly less tedious, there was an option where participants could set the time window for all eight of their responses. -->

<!-- In order to get sufficient data for actions that are relatively uncommon (e.g., *climbs mountains*, *writes novels*), if participants responded that all of their friends do the action with a frequency of "0 times" (in some interval), a follow-up question appeared that read: "Do you know anybody who has \textsc{done action} before?".  -->
<!-- If they responded "Yes", another follow-up question appeared that read: "How often do you think that person \textsc{does action}?", with the same dependent measure format as before. -->

<!-- Participants completed these elicitation trials for a random subset of 15 items, presented in a randomized order. -->
<!-- The experiment can be viewed at \url{http://stanford.edu/~mtessler/generics-paper/experiments/habituals/priors/friends-and-family-1.html} -->

<!-- \begin{figure*}[t] -->
<!-- \centering -->
<!--   \includegraphics[width=0.84\textwidth]{figs/tj-scatters-3} -->
<!--   \caption{ -->
<!--   Human acceptability judgments as a function of the log frequency of action (left) and speaker $S_2$ model predictions (right) for ninety-three unique items (event \textsc{x} frequency).  -->
<!--   Color denotes target-individual frequency of action (log scale), with lighter colors indicating more frequent actions.  -->
<!--   Actual frequency noted on x-axis for examples (left). -->
<!--   Error bars correspond to 95\% bootstrapped confidence intervals for the participant data and 95\% Bayesian credible intervals for the model predictions.  -->
<!--   Error bars suppressed and points jittered on left facet for visual clarity. -->
<!--   } -->
<!--   \label{fig:tjScatters} -->
<!-- \end{figure*} -->

<!-- #### Data preprocessing -->

```{r habituals-prior-data}
annualRates = list("5 years" = 1/5, "2 years" = 1/2,
                   "year" = 1, "6 months" = 2, "2 months" = 6,
                   "month" = 12, "2 weeks" = 26 ,"week" = 52)

d.hab.priors <- read.csv(paste(project.path, "data/habituals/priors/", "habituals-priors.csv", sep = ""))

d.hab.priors <- d.hab.priors %>%
  mutate(comparisonTime_men = as.character(comparisonTime_men),
         comparisonTime_women = as.character(comparisonTime_women),
         item = as.character(item),
         item = ifelse(item == "sell things on eBay", "sells things on eBay", item)) %>%
  rowwise() %>%
  mutate(
    mixture_male = nPersons_men / comparisonNum_men,
    mixture_female = nPersons_women / comparisonNum_women,
    dayRate_male = ifelse(comparisonTime_men == "week", nInstances_men / 7,
                   ifelse(comparisonTime_men == "month", nInstances_men / 30,
                   ifelse(comparisonTime_men == "year", nInstances_men / 365,
                   ifelse(comparisonTime_men == "5 years", 
                          nInstances_men / (5*365), -99)))),
    dayRate_female = ifelse(comparisonTime_women == "week", nInstances_women / 7,
                   ifelse(comparisonTime_women == "month", nInstances_women / 30,
                   ifelse(comparisonTime_women == "year", nInstances_women / 365,
                   ifelse(comparisonTime_women == "5 years", 
                          nInstances_women / (5*365), -99)))),
    annualRate_male = dayRate_male * 365,
    annualRate_female = dayRate_female * 365
  )


d.hab.priors.filtered <- filter(d.hab.priors, 
       !(
         (mixture_male == 0) ||   (mixture_female == 0) ||   
           (dayRate_male == 0) ||   (dayRate_female == 0)  )
       )
```


```{r habituals-prior-forwardsample}
# this is basically bootstrapping and combining using the structured model

d.hab.priors.samples <- data.frame()
for (i in 1:10){
  
  d.hab.priors.samples <- bind_rows(
    d.hab.priors.samples,
    d.hab.priors.filtered %>% 
    select(workerid, item, starts_with("annualRate"), 
           starts_with("mixture")) %>%
    rowwise() %>%
    mutate(
      stable_male = rbinom(n = 1, size = 1, prob = mixture_male),
       freq_male = ifelse(stable_male == 1, annualRate_male, 0.007),
       stable_female = rbinom(n = 1, size = 1, prob = mixture_female),
       freq_female = ifelse(stable_female == 1, annualRate_female, 0.007)
       ) %>%
    select(item, freq_male, freq_female) %>%
    gather(gender, val, freq_male, freq_female) %>%
  mutate(gender = gsub("freq_", "", gender))
  )
}

```

```{r habituals-prior-model}
hab.prior.bda.model <- '
var betaShape = function(p){
  return {a: p.g * p.d, b: (1-p.g) * p.d}
};

var model = function(){
	var mixtureParams = {
		male: {
      g: uniformDrift({a: 0, b: 1, width: 0.2}),
      d: uniformDrift({a: 0, b: 100, width: 5})
    },
		female: {
      g: uniformDrift({a: 0, b: 1, width: 0.2}),
      d: uniformDrift({a: 0, b: 100, width: 5})
    }
	};

	var mixtureShapes = {
		male: betaShape(mixtureParams.male),
		female: betaShape(mixtureParams.female)
	};

	mapData({data: data.mixture}, function(d){
    Beta(mixtureShapes[d.gender]).score(d.val) == -Infinity ? display(JSON.stringify(d)) : null
     // display(Beta(mixtureShapes[d.gender]).score(d.val))
		observe(Beta(mixtureShapes[d.gender]), d.val)
	})

	var stableFrequency = {
		male: {
			mu: uniformDrift({a: -5, b:10, width: 2}),
			sigma: uniformDrift({a:0, b:10, width: 1})
		},
		female: {
			mu: uniformDrift({a:-5, b:10, width: 2}),
			sigma: uniformDrift({a:0, b:10, width: 1})
		}
	}

	mapData({data: data.frequency}, function(d){
    Gaussian(stableFrequency[d.gender]).score(d.logval) == -Infinity ? 
        display(JSON.stringify(d)) : null
    // display(Gaussian(stableFrequency[d.gender]).score(d.logval))
		observe(Gaussian(stableFrequency[d.gender]), d.logval)
	})

	var existenceProb = {
		male: beta(mixtureShapes.male),
		female: beta(mixtureShapes.female)
	};

	var freqWhenPresent = {
		male: gaussian(stableFrequency.male),
		female: gaussian(stableFrequency.female)
	}

  var marginalFreq = {
    male: flip(existenceProb.male) ? freqWhenPresent.male : -5,
    female: flip(existenceProb.female) ? freqWhenPresent.female : -5,
  }

  return {
    mix_mean_male : mixtureParams.male.g,
    mix_mean_female : mixtureParams.female.g,
    mix_samplesize_male : mixtureParams.male.d,
    mix_samplesize_female : mixtureParams.female.d,
    stableFreq_mean_male : stableFrequency.male.mu,
    stableFreq_mean_female :stableFrequency.female.mu,
    stableFreq_sd_male : stableFrequency.male.sigma,
    stableFreq_sd_female :stableFrequency.female.sigma,
    marginalFreq_NA_male: marginalFreq.male,
    marginalFreq_NA_female: marginalFreq.female
  }
}
'
```


```{r habituals-prior-model-run, cache = T}
items <- levels(factor(d.hab.priors.filtered$item))
n_samples <- 500
rs.habituals.bda.prior <- data.frame()

for (it in items){
  
 df.prior.toPass <- list(
   frequency = d.hab.priors.filtered %>%
      filter(item == it) %>%
      select(item, starts_with("annualRate")) %>%
      gather(gender, val, -item) %>%
      mutate(gender = gsub("annualRate_", "", gender),
             logval = log(val)),
   mixture = d.hab.priors.filtered %>% 
      filter(item == it) %>%
      select(item, starts_with("mixture")) %>%
      gather(gender, val, -item) %>%
      mutate(gender = gsub("mixture_", "", gender)) %>%
      rowwise() %>%
      mutate(val = ifelse(val == 1, 0.9999, 
                          ifelse(val == 0, 0.000001, val)))
  )
 
  rs <- webppl(program_code = hab.prior.bda.model,
       model_var = "model",
       inference_opts = list(method = "MCMC", samples = n_samples,
                             burn = n_samples / 2),
       data = df.prior.toPass,
       data_var = "data")
  
  rs.habituals.bda.prior <- bind_rows(
    rs.habituals.bda.prior, 
    rs %>% 
      mutate(item = it) %>% 
      separate(Parameter, into = c("Variable", "Parameter", "Gender"))
  )
}
```

```{r habituals-prior-model-parameters, fig.width = 5, fig.height = 3}

example.habituals <- c("smokes marijuana", "smokes cigarettes","drinks coffee",
                       "climbs mountains", "hikes", "runs", 
                       "wears a suit", "wears a watch", "wears socks", 
                       "writes poems", "goes to the movies", "plays the banjo"
                       )

rs.habituals.bda.prior.summary <- rs.habituals.bda.prior %>%
  filter(Parameter == "mean") %>%
  group_by(Variable, Gender, item) %>%
  summarize(MAP = estimate_mode(value),
            cred_upper = hdi_upper(value),
            cred_lower = hdi_lower(value)) %>%
  ungroup()
rs.habituals.bda.prior.summary.wide <- left_join(
  rs.habituals.bda.prior.summary %>% filter(Variable == "mix") %>%
    select(-Variable) %>%
    rename(mix_mean = MAP, mix_upper = cred_upper, mix_lower = cred_lower),
  rs.habituals.bda.prior.summary %>% filter(Variable == "stableFreq") %>%
    select(-Variable) %>%
    rename(sfreq_mean = MAP, sfreq_upper = cred_upper, sfreq_lower = cred_lower)
)

figure.habituals.priors.scatter <- left_join(rs.habituals.bda.prior.summary.wide,
          d.hab.priors %>% select(item, category)) %>%
ggplot(.,
       aes( x = mix_mean, xmin = mix_lower, xmax = mix_upper,
            y = sfreq_mean, ymin = sfreq_lower, ymax = sfreq_upper,
             fill = category))+
    scale_shape_manual(values=c(21,22))+
  #scale_color_solarized()+scale_fill_solarized()+
  scale_fill_brewer(palette='Set1')+

  geom_point(size=4, inherit.aes = F, 
             aes(x = mix_mean,y = sfreq_mean,  fill = category,
            shape = Gender),  alpha = 0.9)+
  geom_errorbar(alpha = 0.5)+
  geom_errorbarh( alpha = 0.5)+
  xlab("% of Americans who have DONE ACTION")+
  ylab("Log Frequency of DOING ACTION")+
  #scale_
  coord_fixed(ratio = 1/8)+
  theme(legend.title = element_text(hjust=0),
        legend.position="bottom",
        legend.direction="horizontal") +
        scale_y_continuous(limits = c(-1.5, 6.5), 
                     breaks = c(0, 2.5, 4, 5.9),
                     labels = c("annually", "monthly", "weekly", "daily"))+
  geom_text_repel(data = rs.habituals.bda.prior.summary.wide %>%
                    filter(item %in% example.habituals, Gender == "female"), inherit.aes =F,
                  aes(x = mix_mean,y = sfreq_mean, label = item), force = 5, size = 3)
```


```{r habituals-prior-model-forwardSample}
rs.habituals.bda.prior.spread <- rs.habituals.bda.prior %>% 
  mutate(param = paste(Variable, Parameter, sep = "_")) %>%
  select(-Variable, -Parameter, -Chain) %>%
  spread(param, value) 

rs.habituals.bda.prior.samples <- data.frame()
for (i in 1:1){
  rs.habituals.bda.prior.samples <- bind_rows(
    rs.habituals.bda.prior.samples,
    rs.habituals.bda.prior.spread %>%
    rowwise() %>%
    mutate(
      a = mix_mean * mix_samplesize,
      b = (1 - mix_mean) * mix_samplesize,
      theta = rbeta(n = 1, shape1 = a, shape2 = b),
      stable = rbinom(n = 1, size = 1, prob = theta),
      logannualRate = ifelse(stable == 1, 
                             rnorm(n = 1, 
                                   mean = stableFreq_mean, 
                                   sd = stableFreq_sd), -5),
       annualRate =  exp(logannualRate)
    )
  )
}
```



```{r habituals-prior-marginals-fig, fig.width = 4, fig.height = 4, fig.cap="Reconstructed priors on frequency from the structured, prior elicitation task (Expt. 2a). Priors are reconstructed by bootstrapping and forward sampling using the structured model (data, blue) or using Bayesian data analysis on the structured model (model, red)."}

figure.habituals.priors.marginals <- bind_rows(
  d.hab.priors.samples %>% 
    mutate(src = 'data', 
           val = log(val)),
  rs.habituals.bda.prior.samples %>%
    rename(gender = Gender, val = logannualRate) %>%
    select(gender, item, val) %>%
    mutate(src = 'model')
  ) %>%
  filter(item %in% example.habituals,
         src == 'model') %>%
  mutate(item = factor(item, levels = example.habituals)) %>%
ggplot(. , aes( x = val, color = gender))+
  geom_density(size = 0.8, aes( y = ..scaled.. ))+
  facet_wrap(~item, nrow = 4)+
  scale_color_solarized()+
  xlab("Frequency")+
  ylab("Scaled prior density")+
  scale_y_continuous(limits = c(0,1), breaks = c(0, 1)) +
  theme(strip.text.y = element_text(angle = 0),
        legend.position = c(0.9, 0.12),
        axis.text.x = element_text(angle = 90))+
    scale_x_continuous(limits = c(-5, 8), 
                     breaks = c(-5, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))
```

```{r habituals-prior-figure, fig.width = 11, fig.height = 6, fig.cap="Propensity priors for events (Expt. 2a). Left: Inferred mixture mean parameter and stable frequency mean parameter for N events. Right: Reconstructed priors on frequency from the structured, prior elicitation task. Priors are reconstructed by forward sampling using the inferred parameters in the structured model."}
grid.arrange(figure.habituals.priors.scatter, figure.habituals.priors.marginals, nrow = 1)
```


<!-- Every response for a frequency of a person doing an action is a sample from $P_{event}(h)$. -->
<!-- We first transformed all responses onto the same scale of "times per year" (assuming 52 weeks per year; 12 months per year). -->
<!-- An empirical distribution can be found by making a histogram of these responses.  -->
<!-- The empirical distributions follows a kind-of log-normal distribution, with the additional feature of having a substantial probability mass at the frequency of 0 (for those individuals who never do the action).  -->
<!-- We thus log-transformed the empirical data.  -->
<!-- To avoid values of -Infinity, we set the responses of 0 to mean roughly "once every twenty years". -->

<!-- We took the log-transformed (and "-Infinity"-adjusted) raw data and binned each response to the closest bins on a grid with binwidth of 0.5 in the log-"times per year" scale. -->
<!-- This yields a grid of 23 points with frequencies ranging from "once every twenty years" (log-frequency = -3) to "twenty times per day" (log-frequency = 9). -->

### Data analysis and results

All participants responded correctly to both questions in the attention check trial, so all collected data was used in the analysis.
We built a Bayesian data analysis model for this prior elicitation task.
In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior resolves two technical difficulties.
First, it smooths effects that are clearly results of the response format. For example, a very common rating for certain events is \emph{1 time per year}. Presumably participants would be just as happy reporting \emph{approximately} 1 time per year (e.g., on average, 1.2 times per year); the raw data does not reflect this due to demands of the dependent measure.
Second, this methodology better captures the tails of the prior distribution (i.e., very frequent or very infrequent rates) which have relatively little data and need to be regularized by the analysis.

Question 1 elicits the proportion of people who have done an action before. 
This number can be interpreted as a probability coming from a Beta distribution: $d_{mixture} \sim \text{Beta}(\gamma_{mixture}, \xi_{mixture})$. 
Question 2 elicits the rate, or relative frequency, with which a person does the action.
This was modeled by a log-normal distribution: $\ln d_{stable} \sim \text{Gaussian}(\mu_{stable}, \sigma_{stable})$. 
Each item was modeled independently for each gender.
We learned about the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burnin.

The priors elicited cover a range of possible parameter values as intended (Figure \ref{fig:priorScatter}, scatter), resulting in parametrized distributions of dramatically different shapes (insets).  
We observe a correlation in our items between the mean \% of Americans who have \textsc{done action} before (Question 1) and the mean log-frequency of action (Question 2) ($r_{1,2} = 0.74$).
Items in our data set that tend to be more popular actions also tend to be more frequent actions (e.g., \emph{wears socks}) and visa-versa (e.g., \emph{steals cars}), though there are notable exceptions (e.g., \emph{plays the banjo} is not popular but done frequently when done at all, as is \emph{smokes cigarettes}; \emph{goes to the movies} is a popular activity though not done particularly often).
This diversity is relevant because the speaker model (Eq. \ref{eq:S1}) will endorse habitual sentences (e.g., \emph{Sam goes to the movies vs. the ballet.}) contingent on the shape of the prior distribution. 

Analagous to what was done with the prior knowledge for generic language, from the inferred parameters and the assumed functional forms (i.e., the mixture distribution), we can generate $P(h)$ modeled as a mixture of individuals who have done the action before and those that haven't.
That is, $P(h)$ was constructed by sampling $\lambda$ as follows:

\begin{align}
\phi & \sim \text{Beta}(\gamma_{mixture}, \xi_{mixture}) \nonumber \\ 
\ln h & \sim \begin{cases}
		\text{Gaussian}(\mu_{stable}, \sigma_{stable}) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{true} \label{eq:priorModel}  \\
				\text{Delta}(h=0.01) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{false} \\
		\end{cases}
\end{align}

We first sample a mixture weight $\phi$ from the posterior distribution over the mixture component hyperparameters $\text{Beta}(\gamma_{mixture}, \xi_{mixture})$, inferred from the first question of the experiment.
We then flip a coin weighted by $\phi$: $\text{Bernoulli}(\phi)$.
If the coin comes up heads (i.e., $\text{Bernoulli}(\phi) = \textsc{true}$), a frequency $h$ is sampled from the distribution of people who have done the action before. This distribution is a log-normal distribution with parameters inferred from the second question of the experiment $\ln h \sim \text{Gaussian}(\mu_{stable}, \sigma_{stable})$.
If the coin comes up tails, a frequency $h$ is sampled from the distribution of people who have not done the action before.
We make the simplifying assumption that people who have not done the action before will probably never do the action (expected frequency: once every hundred years).
Figure \ref{fig:habituals-prior-figure} (right) shows example inferred priors.

<!-- Some items show substantial differences between the genders (e.g., \emph{wears a bra}) and some show subtle differences (e.g., \emph{watches professional football}). -->
<!-- Recall that the prior distributions $P(h)$ used in the RSA are with respect to an alternative class of entities (e.g., other individuals).  -->
<!-- It's possible that when evaluating habitual statements under certain contexts, $P(h)$ is with respect to either *all people* or *people of the same gender*.  -->
<!-- For example, are the frequency conditions by which a man would qualify to "watch  football" (habitually) different than those by which a woman would qualify to "watch  football"? -->
<!-- We explore the possibility of different truth conditions for habituals of different gendered characters in Experiment 2b, for select items with priors that differ substantially by gender. -->


<!-- We analyze the data using a Bayesian approach to allow for a consistent integration into the computational model. -->
<!-- The data we would like to explain are: the bins $n_i \in \{-3, -2.5, ..., 8.5, 9\}$ that participants gave in response to items $i \in \{1, ..., 27\}$. -->
<!-- The data is explained as a function of subjective beliefs $P_i$, with $P_{ij}$ being participants' (as a collective) belief about the relative likelihood for bin $j$ for item $i$.  -->
<!-- Each $P_i$ defines a likelihood for our data, assuming an appropriate linking function. -->

<!-- Following @Franke2016, the linking function for this data treats each bin $n_{i}$ as a draw from a categorical distribution where the probability of bin $j$ is proportional to $\exp{(a\cdot P_i)}$, i.e., a soft-max choice from $P_{i}$.  -->
<!-- The higher parameter $a$, the more likely $n_{i}$ is the mode of $P_{i}$.  -->
<!-- For $a \rightarrow 0$, all bins become equiprobable. -->

<!-- \begin{eqnarray} -->
<!-- P_{i} &\sim& \text{Dirichlet}(1, ..., 1) \\  -->
<!-- a & \sim & \text{Gamma}(2,1) \\ -->
<!-- n_{i} &\sim & \text{Categorical}(\exp(a \cdot P_i)) -->
<!-- \end{eqnarray} -->

<!-- VIZ (as before)? -->

<!-- We assume each "binned" response is a sample from a multinomial distribution with unknown probability vector. -->
<!-- We put a Dirichlet-prior over this probability vector. -->

<!-- We built a Bayesian data analysis model for this prior elicitation task. -->
<!-- Question 1 elicits the proportion of people who have done an action before.  -->
<!-- We model this data as coming from a Beta distribution: $d_{1} \sim \text{Beta}(\gamma_{1}, \xi_{1})$.  -->
<!-- Question 2 elicits the rate, or relative frequency, with which a person does the action. -->
<!-- This was modeled by a log-normal distribution: $\ln d_{2} \sim \text{Gaussian}(\mu_{2}, \sigma_{2})$.  -->
<!-- Each item was modeled independently for each gender. -->
<!-- We implemented this model using the probabilistic programming language WebPPL \cite{dippl}, and found the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burnin. -->

<!-- The priors elicited cover a range of possible parameter values as intended (Figure \ref{fig:priorScatter}, scatter), resulting in parametrized distributions of dramatically different shapes (insets).   -->
<!-- We observe a correlation in our items between the mean \% of Americans who have \textsc{done action} before (Question 1) and the mean log-frequency  of action (Question 2) ($r_{1,2} = 0.74$). -->
<!-- Items that tend to be more popular actions also tend to be more frequent actions (e.g. \emph{wears socks}) and visa-versa (e.g. \emph{steals cars}), though there are notable exceptions (e.g. \emph{plays the banjo} is not popular but done frequently when done at all, as is \emph{smokes cigarettes}; \emph{goes to the movies} is a popular activity though not done very often).  -->
<!-- This diversity is relevant because the speaker model (Eq.~\ref{eq:S2}) will produce habitual sentences (e.g. \emph{Sam goes to the movies vs. the ballet.}) contingent on the shape of the prior distribution.  -->

<!-- From the inferred parameters and assumed functional forms, we get an inferred $P(p)$ modeled as a mixture of individuals with the possibility of carrying out the action and those without the possibility of doing it.  -->
<!-- That is, $P(p)$ was constructed by sampling $p$ as follows: -->

<!-- \begin{align} -->
<!-- \theta & \sim \text{Beta}(\gamma_{1}, \xi_{1}) \nonumber \\  -->
<!-- \ln \lambda & \sim \begin{cases} -->
<!-- 		\text{Gaussian}(\mu_{2}, \sigma_{2}) &\mbox{if } \text{Bernoulli}(\theta) = \textsc{t} \label{eq:priorModel}  \\ -->
<!-- 				\delta_{\lambda=-\infty} &\mbox{if } \text{Bernoulli}(\theta) = \textsc{f} \\ -->
<!-- 		\end{cases} -->
<!-- \end{align} -->

<!-- In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior in our language model resolves two technical difficulties: (1) It smooths effects that are clearly results of the response format^[ -->
<!-- For example, a very common rating is *1 time per year*. Presumably participants would be just as happy reporting *approximately* 1 time per year; the raw data does not reflect this due to demands of the dependent measure. -->
<!-- ] -->
<!-- and (2) it better captures the tails of the prior distribution which have relatively little data and need to be regularized by the analysis. -->
<!-- Figure \ref{fig:priorScatter} (right) shows example inferred priors. -->

<!-- Some items show substantial differences between the genders (e.g., *wears a bra*) and some show subtle differences (e.g., *watches professional football*).  -->
<!-- We will explore the possibility of different truth conditions for habituals of different gendered characters in Experiment 2, for select items with priors that differ substantially by gender. -->



## Experiment 2b: Endorsing habitual statements

We next explore the endorsements of habituals from the items whose priors were measured the prior elicitation task. 

### Method

#### Participants

We recruited 150 participants from MTurk.
To arrive at this number, we performed a Bayesian precision analysis to determine the minimum sample size necessary to reliably ensure 95\% posterior credible intervals no larger than 0.3 for a parameter whose true value is 0.5 and for which the data is a 2-alternative forced choice.
This analysis revealed a minimum sample size of 50 per item; since participants only completed about one third of the items, we recruited 150 participants.
The experiment took 4 minutes on average and participants were compensated \$0.55 for their work.

#### Procedure and materials

On each trial, participants were presented with a \emph{past frequency statement} for a given event of the form: "In the past M \{weeks, months, years\}, \textsc{person} \textsc{did x} 3 times".
For example, \emph{In the past month, Bill smoked cigarettes 3 times}.
The particular intervals used (\{weeks, months, years\}) were selected after examining the predictions of the speaker model (Eq. \ref{eq:S1}), for each item independently, to yield a range of predicted endorsement rates.
The items were the same as in the prior elicitation task.

Participants were asked whether they agreed or disagreed with the corresponding habitual sentence: "\textsc{person does x}" (e.g., \emph{Bill smokes cigarettes}).
Participants completed thirty-seven trials, which were composed of the thirty-one items from the prior elicitation task randomly paired with either a male or female character name.
Six of these items were then also paired with a name of the opposite gender (e.g., participants saw both a female character and a male character who drank beer).
<!-- These were used for an exploratory analysis on differences in endorsements by gender of the target character. -->

### Results

```{r habituals-prior-mixture-summarize}
d.hab.priors.mixture.summary <- d.hab.priors.filtered %>%
  select(item, starts_with("mixture_")) %>%
  gather(key, value, -item) %>%
  group_by(item) %>%
  multi_boot_standard( column = 'value' )
```

```{r habituals-endorsement}
d.hab.endorsement.catch <- read.csv(paste(project.path, "data/habituals/endorsement/", 
                    "habituals-endorsement-catch_trials.csv", sep = ""))

d.hab.endorsement <- read.csv(paste(project.path, "data/habituals/endorsement/", 
                    "habituals-endorsement.csv", sep = ""))

d.hab.endorsement <- left_join(
  left_join(d.hab.endorsement, 
           d.hab.endorsement.catch %>% 
             select(workerid, pass)
           ) %>%
  filter(pass == 1),
  d.hab.priors.mixture.summary %>% rename(habitual = item, mixture = mean, mix_low = ci_lower, mix_high = ci_upper)) %>%
  mutate(response = ifelse(as.character(response) == "agree-key", 1, 0)) %>%
  rowwise() %>%
  mutate(
    annualRate = annualRates[[as.character(time_period)]]*n_instances,
    logAnnualRate = log(annualRate)
  )


d.hab.endorsement.bayes <- d.hab.endorsement %>% 
  group_by(habitual, logAnnualRate, time_period) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))

example.habituals <- c("climbs mountains_year", "hikes_year", "runs_year",
                       "goes to the movies_month", "smokes cigarettes_month")
fig.habituals.endorsement.vs.freq <- 
  ggplot(d.hab.endorsement.bayes, aes( x = logAnnualRate, y = MAP_h, ymin = low, ymax = high, fill = logAnnualRate))+
  geom_jitter(width = 0.1, shape = 21)+
  ylab("Human endorsement probability")+
  xlab("Frequency of target")+
  geom_label_repel(data = d.hab.endorsement.bayes %>% 
                    mutate(sentence = paste(habitual, time_period, sep = "_")) %>%
                    filter(sentence %in% example.habituals), aes( label = habitual ),
                       fontface = 'bold', color = 'white',
    box.padding = unit(0.35, "lines"),
    point.padding = unit(0.5, "lines"),
    segment.color = 'grey50')+
  scale_x_continuous(limits = c(-1,6), 
                     breaks = c(-1, 0, 0.7, 2.5, 4, 5.9),
                     labels = c("three years", "annually", "bi-annually", "monthly", "weekly", "daily"))+
  scale_y_continuous(limits = c(-0.01, 1.01), 
                     breaks = c(0, 0.5, 1))+
  theme(axis.text.x = element_text(angle = 90))+
  coord_fixed(ratio = 7)+
  guides(fill = F)
```


```{r habituals-endorsement-gender-exploration-scatter, fig.cap='Exploratory analysis of endorsements of habitual statements by gender of target character (e.g., "Mary drinks beer" vs. "John...") for six events that displayed appreciable differences in the prior elicitation. Labeled points all correspond to the time period of 3 times / month.', eval=F}
gendered.items <- c("does cocaine", "drinks beer", "drinks coffee",
                    "wears a bra", "wears a watch", "wears a suit")

d.hab.bayes.gendered <- d.hab.endorsement %>%
  filter(habitual %in% gendered.items) %>%
  group_by(habitual, characterGender, time_period) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))


d.hab.bayes.gendered.wide <- left_join(
    d.hab.bayes.gendered %>%
    select(habitual, characterGender, time_period, MAP_h, low, high) %>%
    filter(characterGender == "female") %>%
    rename(map_female = MAP_h, low_female = low, high_female = high) %>%
    select(-characterGender),
  d.hab.bayes.gendered %>%
    select(habitual, characterGender, time_period, MAP_h, low, high) %>%
    filter(characterGender == "male") %>%
    rename(map_male = MAP_h, low_male = low, high_male = high) %>%
    select(-characterGender)
)

r2.habituals.gendered <- compute_r2(d.hab.bayes.gendered.wide, "map_female", "map_male")

ggplot(d.hab.bayes.gendered.wide, aes( x = map_female, y = map_male,
                 xmin = low_female, ymin = low_male,
                 xmax = high_female, ymax = high_male, fill = habitual))+
  geom_text_repel(data = d.hab.bayes.gendered.wide %>%
                    filter(time_period== "month"), aes(label = habitual,
                                                       color = habitual),
                  force = 3, nudge_x = -1, segment.size = 0.3)+
  guides(fill = F, color = F)+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_point(shape = 21)+
  geom_errorbar(alpha = 0.3) + geom_errorbarh(alpha = 0.3)+
  scale_x_continuous(limits = c(0,1), breaks = c(0,0.5,1))+
  scale_y_continuous(limits = c(0,1), breaks = c(0,0.5,1))+
  coord_fixed()+
  xlab("Habitual endorsement (female character)")+
  ylab("Habitual endorsement (male character)")+
  scale_fill_solarized()+
  scale_color_solarized()

```

```{r habituals-endorsement-gender-exploration, results="asis", eval = F}
gendered.items <- c("does cocaine", "drinks beer", "drinks coffee",
                    "wears a bra", "wears a watch", "wears a suit")

d.hab.bayes.gendered <- d.hab.endorsement %>%
  filter(habitual %in% gendered.items) %>%
  group_by(habitual, characterGender) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))


left_join(
    d.hab.bayes.gendered %>%
    select(habitual, characterGender, MAP_h, low, high) %>%
    filter(characterGender == "female") %>%
    mutate(female = paste(as.character(round(MAP_h, 2)), " [",
                        as.character(round(low, 2)), ", ",
                        as.character(round(high, 2)), "]", sep = "")) %>%
    select(-MAP_h, -low, -high,  -characterGender),
  d.hab.bayes.gendered %>%
    select(habitual, characterGender, MAP_h, low, high) %>%
    filter(characterGender == "male") %>%
    #rename(male_MAP, male_low = low, male_high = high) %>%
    mutate(male = paste(as.character(round(MAP_h, 2)), " [",
                        as.character(round(low, 2)), ", ",
                        as.character(round(high, 2)), "]", sep = "")) %>%
    select(-MAP_h, -low, -high, -characterGender)
    #rename(female_MAP, female_low = low, female_high = high)
) %>% xtable(.,
         caption = c("Exploratory analysis of endorsements of habitual statements by gender for six properties of interest, collapsed across frequencies."),
        label = c("tab:habituals-endorsement-gender")) %>%
print(., type = "latex", 
      tabular.environment = "tabularx", width = "\\textwidth",
      scalebox='0.75',
      include.rownames = FALSE, comment = F)
```



<!-- We find no differences between endorsements of the habitual of characters with male and female names, and overall, the mean endorsements by gender are strongly correlated $r(93) = 0.91$.  -->
<!-- This may be because the felicity of habitual sentences depends on a comparison to individuals of both genders (i.e, the \emph{contrast class} is other people; not just other men or women).  -->
<!-- Less interestingly, the lack of a difference may be the result of gender being not very salient in our paradigm, perhaps because the names used were not sufficiently gendered. -->

```{r habituals-regression-models}

glm.hab.endorse.freq <- glm(response ~ logAnnualRate, 
              data = d.hab.endorsement, family = 'binomial')

glm.hab.endorse.freq.distinct <- glm(response ~ logAnnualRate + mixture, 
              data = d.hab.endorsement, family = 'binomial')


d.hab.endorse.freq.mix.uniq <- unique(select(
    d.hab.endorsement, habitual, logAnnualRate, time_period, 
    mixture, mix_low, mix_high
  ))

d.hab.endorse.freq.mix.uniq.lower <- d.hab.endorse.freq.mix.uniq %>% 
  select(-mixture) %>% 
  rename(mixture = mix_low)

d.hab.endorse.freq.mix.uniq.upper <- d.hab.endorse.freq.mix.uniq %>% 
  select(-mixture) %>% 
  rename(mixture = mix_high)



d.hab.endorse.regression.freq <- left_join(
  d.hab.endorsement.bayes,
  d.hab.endorse.freq.mix.uniq %>%
    mutate(
      prediction = predict(glm.hab.endorse.freq, ., type = "response"), 
      prediction_lower = prediction,
      prediction_upper = prediction,
      src = "regression_freq"
      )
)

r2.hab.n <- length(d.hab.endorse.regression.freq$MAP_h)

r2.habituls.regression.freq <- compute_r2(d.hab.endorse.regression.freq,
                                          "MAP_h", "prediction")

mse.habituls.regression.freq <- compute_mse(d.hab.endorse.regression.freq,
                                          "MAP_h", "prediction")

r2.habituls.regression.infrequent.freq <- d.hab.endorse.regression.freq %>%
  filter(logAnnualRate < 1.1)

r2.hab.infrequent.n <- length(r2.habituls.regression.infrequent.freq$MAP_h)

r2.habituls.infrequent.regression.freq <- compute_r2(r2.habituls.regression.infrequent.freq,
                                          "MAP_h", "prediction")

mse.habituls.infrequent.regression.freq <- compute_mse(r2.habituls.regression.infrequent.freq,
                                          "MAP_h", "prediction")


# n.b.: Since distinctiveness is being captured by "mix", the lower the "mix", the higher the endorsement. Thus, lower bound of errorbars are computed with upper estimate of "mix"
d.hab.endorse.regression.freq.mix <- left_join(
  d.hab.endorsement.bayes,
  d.hab.endorse.freq.mix.uniq %>%
    mutate(
      prediction = predict(glm.hab.endorse.freq.distinct, ., type = "response"),
      prediction_lower =  predict(glm.hab.endorse.freq.distinct, d.hab.endorse.freq.mix.uniq.upper, type = "response"),
      prediction_upper =  predict(glm.hab.endorse.freq.distinct, d.hab.endorse.freq.mix.uniq.lower, type = "response"),
      src = "regression_freq_distinct"
      )
)


r2.habituls.regression.freq.distinct <- compute_r2(d.hab.endorse.regression.freq.mix,
                                          "MAP_h", "prediction")

mse.habituls.regression.freq.distinct <- compute_mse(d.hab.endorse.regression.freq.mix,
                                          "MAP_h", "prediction")

d.hab.endorse.regression <- bind_rows(d.hab.endorse.regression.freq, d.hab.endorse.regression.freq.mix) %>%
  mutate(sqErr = (MAP_h-prediction)^2)


# ggplot(d.hab.endorse.regression,
#        aes (x = prediction, y = MAP_h, ymin = low, ymax = high, color = mixture))+
#   geom_errorbar(alpha = 0.1)+
#   geom_abline(intercept = 0, slope = 1, lty = 3)+
#   # geom_text_repel(data = d.hab.endorse.regression.freq.distinct %>%
#   #                   filter(sqErr > 0.1),
#   #                 aes(label = paste(habitual, time_period)), force = 5, size = 3)+
#   geom_point()+
#   xlim(0,1)+
#   ylim(0,1)+
#   coord_fixed()+
#   xlab("Logistic model prediction")+
#   ylab("Human habitual endorsement")+
#   facet_wrap(~src)



# with(d.hab.endorse.regression %>%
#        filter(src == "regression_freq"), 
#      cor(prediction, MAP_h, use = "pairwise.complete.obs"))^2
# 
# with(d.hab.endorse.regression %>%
#        filter(src == "regression_freq_distinct"), 
#      cor(prediction, MAP_h, use = "pairwise.complete.obs"))^2
```

```{r habituals-fullmodel}
n_chains <- 3
n_samples <- 100000
burn <- n_samples / 2
lg <- 20
model_prefix <- "results-habituals-jointModel-S1-"

# m.hab.samp <- data.frame()
# m.hab.fixed.samp <- data.frame()

# for (i in seq(1, n_chains)){
#   mi <- fread(paste(project.path,  "models/habituals/results/",
#                     model_prefix, "smtncs_habitual-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.hab.samp <- bind_rows(m.hab.samp, mi %>% mutate(chain = i))
# 
#   mi.fixed <- fread(paste(project.path,  "models/habituals/results/",
#                     model_prefix, "smtncs_some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.hab.fixed.samp <- bind_rows(m.hab.fixed.samp, mi.fixed %>% mutate(chain = i))
# 
# }
# 
# save(m.hab.samp,
#      file = paste(project.path,  "models/habituals/results/", model_prefix, "habitual-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))
# 
# save(m.hab.fixed.samp,
#      file = paste(project.path,  "models/habituals/results/", model_prefix, "some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))


load(paste(project.path,  "models/habituals/results/", model_prefix, "habitual-",
                    n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))

load(paste(project.path,  "models/habituals/results/", model_prefix, "some-",
                    n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))

m.hab.somemodel.endorsement <- m.hab.fixed.samp %>%
  filter(type == 'predictive') %>%
  rename(habitual = B, time_period = D, binned_freq = E) %>%
  group_by(habitual, time_period, binned_freq) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.hab.fullmodel.endorsement <- m.hab.samp %>%
  filter(type == 'predictive') %>%
  rename(habitual = B, time_period = D, binned_freq = E) %>%
  group_by(habitual, time_period, binned_freq) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.hab.endorse.rsa <- bind_rows(
  left_join(
    d.hab.endorsement.bayes,
    m.hab.fullmodel.endorsement
  ) %>%
    mutate(src = "habituals_model"),
  left_join(
    d.hab.endorsement.bayes,
    m.hab.somemodel.endorsement
  ) %>%
    mutate(src = "some_model")
)

r2.habituals.rsa.fixed <- compute_r2(
  m.hab.endorse.rsa %>% filter(src == "some_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.fixed <- compute_mse(
  m.hab.endorse.rsa %>% filter(src == "some_model"),
                                          "MAP_h", "MAP")


r2.habituals.rsa.uncertain <- compute_r2(
  m.hab.endorse.rsa %>% filter(src == "habituals_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.uncertain <- compute_mse(
  m.hab.endorse.rsa %>% filter(src == "habituals_model"),
                                          "MAP_h", "MAP")


habituals.rsa.uncertain.infrequent <- m.hab.endorse.rsa %>% filter(src == "habituals_model", logAnnualRate < 1.1)

r2.hab.rsa.infrequent.n <- length(habituals.rsa.uncertain.infrequent$MAP_h)

r2.habituals.rsa.uncertain.infrequent<- compute_r2(habituals.rsa.uncertain.infrequent,
                                          "MAP_h", "MAP")

mse.habituals.rsa.uncertain.infrequent <- compute_mse(habituals.rsa.uncertain.infrequent,
                                          "MAP_h", "MAP")




habituals.endorsement.models <- bind_rows(
  d.hab.endorse.regression,
  left_join(
    m.hab.endorse.rsa %>% 
    rename(prediction = MAP, prediction_lower = cred_lower, prediction_upper = cred_upper),
    d.hab.endorse.regression %>% 
    select(habitual, time_period, logAnnualRate, mixture)
  )
) %>%
  mutate(src = factor(src, levels = c( 
                                      "some_model",
                                      "habituals_model",
                                      "regression_freq", 
                                      "regression_freq_distinct"
                                     ),
                      labels = c(
                                 "Speaker model - fixed semantics",
                                 "Speaker model - uncertain semantics",
                                 "Frequency alone", "Frequency + Distinctiveness"
                                 ))) %>%
  ggplot(., aes ( x = prediction, xmin = prediction_lower, xmax = prediction_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = logAnnualRate))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.7)+
  geom_errorbarh(alpha = 0.7)+
  geom_point(shape = 21)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human habitual endorsement")+
  facet_wrap(~src, nrow = 2)+
  theme(legend.position = "bottom")
```

```{r habituals-simulation-model}
l0.hab.model <- '
var probability = function(Dist, x) {
    return Math.exp(Dist.score(x));
}
var betaShape = function(p){
  return {a: p.g * p.d, b: (1-p.g) * p.d}
};

var probBins = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9];

var targetUtterance = "habitual";


var roundTo3 = function(x){
  return Math.round(x * 10000) / 10000
}

var lowerBound = -5, upperBound = 10, binWidth = 0.5;

var stateBins =  _.range(
  lowerBound, upperBound, binWidth
)

var thetaBins = map2(function(b1, b2){
  var diff = Math.abs(b2 - b1) / 2;
  return roundTo3(diff+ b1);
}, stateBins.slice(0, stateBins.length-1), stateBins.slice(1))

var utterancePrior = Infer({model: function(){
  return uniformDraw([targetUtterance,"silence"])
}});

var thetaPrior = Infer({model: function(){
 return uniformDraw(thetaBins)
}});

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=="silence"? true :
         utt=="some"? state> _.min(thetaBins):
         true
}


var priorParams = data.priorParams[0];

var stable_female_params = betaShape({
  g: priorParams.mixture_female_mean, d: priorParams.mixture_female_samplesize
})

var stable_male_params = betaShape({
  g: priorParams.mixture_male_mean, d: priorParams.mixture_male_samplesize
})

// this marginalizes out the mixture parameter
var statePrior = Infer({model: function(){
  var gender = flip(0.5) ? "female" : "male";
  return gender == "female" ? // 
    flip(
        categorical({
          vs: probBins,
          ps: map(function(b) {
            return probability(Beta(stable_female_params), b) + Number.EPSILON
          }, probBins )
        })
    ) ? // stable female distribution
      categorical({
        vs: stateBins,
        ps: map(function(b) {
          return probability(Gaussian({
                mu: priorParams.stableFreq_female_mean, 
                sigma: priorParams.stableFreq_female_samplesize
          }), b) + Number.EPSILON
        }, stateBins )
      }) : // unstable female distribution
    _.min(stateBins) : 
    flip(        
      categorical({
          vs: probBins,
          ps: map(function(b) {
            return probability(Beta(stable_male_params), b) + Number.EPSILON
          }, probBins )
      })
    ) ? 
      categorical({
        vs: stateBins,
        ps: map(function(b) {
          return probability(Gaussian({
                mu: priorParams.stableFreq_male_mean, 
                sigma: priorParams.stableFreq_male_samplesize
          }), b) + Number.EPSILON
        }, stateBins )
      }) :
    _.min(stateBins)
}});

var listener0 = cache(function(utterance) {
  Infer({model: function(){
    var state = sample(statePrior)
    var state_prior = sample(statePrior)
    var theta = utterance == "habitual" ? sample(thetaPrior) : -99
    condition(meaning(utterance, state, theta))
    return {
      state_Posterior: state, 
      state_Prior: state_prior
  }
 }})}, 10000)

var continuousListener0 = function(utterance) {
  Infer({model: function(){
   var state = flip(0.5) ? 
        flip(beta(stable_female_params)) ? 
          gaussian({
                mu: priorParams.stableFreq_female_mean, 
                sigma: priorParams.stableFreq_female_samplesize
          }) : delta({v:-5}) :
        flip(beta(stable_male_params)) ? 
          gaussian({
                mu: priorParams.stableFreq_male_mean, 
                sigma: priorParams.stableFreq_male_samplesize
          }) : delta({v:-5})
   var state_prior = flip(0.5) ? 
        flip(beta(stable_female_params)) ? 
          gaussian({
                mu: priorParams.stableFreq_female_mean, 
                sigma: priorParams.stableFreq_female_samplesize
          }) : delta({v:lowerBound}) :
        flip(beta(stable_male_params)) ? 
          gaussian({
                mu: priorParams.stableFreq_male_mean, 
                sigma: priorParams.stableFreq_male_samplesize
          }) : delta({v:lowerBound})
    var theta = uniform(lowerBound, upperBound)
    condition(
      (utterance == "habitual") ? 
      state > theta : 
      (state > _.min(lowerBound))
    )
    return {
      state_Posterior: state, 
      state_Prior: state_prior
  }
 }, method: "rejection", samples: 10000, burn:5000, verbose: T})}

continuousListener0(data.utt[0])
'
```

```{r habituals-model-insets, fig.width = 4.75, fig.height = 1.5, cache = T}
example.habituals.actions <- separate(data.frame(example.habituals), 
                                      example.habituals, 
                                      into=c("action", "time_period"), sep = "_")$action


m.hab.fullmodel.prior.parameters <- m.hab.samp %>%
  filter(type == "prior", B %in% example.habituals.actions) %>%
  rename(action = B, variable = C, gender = D, parameter = E) %>%
  group_by(action, variable, gender, parameter) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

# use MAP estimates to generate L(h | generic) & L(h | silence) predictions

m.hab.fullmodel.prior.parameters.tidy <- m.hab.fullmodel.prior.parameters %>%
  ungroup() %>%
  select(action, variable, gender, parameter, MAP) %>%
  mutate(param = paste(variable, gender, parameter, sep = "_")) %>%
  select(-variable, -gender, -parameter) %>%
  spread(param, MAP)

hab.listener.predictions <- data.frame()
  
for (p in example.habituals.actions){
 priorParams <- m.hab.fullmodel.prior.parameters.tidy %>% filter(action == p) 
 
 inputData = list(priorParams = priorParams,
                  utt = "habitual")
 
 l0.rs <- webppl(l0.hab.model, data = inputData, data_var = "data")
 
 hab.listener.predictions <- bind_rows(
   hab.listener.predictions, 
   l0.rs %>% select(Parameter,value) %>% mutate(action = p)
   )
 
}

# m.gen.fullmodel.target.prevalence <- m.samp %>%
#   filter(type == "withinKind") %>%
#   group_by(param, property, category) %>%
#   summarize(MAP = estimate_mode(val),
#             cred_upper = hdi_upper(val),
#             cred_lower = hdi_lower(val))

# m.gen.speakerBeliefs <- m.samp %>%
#   filter(type == "withinKind") %>%
#   mutate(Sentence = paste(category, property)) %>%
#   filter(Sentence %in% example.generics) 

# m.gen.speakerBeliefs <- m.samp.prev.params <- m.samp %>%
#     filter(type == "targetPrevalence") %>%
#     mutate(Sentence = paste(category, property)) %>%
#     filter(Sentence %in% example.generics) %>%
#     mutate(parameter = paste(param, property, category, sep = "_")) %>%
#     select(-param, -property, -category, -chain, -type) %>%
#     group_by(parameter) %>%
#     mutate(iteration = ave(parameter==parameter, parameter, FUN=cumsum)) %>%
#     ungroup() %>%
#     separate(parameter, into = c("parameter", "property", "category"), sep= "_") %>%
#     group_by(category, property, iteration) %>%
#     spread(parameter, val) %>%
#     rowwise() %>%
#     mutate(
#       a = mean*sampleSize,
#       b = (1-mean)*sampleSize,
#       val = rbeta(n = 1, shape1 = a, shape2 = b)
#       ) %>%
#     ungroup()

# gen.inset.distributions <- bind_rows(
#   gen.listener.predictions %>%
#     mutate(category = NA),
#   m.gen.speakerBeliefs %>%
#     select(property, val) %>%
#     rename(value = val) %>%
#     mutate(Parameter = "speakerBeliefs")
# )
# 
# 
# category.text.labels <- data.frame(property = c("dont eat people", 
#                       "carry malaria", "lay eggs",  "are female", "have spots"),
#              category = c("Tigers", "Mosquitos", "Robins", "Robins", "Leopards"),
#              x = c(0.3, 0.3, 0.47, 0.05, 0.6),
#             y = c(0.45, 0.5, 0.45, 0.5, 0.26))



habituals.endorsement.insets <- hab.listener.predictions %>% 
    mutate(Parameter = factor(Parameter, levels = c("state_Prior",
                                                  "state_Posterior"
                                                  ),
                            labels = c("Listener Prior (Posterior given Silence)",
                                       "Listener Posterior given Habitual")),
         action = fct_relevel(action,
                                "climbs mountains", "hikes", "runs",
                              "goes to the movies", "smokes cigarettes"
                              ) ) %>%
  ggplot(., aes( x = value, fill = Parameter, color = Parameter, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 4, size = 1)+
  facet_wrap(~action, nrow = 1)+
  # geom_text_repel(data = category.text.labels,
  #                 aes(label = category, x = x , y = y),
  #                 inherit.aes = F, color = "#2b83ba")+
  #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
  #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
  scale_fill_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  scale_color_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  scale_alpha_manual(values = c(0.6, 0.4))+#, 0))+
  #scale_linetype_manual(values = c(3, 4, 2, 1))+
  #scale_linetype_manual(values = c(3, 4, 1))+
  #scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
  scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
  xlab("Prevalence") +
  ylab("Scaled probability density")+
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 90))+
      scale_x_continuous(limits = c(-5, 8), 
                     breaks = c(-5, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))

```

```{r habituals-endorsement-figure, fig.width=11, fig.height=8}
grid.arrange(fig.habituals.endorsement.vs.freq, 
             habituals.endorsement.models,
             habituals.endorsement.insets, ncol = 2,
             layout_matrix = cbind(c(1,1,3), c(2,2,3)))
```

<!-- For the six items for which data for characters of both genders were collected for each participant, we analyzed only the data from participants' first presentation of the item. -->
Parallel to our analysis of generic language endorsements, we articulate a set of simple regression models and an alternative to our uncertain-threshold RSA model in order to understand the data and our model.
For all analyses, we collapse across gender of the target character.
<!-- Before comparing models, we perform an exploratory analysis on a subset of our data, exploring whether or not endorsements for items whose priors differed by gender will also differ. -->

<!-- NOTE: Alternatively: We could not discuss this analysis (or, in a footnote). Since participants judged both genders for these items, we could only analyze their first gender and drop their second, for consistency with the other items. -->

<!-- For our six items that showed substantial variability in the elicited priors between genders, we explore differences in endorsements.  -->
<!-- Differences in endorsements could suggest that the *comparison class* of entities in the prior distribution used in our model might be gender-specific (i.e., when evaluating "Mary drinks beer." participants implicitly consider the distribution of frequencies for other women and not other men). -->
<!-- Endorsement probabilities and 95\% credible intervals for these six items at each time interavl for each gender are showed in Figure \ref{fig:habituals-endorsement-gender-exploration-scatter}. -->
<!-- We see no appreciable differences in endorsements for the two genders.  -->
<!-- This suggests that, at least the minimal experimental contexts we are exploring, the comparison class is not different for female vs. male target characters. -->
<!-- For all remaining analyses, we collapse across character gender when analyzing endorsements. -->

#### Frequency baseline

The simplest alternative hypothesis for habitual language endorsement is that the degree to which the habitual statement is good is the frequency itself. 
This is unlikely to be the case based on Figure \ref{fig:habituals-endorsement-figure} (top-left), which shows the correspondence between the frequency of the event (transformed to a log times-per-year scale) and the human-judged felicity of the corresponding habitual sentence. 
First, it is clear that not all habitual statements are strongly assented to, even though it is true that all the stimuli involved an agent doing the action with some non-zero frequency (at least 3 times before).
It is also clear that a habitual sentence can receive strong agreement even when the actions are very infrequent (3 times in a 5-year interval; e.g. \emph{writes novels}, \emph{climbs mountains}), arguing against the second alternative.
We see in addition that even when actions are done relatively frequently (e.g. 3 times in a one month interval), some habitual sentences receive less than full endorsement (e.g. \emph{wears socks}, \emph{drinks coffee}). 
In our data, actions completed with a high frequency (3 times in a one week interval) receive at a minimum 75\% endorsement, though there is still variability among them (e.g. between 10-25\% of people disagree with \emph{wears a watch} and \emph{wears a bra}).
Overall, the raw log-frequency of action predicts only a fraction of the variability in responses ($r^2(`r r2.hab.n`) = `r r2.habituls.regression.freq`$; MSE=$`r mse.habituls.regression.freq`$). 
In addition, for actions that are done on the time scale of years or longer (lower median of frequency), frequency itself no longer explains the endorsements ($r^2(`r r2.hab.infrequent.n`) = `r r2.habituls.infrequent.regression.freq`$; MSE =$`r mse.habituls.infrequent.regression.freq`$)

We observe that none of our items receive less than 25\% endorsement (i.e., a maximum of about 75\% of participants disagree with the felicity of the utterance).
This reflects the fact that these statements are not altogether *false* even though the action is done very rarely.


#### Frequency and distinctiveness

In the prior elicitation task (Expt. 2a), we saw that items differed in both their mean expected frequency as well as the proportion of people who have done / do the action (i.e., the mixture parameter in the frequency prior $P(h)$).
With generics, we constructed a regression model that used empirical *cue validity* as a predictor of generic endorsement. Here, we articulate a regression model that uses participants' responses to the question about the mixture parameter $\phi$ (i.e., the proportion of people who have done the action before) to predict habitual endorsement. This is analagous to the regression model based on cue validity and prevalence in the domain of generics.

```{r habituals-distinctiveness-counterexamples}

format_endorsment_ci <- function(
  df, estimate = "MAP_h", low = "low",
  high = "high", sigfigs = 2){
  return(paste(
    round(df[[1,estimate]], sigfigs),
    " [",
    round(df[[1,low]], sigfigs),
    ", ",
    round(df[[1,high]], sigfigs),
    "]", sep = ""))
}

movies.year.data <- format_endorsment_ci(filter(d.hab.endorsement.bayes, habitual == "goes to the movies", time_period == "year"))

movies.year.regression.distinct <- format_endorsment_ci(filter(d.hab.endorse.regression.freq.mix, habitual == "goes to the movies",
                                                        time_period == "year"),
                                                        estimate = "prediction",
                                                        low = "prediction_lower",
                                                        high = "prediction_upper")

banjo.2years.data <- format_endorsment_ci(filter(d.hab.endorsement.bayes, habitual == "plays the banjo", time_period == "2 years"))

banjo.2years.regression.distinct <- format_endorsment_ci(filter(d.hab.endorse.regression.freq.mix, habitual == "plays the banjo",
                                                        time_period == "2 years"),
                                                        estimate = "prediction",
                                                        low = "prediction_lower",
                                                        high = "prediction_upper")
```

We find that this model is able to explain more of the variance in endorsements ($r^2(`r r2.hab.n`) = `r r2.habituls.regression.freq.distinct`$; MSE=$`r mse.habituls.regression.freq.distinct`$). 
Actions that are more rare across people (e.g., writing novels) are endorsed more overall. 
Still, this model is not able to capture some of the subtle sensitivities in endorsments.
For example, endorsements for *goes to the movies* for a person who has *gone to the movies* three times in the past year are about $`r movies.year.data`$, while this model predicts quite lower judgments $`r movies.year.regression.distinct`$.
Going to the movies is relatively common and going three times in a year is not very frequent, and yet people still strongly endorse the habitual. 
On the other hand, playing the banjo three times in the past two years is not very strong evidence for the habitual $`r banjo.2years.data`$.
Nevertheless, because playing the banjo is a distinct skill, this simple regression model wants to endorse the habitual strongly in this case $`r banjo.2years.regression.distinct`$.


#### Fixed-threshold RSA model

The above regression models are too rigid to explain the flexibility in habitual endorsements. 
Our probabilistic RSA model has more flexibility than these models because of the introduction of prior knowledge. 
Additionally, our theory posits that the flexibility in endorsements comes from an interaction of the prior knowledge with an uncertain threshold.
To isolate the contribution of this interaction (prior knowledge with an uncertain threshold), we first show the predictions of a model that lacks the uncertainty over the threshold (i.e., it has a fixed threshold).

We used the pragmatic speaker model $S_1$ (Eq. \ref{eq:S1}) with the priors elicited above (Expt. 2a) to predict felicity judgments in Expt. 2b, assuming the target propensity ($h$, to be conveyed by $S_1$) is the provided frequency (e.g., 3 times in the past year).
Because we observe no difference between the felicity judgments for habituals of male and female characters, we use a 50\% mixture of the inferred priors for each gender to construct a single frequency distribution $P(h)$ across individuals.
The RSA model has a single free parameter---the speaker optimality parameter, $\alpha_1$, in Eq. \ref{eq:S1}. 
As we did for the generics case study, we use Bayesian data analytic techniques to integrate over these parameters \cite{LW2014}, comparing the posterior predictive distribution to the empirical data in Expt. 2b.
To construct the posterior predictive distribution over responses, we collected 2 MCMC chains of 100,000 iterations, discarding the first 50,000 iterations for burn in.
<!-- The Maximum A-Posteriori value and 95\% highest probability density interval for $\alpha_1$ is 19.3 [14.9,19.9] and $\alpha_2$ is 1.5 [1.4,1.6]. -->


On each trial of the experiment, the participant was told a person did a particular past frequency and was asked to evaluate the corresponding habitual statements.
A fixed threshold model only conveys that the a person *has done this action before*, analagous to how the quantifier *Some* conveys that there exists at least one entity with the feature.
Under this hypothesis, all of the items used in our experiment would qualify as good habitual statements as the target character always did the action three times during some time window.
Incorporating this semantics into the RSA framework makes it difficult for the speaker model (Eq. \ref{eq:S1}) to differentiate between the various habitual statements (Figure \ref{fig:habituals-endorsement-figure}, top-right: upper-left facet).
The model has to decide between producing silence (which is always true) and a very uninformative statement (which is almost always true). The relatively small difference in information-gain on behalf of the listener is reflected in the small dynamic range of endorsments for the speaker. Overall, this model does poorly at predicting the range of endorsement data ($r^2(`r r2.hab.n`) = `r r2.habituals.rsa.fixed`$; MSE=$`r mse.habituals.rsa.fixed`$).

Another shortcoming of this fixed threshold is illuminated by our manipulation of target frequency.
With a fixed threshold on what makes a statement true or false, any two points that lie above the threshold are indistinguishable. 
Thus, this model makes the same predictions for different target frequencies in the same event.
For example, the model endorses somebody who hikes three times every two years the same as somebody who hikes three times every month. 


#### Uncertain-threshold RSA model

We constructed the same joint model for the uncertain threshold RSA speaker model and performed the same Bayesian statistical inference over the model to learn about its parameters and predictions.
As shown in Figure \ref{fig:habituals-endorsement-figure}(top right: top right facet), the pragmatic speaker model does a good job of accounting for the variability in responses ($r^2(`r r2.hab.n`) = `r r2.habituals.rsa.uncertain`$; MSE=$`r mse.habituals.rsa.uncertain`$), including actions done on the time scale of years or more  ($r^2(`r r2.hab.rsa.infrequent.n`) = `r r2.habituals.rsa.uncertain.infrequent`$; MSE=$`r mse.habituals.rsa.uncertain.infrequent`$).

### Discussion

In Expt. 2b, we manipulated the event type (e.g., drinks coffee vs. beer) as well as the frequency with which a person has done the action in the past (e.g., three times in the past week vs. year) and measured the corresponding endorsements for habitual statements.
We saw tremendous flexibility in the frequency conditions that leads participants to endorse habituals. 

Parallel to our analysis of generic statements, we articulated a number of alternative models and found that they were unable to explain the variability in endorsements. 
The regression model that uses the distinctiveness of the action (as measured in Expt. 2a) and the frequency of action is the best alternative model to explain endorsements. 
This model is not a predictive model, however; rather, it is a re-description of the data and helps us understand what the *predictive* RSA model is doing. 

Distinctly from our case study on generic language, we manipulated rather than measured the target frequency (e.g., the frequency with which a person drinks coffee).
People are used to learning new information about others, and thus habitual language about people doing actions is particularly amenable to this kind of manipulation. 
By manipulating the target frequency, we have shown that it is causally related to habitual endorsements.
The relationship is not linear (or log-linear), however; habitual endorsements vary in complex ways that reflect listeners' prior knowledge about the event in the question and an underspecified threshold criterion. 

In Experiment 2b, we supplied participants with a statement about how often a person has done the action in the past and asked them to judge the correspoding habitual statement.
This raises an interesting question: Does the propensity communicated by a generalization indicate an objective, past frequency or a subjective, future expectation?
Across two follow-up experiments (Expts. 2c \& 2d), we address this question by manipulating and measuring *predictive frequency* (Expt. 2c) and habitual endorsement (Expt. 2d).

<!-- The speaker model endorses the statement when the observed frequency is relatively high, compared to the prior distribution over people doing the action.  -->

<!-- As another potential alternative hypothesis, we formulate a model of a speaker who judges the felicity of the habitual utterance based on simple summary statistics of the prior, such as the mean and variance. -->

<!-- Only our pragmatic speaker model who reasons about a listener is able to capture the quantitative variability in our data. -->


<!-- \begin{figure*}[t] -->
<!-- \centering -->
<!--   \includegraphics[width=\textwidth]{figs/expt3-4-scatters-camera.pdf} -->
<!--   \caption{Left: Predicted log frequency as a function of past log frequency given to the participant (Expt. 3a; CIs suppressed and jitter added for visual clarity). -->
<!--   Middle: Human endorsements of habitual sentences (Expt. 3b) vs. Predicted log frequency (Expt. 3a), with data for corresponding items from Expt. 2 (assumed to have the same predictive log frequency as baseline).  -->
<!--   Right: Endorsements (Expt. 3b) vs. Speaker $S_2$ model predictions using empirically elicited predictive frequencies (Expt. 3a).} -->
<!--   \label{fig:tj3} -->
<!--   \vspace{-7pt} -->
<!-- \end{figure*} -->


## Experiment 2c: Measuring predictive frequency

While past frequency is often a good indicator of future tendency, people change abruptly due to a variety of decisions and outside events.
Does habitual language communicate propensity in terms of past frequency or future expectations?
On one hand, speakers can only *know* about what has happened in the past.
On the other hand, language has a communicative function, and it would seem useful for speakers to convey what they *believe* will be the case in the future.

In the following pair of experiments, we address this by introducing causal events that enable or prevent future actions (e.g., buying a pack of cigarettes; developing an allergy).
In Expt. 2c, we measure *predictive frequency* both when past frequency alone is observed and when these causal factors are introduced.
In Expt. 2d, we examine endorsements of the habitual sentence (e.g., \emph{John smokes cigarettes.}; \emph{Susan eats peanut butter.}) in the presence of these causal modifiers.
This allows us to test whether habituals are best explained by a speaker $S_1$ who communicates the objective past frequency or the subjective future frequency.


```{r habituals-predictive-elicitation}
d.hab.predictive <- read.csv(paste(
  project.path, "data/habituals/predictive/", "predictive-1-trials.csv", sep = ""))

n.subj.hab.pred <- length(unique(d.hab.predictive$workerid))
ave.seconds.hab.pred <-  d.hab.predictive %>% select(workerid, rt) %>%
      group_by(workerid) %>% summarize(totalSec= sum(rt) / 1000) %>% ungroup() %>% 
      summarize(total = mean(totalSec))


# participant reported this in the comments section (it was recorded as a symbol)
d.hab.predictive[
  (d.hab.predictive$workerid==44 & 
     d.hab.predictive$item=="smokes cigarettes"),"response"] <- 5
d.hab.predictive <- d.hab.predictive %>% mutate(response = as.numeric(as.character((response))))


d.hab.predictive<- d.hab.predictive %>%
  rowwise() %>%
  mutate(
    annualPredictiveRate =
           annualRates[[as.character(past_interval)]]*response,
    annualPastRate =
           annualRates[[as.character(past_interval)]]*past_freq,
    annualPredictiveRate = ifelse(
      annualPredictiveRate == 0, 0.05, 
      annualPredictiveRate),
    logAnnualPredictiveRate = log(annualPredictiveRate),
    logAnnualPastRate = log(annualPastRate)
    )


d.hab.predictive.summary <- d.hab.predictive %>%
  group_by(condition, item, logAnnualPastRate) %>%
  multi_boot_standard(column = "logAnnualPredictiveRate")

fig.hab.freq.predictive.vs.past <- ggplot(d.hab.predictive.summary,
       aes(x=logAnnualPastRate,
                              y = mean,
                              ymin = ci_lower, ymax = ci_upper,
                              fill = condition, shape=condition))+
  geom_jitter(position = position_jitter(width = .13), alpha = 0.6)+
  scale_shape_manual(values=c(21,22,23))+
  geom_abline(intercept = 0, slope = 1, lty =3, color = 'black')+
  # #geom_errorbar(width=0.1)+
  scale_x_continuous(limits = c(-3,6),
                     breaks = c(-3, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+
  scale_y_continuous(limits = c(-3,6),
                     breaks = c(-3, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+  coord_fixed()+
  guides(fill=F, shape=F)+
  scale_fill_solarized()+
  xlab("Past frequency (log scale)")+
  ylab("Predicted frequency (log scale)")+
  theme(axis.text.x = element_text(angle = 90))

```



### Methods

We recruited `r n.subj.hab.pred` participants from MTurk, using the same criterion as Expt. 2b.
The experiment took `r round(ave.seconds.hab.pred/60,1)` minutes on average and participants were compensated \$0.40.

The procedure was identical to Expt. 2b except for the inclusion of a second sentence on a subset of trials and the use of a different dependent measure. 
On all trials, participants were presented with a \emph{past frequency sentence} (same as Expt. 2b).
Additionally, on one third of the trials, participants were presented with a \textbf{preventative sentence} (e.g. \emph{Yesterday, Bill quit smoking.}).
On one third of the trials, participants were presented with an \textbf{enabling sentence} (\emph{Yesterday, Bill bought a pack of cigarettes.}) 
The final third of trials had no additional evidence and were identical to Expt. 2b. 

Only twenty-one of the original thirty-one items were used in order to shorten the experiment.
To increase expected variability, participants saw only the frequencies that led to the most intermediate endorsement of the habitual in Expt. 2b. 
In addition, we did not include separate trials for both male and female names for the select items we did in Expt. 2b, since we saw no differences in their endorsements of the habitual.

Participants were asked ``In the next \textsc{time window}, how many times do you think \textsc{person} does \textsc{event}?'', where the \textsc{time window} was the same as given in the \emph{past frequency statement}.


```{r habituals-predictive-model}
hab.predictive.bda.model <- '
var model = function(){

	var predictiveFrequency = {
			mu: uniformDrift({a: -5, b:10, width: 2}),
			sigma: uniformDrift({a:0, b:10, width: 1})
	}

	mapData({data: data}, function(d){
		observe(Gaussian(predictiveFrequency),
        d.logAnnualPredictiveRate)
	})

  return predictiveFrequency
}
'
```

```{r habituals-predictive-model-run, cache = T, eval=F}
items <- levels(factor(d.hab.predictive$item))
conditions <- levels(factor(d.hab.predictive$condition))
n_samples <- 5000
rs.habituals.bda.predictive <- data.frame()

for (it in items){
  for (cn in conditions){
     df.predictive.toPass <- d.hab.predictive %>%
       filter(condition == cn, item == it)
     
    rs <- webppl(program_code = hab.predictive.bda.model,
       model_var = "model",
       inference_opts = list(method = "MCMC", samples = n_samples,
                             burn = n_samples / 2),
       data = df.predictive.toPass,
       data_var = "data")
    rs.habituals.bda.predictive <- bind_rows(
        rs.habituals.bda.predictive, 
        rs %>% 
            mutate(item = it, condition = cn) 
        )
  }
}
```

```{r eval = F}
  d.hab.predictive %>% 
    mutate(src = 'data') %>% 
    select(item, condition, src, logAnnualPredictiveRate) %>%
ggplot(. , aes( x = logAnnualPredictiveRate, fill = condition))+
#  geom_density(size = 1, aes( y = ..scaled.. ))+
  geom_histogram(aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
               position = position_dodge())+
  facet_wrap(~item, scales = 'free')+
  scale_color_solarized()+
  #scale_x_continuous(limits = c(-0.05,1.05), breaks = c(0, 0.5, 1)) +
  scale_x_continuous(limits = c(-5,10)) +
  theme(strip.text.y = element_text(angle = 0))
```

```{r habituals-predictive-bda-forwardSample, fig.width=14, eval=F}
rs.habituals.bda.predictive.samples <- rs.habituals.bda.predictive %>% 
  spread(Parameter, value) %>%
  rowwise() %>%
  mutate(
    logannualRate = rnorm(n = 1, mean = mu, sd = sigma)
  )

  d.hab.predictive %>% 
    mutate(src = 'data') %>% 
    select(item, condition, src, logAnnualPredictiveRate) %>%
ggplot(. , aes( x = logAnnualPredictiveRate, color = condition))+
#  geom_density(size = 1, aes( y = ..scaled.. ))+
  geom_histogram(aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
               position = position_dodge())+
  facet_wrap(~item, scales = 'free')+
  scale_color_solarized()+
  #scale_x_continuous(limits = c(-0.05,1.05), breaks = c(0, 0.5, 1)) +
  scale_x_continuous(limits = c(-5,10)) +
  theme(strip.text.y = element_text(angle = 0))

bind_rows(
  d.hab.predictive %>% 
    mutate(src = 'data') %>% 
    select(item, condition, src, logAnnualPredictiveRate),
  rs.habituals.bda.predictive.samples %>% 
    select(item, condition, logannualRate) %>%
    rename(logAnnualPredictiveRate = logannualRate) %>%
    mutate(src = 'model') 
  ) %>%
ggplot(. , aes( x = logAnnualPredictiveRate, color = src))+
  geom_density(size = 1, aes( y = ..scaled.. ))+
#  geom_histogram(aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
#               position = position_dodge())+
  facet_grid(condition ~ item )+
  scale_color_solarized()+
  scale_x_continuous(limits = c(-5,10)) +
  #scale_y_continuous(limits = c(-0.01,1.01), breaks = c(0, 0.5, 1)) +
  theme(strip.text.y = element_text(angle = 0))
```

```{r habituals-predictive-bda-summary, eval =F}
rs.habituals.bda.predictive.summary <- rs.habituals.bda.predictive %>% 
  spread(Parameter, value) %>%
  rowwise() %>%
  mutate(
    logannualRate = rnorm(n = 1, mean = mu, sd = sigma)
  ) %>%
  gather(key, val, mu, sigma, logannualRate) %>%
  group_by(item, condition, key) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))
```

```{r habituals-predictive-bda-figure, eval =F}
fig.hab.freq.predictive.vs.past <- left_join(
  rs.habituals.bda.predictive.summary %>% filter(key == "logannualRate"),
  unique(d.hab.predictive %>% 
    select(item, condition, logAnnualPastRate)
  )) %>% ggplot(., 
       aes(x=logAnnualPastRate, y = MAP, 
                              ymin = cred_lower, ymax = cred_upper, 
                              fill = condition, shape=condition))+
  geom_jitter(position = position_jitter(width = .13), alpha = 0.6)+
  scale_shape_manual(values=c(21,22,23))+
  geom_abline(intercept = 0, slope = 1, lty =3, color = 'black')+
  # #geom_errorbar(width=0.1)+
  scale_x_continuous(limits = c(-4,7), 
                     breaks = c(-4, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+
  scale_y_continuous(limits = c(-4,7), 
                     breaks = c(-4, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+  coord_fixed()+
  guides(fill=F, shape=F)+
  scale_fill_solarized()+
  xlab("Past frequency (log scale)")+
  ylab("Predicted frequency MAP (log scale)")+
  theme(axis.text.x = element_text(angle = 90))
```

### Results

<!-- We analyze this data by building a simple Bayesian model of the measurement so that it may be integrated seamlessly into a joint model with the endorsement data (as we have done for all previous case studies). -->
<!-- As we assumed for the prior elicitation task (Expt. 2a), we assume participant's responses for each event $i$ and condition $c$ are generated from a log-normal distribution with unknown mean and variance. -->
<!-- $$ -->
<!-- \ln d_{ic} \sim \text{Gaussian}(\mu_{ic}, \sigma_{ic}) -->
<!-- $$ -->


Figure \ref{fig:habituals-predictive-figure} (left) shows the mean predicted future frequency as a function of the past frequency given to the participant and the type of causal information given. 
We observe in the baseline condition that future frequency perfectly tracks past frequency (e.g., participants believe if a person smoked cigarettes 3 times last month, they will smoke cigarettes 3 times next month). 
This means that our model makes identical predictions for Expt. 2b whether the target is past frequency or expected future frequency (indicating, as expected, that we must look to the new data to distinguish these models).
Critically, we observe the preventative information appreciably decreasing and the enabling information slightly increasing predicted frequency (Figure \ref{fig:habituals-predictive-elicitation} left; blue and green dots).


```{r habituals-predictive-elicitation-predictions}
lmer.rs.hab.predictive <- lmer(data = d.hab.predictive, 
     logAnnualPredictiveRate ~ logAnnualPastRate + condition + 
       (1 + condition | workerid) + 
       (1 + condition | item)
)
lmer.rs.hab.predictive.summary <- summary(lmer.rs.hab.predictive)
```


We confirm these observations using a linear mixed-effects model, predicting the log-transformed responses from the log-transormed past frequency and the experimental condition (baseline, preventative, enabling).
To account for participant and item variability in this analysis, we also include random effects of intercept and condition for both participants and items.
Confirming our predictions, the preventative information led to significantly lower predictions for future frequency, relative to the baseline condition (
$\beta = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionpreventative","Estimate"],2)`$;
$SE = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionpreventative","Std. Error"],2)`;$
$t = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionpreventative","t value"],2)`$
).
There was also tendency for the enabling information to lead to higher predictions for future frequency, relative to baseline (
$\beta = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionenabling","Estimate"],2)`$;
$SE = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionenabling","Std. Error"],2)`;$
$t = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionenabling","t value"],2)`$
).
Finally, past frequency was a significant predictor of predicted future frequency (
$\beta = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["logAnnualPastRate","Estimate"],2)`$;
$SE = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["logAnnualPastRate","Std. Error"],2)`;$
$t = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["logAnnualPastRate","t value"],2)`$
).



## Experiment 2d: Endorsing habituals under predictive-manipulated conditions

```{r habituals-predictive-endorsement}
d.hab.predictive.endorsment <- read.csv(paste(
  project.path, "data/habituals/endorsement/", "habituals-endorsement-predictives.csv", sep = ""))
d.hab.predictive.endorsment.catch <- read.csv(paste(
  project.path, "data/habituals/endorsement/", "habituals-endorsement-predictives-catch_trials.csv", sep = ""))


n.subj.hab.pred.endorse <- length(unique(d.hab.predictive.endorsment$workerid))
ave.seconds.hab.pred.endorse <-  d.hab.predictive.endorsment %>% select(workerid, rt) %>%
      group_by(workerid) %>% summarize(totalSec= sum(rt) / 1000) %>% ungroup() %>% 
      summarize(total = mean(totalSec))

d.hab.predictive.endorsment.condition.summary <- left_join(
  d.hab.predictive.endorsment, 
  d.hab.predictive.endorsment.catch %>% select(workerid, pass),
  by = "workerid"
  ) %>% filter(pass==1) %>%
  mutate(response = ifelse(response=="agree-key", 1, 0)) %>%
  rename(item = habitual) %>%
  group_by(condition) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))

d.hab.predictive.endorsment.baseline.summary <- d.hab.predictive.endorsment.condition.summary %>%
  filter(condition == 'baseline')

d.hab.predictive.endorsment.preventative.summary <- d.hab.predictive.endorsment.condition.summary %>%
  filter(condition == 'preventative')

d.hab.predictive.endorsment.enabling.summary <- d.hab.predictive.endorsment.condition.summary %>%
  filter(condition == 'enabling')

d.hab.predictive.endorsment.summary <- left_join(
  d.hab.predictive.endorsment, 
  d.hab.predictive.endorsment.catch %>% select(workerid, pass),
  by = "workerid"
  ) %>% filter(pass==1) %>%
  mutate(response = ifelse(response=="agree-key", 1, 0),
         time_period = factor(time_period,
                              levels=c("month",
                                       "2 months",
                                       "year",
                                       "2 years",
                                       "5 years")),
         condition = factor(condition,
                            levels=c("enabling", "baseline", "preventative")),
         hab_time = paste(habitual, time_period, sep='-')) %>%
  rename(item = habitual) %>%
  group_by(item, condition, hab_time, time_period) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))

d.hab.predictive.endorsment.summary <- left_join(
  d.hab.predictive.endorsment.summary,
  d.hab.predictive.summary %>%
    rename(logAnnualFutureRate = mean,
         future_lower = ci_lower, future_upper = ci_upper)
  # rs.habituals.bda.predictive.summary %>% 
  #   filter(key == "logannualRate") %>%
  #   rename(logAnnualFutureRate = MAP,
  #        future_lower = cred_lower, future_upper = cred_upper)
  # %>%
  #   rowwise() %>%
  #   mutate(future_lower = ifelse(future_lower < -4, -4, future_lower),
  #          future_upper = ifelse(future_upper > 7, 7, future_upper))
)

r2.habituals.predictiveFreq <- compute_r2(d.hab.predictive.endorsment.summary,
                                          "logAnnualFutureRate", "MAP_h")
mse.habituals.predictiveFreq <- compute_mse(d.hab.predictive.endorsment.summary,
                                          "logAnnualFutureRate", "MAP_h")



fig.hab.endorse.vs.predfreq <- d.hab.predictive.endorsment.summary %>%
  ggplot(., aes(x=logAnnualFutureRate, y=MAP_h, 
                 fill=condition, shape=condition,
                 xmin = future_lower, xmax=future_upper,
                 ymin=low, ymax = high))+
  geom_errorbar(alpha=0.3)+
  geom_errorbarh(alpha=0.3)+
  scale_shape_manual(values=c(21,22,23,24))+
  geom_point(alpha = 0.7)+
  xlab("Predicted frequency (log scale)")+
  ylab("Endorsement probability")+
  scale_x_continuous(limits = c(-4.2,7.2), 
                     breaks = c(-3, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  coord_fixed(ratio=9)+
  scale_fill_solarized()+
  guides(fill = F, shape = F)+
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position="bottom",
    legend.direction="horizontal"
    )

```

```{r habituals-predictive-rsa}
n_chains <- 3
# n_samples <- 500000
# burn <- n_samples / 2
# lg <- 150
model_prefix <- "results-habituals-predictive-jointModel-S1-smtncs_habitual-"
model_prefix <- "results-habituals-predictive-meanPredictive-min-5-jointModel-S1-smtncs_habitual-"
n_samples <- 100000
burn <- n_samples / 2
lg <- 50


m.hab.pred.samp <- data.frame()
m.hab.past.samp <- data.frame()


# for (i in seq(1, n_chains)){
#   mi <- fread(paste(project.path,  "models/habituals/results/",
#                     model_prefix, "targetH_predictive-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.hab.pred.samp <- bind_rows(m.hab.pred.samp, mi %>% mutate(chain = i))

  # mi.past <- fread(paste(project.path,  "models/habituals/results/",
  #                   model_prefix, "targetH_past-",
  #                   n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
  # m.hab.past.samp <- bind_rows(m.hab.past.samp, mi.past %>% mutate(chain = i))

#}
# 
# 
# # 
# save(m.hab.pred.samp,
#      file = paste(project.path,  "models/habituals/results/", model_prefix, "predictive-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))
# # # 
# save(m.hab.past.samp,
#      file = paste(project.path,  "models/habituals/results/", model_prefix, "past-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))


load(file = paste(project.path,  "models/habituals/results/results-habituals-predictive-jointModel-S1-smtncs_habitual-predictive-100000_burn50000_lag50_3chains.RData", sep = ""))


load(file = paste(project.path,  "models/habituals/results/results-habituals-predictive-jointModel-S1-smtncs_habitual-past-100000_burn50000_lag20_3chains.RData", sep = ""))



m.hab.pastmodel.endorsement <- m.hab.past.samp %>%
  filter(type == 'predictive', C == "endorsement") %>%
  rename(item = B, condition = D, binned_freq = E) %>%
  group_by(item, condition) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.hab.predictive.endorsement <- m.hab.pred.samp %>%
  filter(type == 'predictive', C == "endorsement") %>%
  rename(item = B, condition = D, binned_freq = E) %>%
  group_by(item, condition) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.hab.predictive.endorse.rsa <- bind_rows(
  left_join(
    d.hab.predictive.endorsment.summary,
    m.hab.predictive.endorsement
  ) %>%
    mutate(src = "predictive_model"),
  left_join(
    d.hab.predictive.endorsment.summary,
    m.hab.pastmodel.endorsement
  ) %>%
    mutate(src = "past_model")
)

r2.habituals.rsa.past <- compute_r2(
  m.hab.predictive.endorse.rsa %>% filter(src == "past_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.past <- compute_mse(
  m.hab.predictive.endorse.rsa %>% filter(src == "past_model"),
                                          "MAP_h", "MAP")


r2.habituals.rsa.predictive <- compute_r2(
  m.hab.predictive.endorse.rsa %>% filter(src == "predictive_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.predictive <- compute_mse(
  m.hab.predictive.endorse.rsa %>% filter(src == "predictive_model"),
                                          "MAP_h", "MAP")

r2.hab.rsa.predictive.n <- length(d.hab.predictive.endorsment.summary$MAP_h)

habituals.predictive.past.model <- m.hab.predictive.endorse.rsa %>%
  mutate(src = factor(src, levels = c( 
                                      "past_model",
                                      "predictive_model"
                                     ),
                      labels = c(
                                 "Speaker model - past frequency",
                                 "Speaker model - predictive frequency"
                                 
                                 ))) %>%
  filter(src == "Speaker model - past frequency") %>%
  ggplot(., aes ( x = MAP, xmin = cred_lower, xmax = cred_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = condition))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.7)+
  geom_errorbarh(alpha = 0.7)+
  geom_point(shape = 21)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human habitual endorsement")+
  facet_wrap(~src, nrow = 1)+
  guides(fill = F)+
  #theme(legend.position = "bottom")+
    scale_fill_solarized()

habituals.predictive.predictive.model <- m.hab.predictive.endorse.rsa %>%
  mutate(src = factor(src, levels = c( 
                                      "past_model",
                                      "predictive_model"
                                     ),
                      labels = c(
                                 "Speaker model - past frequency",
                                 "Speaker model - predictive frequency"
                                 
                                 ))) %>%
  filter(src == "Speaker model - predictive frequency") %>%
  ggplot(., aes ( x = MAP, xmin = cred_lower, xmax = cred_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = condition))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.7)+
  geom_errorbarh(alpha = 0.7)+
  geom_point(shape = 21)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human habitual endorsement")+
  facet_wrap(~src, nrow = 1)+
  #guides(fill = F)+
  #theme(legend.position = "bottom")+
  scale_fill_solarized()
#habituals.predictive.predictive.model
```

```{r fig.height = 6, fig.width=14, echo = F, eval=F}
m.hab.pred.samp %>%
  filter(type == "predictive", C == "predictiveFreq", E == "sigma") %>%
  #filter(type == "predictive", C == "endorsement") %>%
#  mutate(E = as.numeric(as.character(E))) %>%
  ggplot(., aes(x = val, fill = D))+
  geom_histogram(position = position_dodge())+
  #facet_grid(B~E)+
  facet_grid(D~B)
```

```{r, eval = F}
m.hab.predfreq.summary <- m.hab.pred.samp %>%
  filter(type == "predictive", C == "predictiveFreq") %>%
  group_by(B, D, E) %>%
  summarize(MAP = estimate_mode(val)) %>% spread(E, MAP)

left_join(m.hab.predictive.endorse.rsa %>%
  mutate(src = factor(src, levels = c( 
                                      "past_model",
                                      "predictive_model"
                                     ),
                      labels = c(
                                 "Speaker model - past frequency",
                                 "Speaker model - predictive frequency"
                                 
                                 ))) %>%
  filter(src == "Speaker model - predictive frequency"), m.hab.predfreq.summary %>% rename(item = B, condition = D, freqmean = mean)) %>% View()
```


```{r habituals-predictive-figure, fig.cap="A: Results of Expt 3a. B: Results of Expt. 3b as they relate to the predicted frequencies from Expt. 3a, C & D: Endorsement models based on past frequency (C) and predictive frequency (D).", fig.width = 10.5, fig.height = 11}
legend_b <- get_legend(habituals.predictive.predictive.model + theme(legend.position="bottom"))

# add the legend underneath the row we made earlier. Give it 10% of the height
# of one plot (via rel_heights).

grid.arrange(
  arrangeGrob(
    fig.hab.freq.predictive.vs.past, 
    fig.hab.endorse.vs.predfreq, 
    habituals.predictive.past.model, 
    habituals.predictive.predictive.model + theme(legend.position="none"),
    nrow=2, layout_matrix = cbind(c(1,3), c(2,4))),
legend_b, nrow=2,heights=c(8, 1))


#prow <- plot_grid(fig.hab.freq.predictive.vs.past, fig.hab.endorse.vs.predfreq, habituals.predictive.past.model, habituals.predictive.predictive.model #+ theme(legend.position="none"), labels = c("A", "B", "C", "D"), ncol = 2, align = 'v')
  
#plot_grid( prow, legend_b, ncol = 1, rel_heights = c(5, 0))
```


### Methods

We recruited `r n.subj.hab.pred.endorse` participants from MTurk, using the same criterion as Expt. 2b.
The experiment took `r round(ave.seconds.hab.pred.endorse / 60)` minutes on average on participants were compensated \$0.40 for their work.
None of the participants had completed Expt. 2c.
The only difference from Expt. 2c is the dependent measure. 
On each trial, participants were asked if they agreed or disagreed with the corresponding habitual sentence (as in Expt. 2b).

### Results

There is a clear and consistent negative effect of preventative information on endorsements for the habitual sentence (Figure \ref{fig:habituals-predictive-figure} B; green points).
Still, frequency --- even predictive frequency --- does not perfectly explain the endorsements ($r^2(`r r2.hab.rsa.predictive.n`) = `r r2.habituals.predictiveFreq`$; MSE = $`r mse.habituals.predictiveFreq`$). 

When collapsing across items, the Bayesian Maximum A-Posteriori estimate and 95\% highest probability density interval for the true endorsement probabilities per condition are: baseline = 
`r round(d.hab.predictive.endorsment.baseline.summary[[1,"MAP_h"]], 2)` [`r round(d.hab.predictive.endorsment.baseline.summary[[1,"low"]], 2)`,
`r round(d.hab.predictive.endorsment.baseline.summary[[1,"high"]], 2)`
], enabling = 
`r round(d.hab.predictive.endorsment.enabling.summary[[1,"MAP_h"]], 2)` [`r round(d.hab.predictive.endorsment.enabling.summary[[1,"low"]], 2)`,
`r round(d.hab.predictive.endorsment.enabling.summary[[1,"high"]], 2)`
], preventative = `r round(d.hab.predictive.endorsment.preventative.summary[[1,"MAP_h"]], 2)` [`r round(d.hab.predictive.endorsment.preventative.summary[[1,"low"]], 2)`,
`r round(d.hab.predictive.endorsment.preventative.summary[[1,"high"]], 2)`
]

We use our formal model to test whether past or predictive frequency is what is being communicated by these statements.
To formalize the predictive frequency speaker model, we use the mran predictive frequency data (Expt. 2c) as the $h$ that the speaker model $S$ (Eq. \ref{eq:S1}) is aiming to communicate. 
The past frequency model is constructed using the past frequency supplied to participants as the $h$ that the speaker $S$ model is trying to communicate.

We analyze this model in the same Bayesian data analysis regime as for our previous models.
We use the same priors over the parameters as before, and learn about the posterior distribution by collecting three independent MCMC chains of 100,000 iterations (removing the first 50,000 for burn-in). 
Figure \ref{fig:habituals-predictive-figure} (bottom, left and right) shows the resulting model predictions for the past frequency and the predictive frequency speaker models. 
Participants judgments of the habitual statements was indeed influenced by the causal manipulations in the way predicted by the speaker model using the predictive frequency as input ($r^2(`r r2.hab.rsa.predictive.n`) = `r r2.habituals.rsa.predictive`$; MSE = $`r mse.habituals.rsa.predictive`$).
The model based on past frequency does not make different predictions for the different causal manipulation conditions and does a poor job at explaining the endorsements ($r^2(`r r2.hab.rsa.predictive.n`) = `r r2.habituals.rsa.past`$; MSE = $`r mse.habituals.rsa.past`$). 


<!-- We use the mean predicted log frequency from Expt. 3a as the input to the speaker $S_1$ model to predict the felicity judgments measured in Expt. 3b. -->
<!-- We infer the two model parameters using the same analysis approach in Expt. 2.  -->
<!-- The model matches the data well ($r^2(63) = 0.91$; Figure \ref{fig:tj3}, right). -->
<!-- The same model using the past frequency as the object of communication does not match the data at all ($r^2(63) = 0.02$). -->
<!-- These results suggest that the felicity of habituals is based on an underlying scale of predicted future propensity, not merely the observed frequency in the past. -->

Interestingly, we observe endorsements in this experiment that are appreciably higher than in Expt. 2 for the same items (Figure \ref{fig:tj3}, middle; red vs. purple points).
This may be due, in part, to an effect of the experimental context on participants:
in this experiment the overall population of frequencies is much lower (both because we selected moderate frequencies from Expt. 2 and because of the preventative information) and participants may infer that the experimenter believes this to be a representative range and adjust judgments accordingly.
Future investigation into this issue is warranted.

## Discussion

Habitual language conveys generalizations about events and our model decides if a habitual sentence is a pragmatically useful way to describe the rate at which a person does an action, taking into account the listeners prior beliefs about the action (measured in Expt. 2a). 
We saw that our computational model endorses statements that communicate generalizations about events with the same sensitivity to context and frequency that people exhibit (Expt. 2b).
In Experiment 2b, we varied the type of event and the past frequency with which the person did the action, and found substantial graded variability in the endorsement judgments of people.
By manipulating (rather than measuring) the target frequency with which a person does an action, we showed how alternative models were insufficient to account for this gradience.
In particular, a fixed threshold model that has access to the same prior knowledge as our model and goes through the same information-theoretic decision making procedure as our speaker model is unable to make different predictions for different frequencies.
Only our uncertain threshold model was able to precisely account for the wide range of endorsements.

<!-- Naive baseline models could not explain these endorsements judgments and our communicative model provided the most parsimonious account of the data in comparison to non-Bayesian models.  -->
<!-- In Experiment 3, we looked at interpretations of habitual statements about different events and again found substantial gradedness that our theory provided the most parsimonious account of. -->
<!-- The theory defines a pair of interpretation and endorsement models and these models can simultaneously account for both data sets. -->
In Experiments 2c \& 2d, we further investigated the nature of the underlying propensity scale by introducing enabling and disabling causal evidence, measuring the predicted future frequency (Expt. 2c) and using that, with the model, to predict the felicity of habitual sentences (Expt. 2d). 
To our knowledge, the experiments presented here are first empirical investigations into the truth conditions of habitual sentences and the first test of a formal psychological model of such generalizations in language.

In these experiments, we directly manipulated the frequency that the speaker was trying to communicate while still measuring the priors over the propensity of the event.
Thus, it is still possible that the propensity priors that are central to our communicative theory of generalizations are somehow epiphenomenal, correlationally associated with generic endorsement but not directly causally related.
In our final case study, we extend our theory to communicating generlizations about causes, and experimentally test whether the propensity prior is indeed causally related to generic endorsement. 

# Case Study 3: Causal Language

Learning that an event consistently tends to result in another event serves as a foundation for an intuitive theory of causality [@Goodman2011; @Kemp2010a].
Communicating generalizations about causally related events is potentially highly valuable because relevant data can be prohibitively expensive and no one individual can run all the experiments that she wants to run in a life time. 
The history of civilizations has shown that it is incredibly useful to divide labor: Some members can investigate modifications to the food preparation procedures while others can examine different ways of cutting stone. 

Our framework for communicating generalizations makes predictions about statements conveying genericity about a category of causal events. 
In this domain, an instance of the category is a single causal event, or *token causality* (e.g., adding a particular amount of a particular kind of yeast to an actual piece of dough). 
<!-- [Gerstenberg] -->
Ascribing causality to a singular pair of conjoined events is itself a challenging problem [e.g., @Gerstenberg2015how]. 
Inducing the existence of an abstract cause from observational evidence also depends in complex ways on an observer's background theory of causality [e.g., @Griffiths2005; @Schulz2008a].

Language provides a way to bypass the observational data needed to make a causal induction.
Indeed, causal language has been shown to faciliate causal learning for nonobvious causal relations in 2-year-olds [@Bonawitz2010].

In this paper, we posit that prevalence priors are a mediating representation between abstract conceptual structure and generalizations in language.
In this last set of experiments, we test the causal relationship between the prevalence priors and endorsements of generalizations about causes.
Also, by designing the experiments in the domain of causal language, we further demonstrate the generality of our theory of communicating generalizations.

In Experiment 3a, we manipulate participants' beliefs about a causal system and measure their updated beliefs, serving a manipulation check.
In Experiment 3b, we test whether these manipulated priors give rise to different endorsements of causal statements.

## Experiment 3a: Manipulating the Prevalence Prior for Causes

In this experiment, we introduce participants to statistics about novel causal forces and have them make predictions about future instances of those causes.
This prediction task serves as a manipulation check for our prevalence prior manipulation.

<!-- examine causal domains that vary as to whether or not probabilistic causes are plausible and whether or not a background cause of the event is intuitively present. -->
<!-- We introduce participants to the results of several experiments about each domain and see how different experimental results update participants' beliefs about causal power in these domains. -->

<!-- People have theories about how different casusal systems work.  -->
<!-- Physical causation tends to have a deterministic bend to it: If one billiard ball hits a second with a certain force, the second will respond by moving with a certain force.  -->
<!-- If the experiment is repeated exactly, we would expect the same results. -->
<!-- In other domains, probabilistic causes are plausible: Giving a sick animal a certain medication may sometimes make it better and other times it won't.  -->
<!-- Finally, some events could have multiple possible causes: Testing whether or not a drug makes an animal *blink* might be difficult because animals will tend to *blink* even without intervention.  -->



### Method

```{r causal-priors-data}
d.cas.priors <- read.csv(paste(project.path,
                           "data/causals/priors/pilot-causals-7-prior-trials.csv",
                           sep= ""))

n.subj.cas <- length(unique(d.cas.priors$workerid))

ave.minutes.cas <- round(mean(unique(d.cas.priors %>% select(workerid, rt))$rt) / 1000 / 60, 1)
```

```{r causal-priors-runModel, cache = T}
causal.distributions <- levels(factor(d.cas.priors$distribution))
p <- causal.distributions[1]
m.cas.marginal.prevalence <- data.frame()
m.cas.endorse.priors.summary <- data.frame()

for (p in causal.distributions){
  priorData <- 100*filter(d.cas.priors, distribution == p)$response
  
  m.cas.endorse.priors <- webppl(genericEndorsementPriorModel, data = priorData, data_var = "data",
         model_var = "model", inference_opts = list(method = "MCMC", samples = 5000, burn = 2500, verbose = T))
  
  m.cas.marginal.prevalence <- bind_rows(m.cas.marginal.prevalence, m.cas.endorse.priors %>% 
    spread(Parameter, value) %>%
    rowwise() %>%
    mutate(
      a = g * d,
      b = (1 - g) * d,
      stable = rbinom(n = 1, size = 1, prob = phi),
      prevalence = ifelse(stable == 1, 
                             rbeta(n = 1, shape1 = a, shape2 = b),
                             rbeta(n = 1, shape1 = 1, shape2 = 100)),
      distribution = p
    )
  )
  
  m.cas.endorse.priors.summary <- bind_rows(m.cas.endorse.priors.summary,
                                            m.cas.endorse.priors %>% 
                                              mutate(distribution = p) %>%
                                              group_by(distribution, Parameter) %>%
                                              summarize(MAP = estimate_mode(value),
                                                        cred_upper = hdi_upper(value),
                                                        cred_lower = hdi_lower(value))
                                              )
}

```



#### Participants 

We recruited `r n.subj.cas` participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average `r ave.minutes.cas` minutes and participants were compensated \$0.50 for their work.


<!-- The different kinds of system are described in more detail below. -->
<!-- Participants are then told that the team of scientists is conducting experiments with different possible causes (e.g., different kinds of foods). -->


#### Procedure

The experiment is a single-trial experiment with 2 phases to the trial.
In the first phase, participants are given a story about a causal domain.
In the second phase, participants are asked to make a judgment corresponding to the between-subjects condition they are assigned (prior, endorsement, interpretation). 

In the first phase, participants are told that they are an astronaut-scientist on a distant planet trying to figure out how some system works (how to make a certain kind of animal sleepy with different herbs or how to make a plant grow tall with different fertilizers). The story for the "sleepy animals" condition read:

\begin{quotation}
You are an astronaut-scientist exploring a distant planet. 
On this planet, there are animals called cheebas and your team of scientists wants to figure out how to make these animals sleepy. 
Your team runs experiments trying to make cheebas sleepy with different naturally occurring herbs.
The results are shown below:
\end{quotation}

Participants then must click a button to show the results of the experiments.
Experiment results appear one at a time (upon a click), and are described verbally (e.g., *Your team gave herb A to 100 different cheebas. Of those 100 treated, 98 cheebas were made sleepy.*) as well as displayed in a table showing the number of successes per number of attempts (always 100 per experiment). 
Eleven "experiments" are shown, though one of the experiments had their results misplaced so they cannot report on the results at that point (the significance of this "missing experiment" will be described in Experiment 4b).
After participants view the results of the 10 "experiments" (and 1 missing experiment), they are told to review the results of the experiments before continuing. 

Upon proceeding to phase 2, the table of "experiment results" is removed and participants are told more experiments were conducted.
Participants were asked to predict the results of the next 5 experiments.
Participants were given 5 slider bars ranging from 0 - 100, and asked to predict the next four treatments (e.g., herbs M, N, P, Q, and R). 

#### Materials

Participants were either told a story of trying to make animals sleepy (described above) or trying to make plants grow tall. 
The cover story for the "tall plants" condition read:

\begin{quotation}
You are an astronaut-scientist exploring a distant planet. 
On this planet, there is a plant called feps and your team wants to figure out how to make these plants grow tall. 
Your team runs experiments trying to make feps grow tall with different fertilizers. 
The results are shown below:
\end{quotation}

Participants with either cover story were then exposed to one of four distributions of "experiment results", corresponding to our four experimental conditions, shown in Figure \ref{fig:causals-conditions}. 
In two of the conditions, participants saw results that were all approximately the same across the various experiments. 
In one of these conditions, all causes produced a strong effect (average efficacy approximately 98\%; the "common deterministic" condition).
In the second of these conditions, all causes produced a weak effect (average efficacy approximately 20\%; the "common weak" condition).

The two other conditions used distributions in which some experiments resulted in either no or very few succeses (i.e., produced 0s or 1s or 2s), and others that either had strong or weak effects as above. 
These are the "rare deterministic" and "rare weak" conditions, because the very existence of a greater than zero effect was "rare".

```{r figure-causals-priors, fig.caption = "Four background distributions participants were exposed to in Expt. 4 (stimuli) and priors reconstructed from participants' responses (model). ", fig.width = 8, fig.height = 3, eval = F}
sample.dists <- bind_rows(
  data.frame(
    dist = "rare weak",
    x = c(rbeta(n = 3000, shape1 = 2, shape2 = 10),
          rbeta(n = 7000, shape1 = 2, shape2 = 50)
          )
  ),
  data.frame(
    dist = "common weak",
    x = rbeta(n = 10000, shape1 = 2, shape2 = 10)
  ),
  data.frame(
    dist = "rare deterministic",
    x = c(rbeta(n = 3000, shape1 = 50, shape2 = 2),
          rbeta(n = 7000, shape1 = 2, shape2 = 50)
          )
  ),
  data.frame(
    dist = "common deterministic",
    x = rbeta(n = 10000, shape1 = 50, shape2 = 2)
  )
) %>%
    mutate(Distribution = factor(dist, 
                               levels = c("common deterministic", "rare deterministic", "common weak", "rare weak"),
         labels = c("Common Deterministic", "Rare Deterministic",
                    "Common Weak", "Rare Weak")))
  
figure.causals.priors <- bind_rows(
  sample.dists %>%
    mutate(src = "stimuli") %>%
    select(-dist),
  m.cas.marginal.prevalence %>%
    filter(distribution %in% c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak")) %>%
    mutate(Distribution = factor(distribution, 
                                 levels = c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak"),
           labels = c("Common Deterministic", "Rare Deterministic",
                      "Common Weak", "Rare Weak"))) %>%
    select(Distribution, prevalence) %>%
    mutate(src = 'model') %>%
    rename(x = prevalence)
) %>%
  mutate(src = factor(src, levels = c("stimuli", "model"))) %>%
  ggplot(., aes(x = x, fill = Distribution, color = Distribution, alpha = src, lty = src
                ))+ 
  geom_density( adjust = 1.3, 
               size = 1.2,
               aes( y = ..scaled..))+ #
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  #scale_fill_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  scale_fill_solarized()+scale_color_solarized()+
  #scale_color_manual(values = c("#636363", "#d7191c"))+
  scale_alpha_manual(values = c(0.1, 0.4))+
  scale_linetype_manual(values = c(2, 1))+
  ylab("Scaled Prior Probability")+
  xlab("Target probability")+
  facet_wrap(~Distribution, scales = 'free', nrow = 1)+
  guides(fill = F, color = F)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal')
```


```{r s1-model}
s1.model <- '
var probability = function(Dist, x) {
    return Math.exp(Dist.score(x));
}
var targetUtterance = "generic";

var utterancePrior = Infer({model: function(){
  return uniformDraw([targetUtterance,"silence"])
}});

var betaShape = function(params){
  return {
      a: params.g * params.d,
      b : (1-params.g) * params.d
  }
}

var thetaPrior = Infer({model: function(){
 return uniformDraw([
   0.01, 0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
   0.5, 0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95
 ])}
});

var bins = [
  0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
  0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99
];

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=="silence"? true :
         utt=="some"? state>0.01:
         utt=="most"? state> 0.5:
         utt=="all"? state >= 0.99:
         true
}
var alpha = 2;
var mixture = data.prior[0].phi;
var priorParams = betaShape(data.prior[0]);

var statePrior = Infer({model: function(){
  var component = flip(mixture);
  return component ?
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta(priorParams), b) + Number.EPSILON
      }, bins )
    }) :
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta({a:1,b:100}), b) + Number.EPSILON
      }, bins )
    })
}});

var listener0 = cache(function(utterance) {
  Infer({model: function(){
    var state = sample(statePrior)
    var theta = utterance == "generic" ? sample(thetaPrior) : -99
    condition(meaning(utterance, state, theta))
    return state
 }})}, 10000)

var speaker1 = function(state) {
  Infer({model: function(){
    var utterance = sample(utterancePrior);
    var listener_posterior = listener0(utterance);
    factor(alpha * listener_posterior.score(state))
    return utterance
  }})
}

probability(speaker1(data.state[0]), "generic")
'
```

```{r causal-endorsement-predictions, cache = T, eval = F}
causal.priorParams <- m.cas.endorse.priors.summary %>%
    filter(distribution %in% c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak"),
           Parameter != 'marginalPrevalence') %>%
  select(distribution, Parameter, MAP) %>%
  spread(Parameter, MAP)
experimental.frequencies <- c(0.2, 0.7)
causal.endorsement.predictions <- data.frame()
for (p in causal.priorParams$distribution){
  for (s in experimental.frequencies){
    
    inputData = list(prior = filter(causal.priorParams, distribution == p), 
                     state = s)
    
    s1.rs <- webppl(s1.model, data = inputData, data_var = "data")

    causal.endorsement.predictions <- bind_rows(
      causal.endorsement.predictions,
      data.frame(state = s, distribution = p, prob = s1.rs)
    )
  }
  print(p)
}
```


<!-- #### Materials  -->

<!-- The kind of causal events in question were selected to correspond roughly to different positions in a theoretically meaningful space defined by the model. -->
<!-- Research in elemental causal induction suggests people use causal models that correspond to a mixture probability distributions, where one component of the distribution is a consequence to the intrinsic causal force while the other component is a result of some extrinsic background cause. -->
<!-- So we designed materials that corresponded to domains where various parameters of that mixture model could be manipulated.  -->

<!-- A second dimension of man ipulation corresponds to the extent to which *determinism* is expected in the domain.  -->
<!-- For example, people's theories of physical causation tends to be more deterministic (i.e., favoring probabilities 1 and 0) then causation in the social or psychological domain.  -->


### Results

The distributions that resulted from participants predicting the causal efficacy of the new herbs / fertilizers are shown in Figure \ref{fig:figure-causals}.
These distributions nicely recapitulate the distributions supplied in the different experimental conditions, suggesting that the manipulation does indeed change participants' representations of what probabilities are likely to occur in each experimental condition. 
We hypothesize that with the distributions altered, endorsements of generalizations will similarly be affected.

```{r empirical-causal-prior-densities, eval = F, fig.width = 5, fig.height = 3, fig.caption="Empirical densities from the prediction task in the prior manipulation experiment (Expt. 4a)."}

d.cas.priors %>%
  filter(distribution %in% c("common_weak", "rare_deterministic", "rare_weak")) %>%
      mutate(Distribution = factor(distribution, 
                               levels = c("rare_deterministic", "common_weak", "rare_weak"),
         labels = c("Rare Deterministic",
                    "Common Weak", "Rare Weak"))) %>% 
ggplot(., aes(x = response, fill = Distribution, color = Distribution))+ 
  geom_density( adjust = 0.8, alpha =0.8, 
               size = 1.2,
               aes( y = ..scaled..))+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  scale_fill_manual(values = c("#d01c8b", "#f1b6da", "#4dac26", "#b8e186"))+
  scale_color_manual(values = c("#d01c8b", "#f1b6da", "#4dac26", "#b8e186"))+
  ylab("Scaled Prior Probability")+
  xlab("Target probability")+
  facet_wrap(~Distribution, scales = 'free')+
  guides(fill = F, color = F)
```

### Model predictions

... run RSA model using fixed parameters and MAP estimates of these priors ?



## Experiment 3b: Endorsing Causal Language

In this experiment, we tested whether the manipulated priors of Expt. 3a are causally related to the endorsement of causal statements.

### Method

Most of the experiment was identical to that of Experiment 3a.


```{r causal-endorsement-data}
d.caus.endorse.catch.20 <- read.csv(paste(project.path, 
                                       "data/causals/endorsement/",
                                       "causals-8-20-catch_trials.csv", sep = ""))

d.caus.endorse.catch.70 <- read.csv(paste(project.path, 
                                       "data/causals/endorsement/",
                                       "causals-8-70-catch_trials.csv", sep = "")) %>%
  mutate(workerid = workerid + 1 +  max(d.caus.endorse.catch.20$workerid) )

d.caus.endorse.catch <- bind_rows(d.caus.endorse.catch.20, d.caus.endorse.catch.70)

d.caus.endorse.catch <- d.caus.endorse.catch %>% 
  mutate(passBoth = ifelse(pass_numeric + pass_story == 2, 1, 0))

d.caus.endorse.20 <- read.csv(paste(project.path, 
                                "data/causals/endorsement/",
                                "causals-8-20-trials.csv", sep = ""))

d.caus.endorse.70 <- read.csv(paste(project.path, 
                                "data/causals/endorsement/",
                                "causals-8-70-trials.csv", sep = "")) %>%
  mutate(workerid = workerid + 1 + max(d.caus.endorse.20$workerid))

d.caus.endorse <- bind_rows(d.caus.endorse.20, d.caus.endorse.70)


n.subj.cas.endorse <- length(unique(d.caus.endorse$workerid))
ave.minutes.cas.endorse <- round(mean(unique(d.caus.endorse %>% select(workerid, rt))$rt) / 1000 / 60, 1)

n.subj.cas.endorse.failed <- length(filter(d.caus.endorse.catch, passBoth == 0)$workerid)


d.caus.endorse.summary <- left_join(
  d.caus.endorse, 
  d.caus.endorse.catch %>% select(workerid, passBoth)
  ) %>%
  filter(passBoth == 1) %>%
  group_by(distribution, frequency) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))
```

#### Participants

We recruited `r n.subj.cas.endorse` participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
None of the participants had participated in Experiment 3a. 
The experiment consisted of one trial and took on average `r ave.minutes.cas.endorse` minutes; participants were compensated \$0.25 for their work.

#### Procedure and materials

The materials were the same as in Experiment 3a.
Phase 1 of the experiment was identical to that of Experiment 3a.
In phase 2 of the endorsement task, the table of results and background story were removed from the screen and the participant is told that the results of the "lost experiment" were found. 
The results are reported to the participant in terms of how many out of 100 of the attempts were successful. 
Participants saw 1 of 2 reported frequencies: 20\% or 70\% (randomized between-subjects).
Participants were then asked to judge the causal sentence (e.g., "Herb X makes the animals sleepy"). 
Partipants responded by either clicking "Yes" or "No". 

After responding, participants were then asked to recall what the herbs / fertilizers aimed to achieve (e.g., "make animals sleepy") and one of the ten results they saw on the previous page. 
This attention check served to confirm that participants had encoded some part of both relevant aspects of the experiment (the domain and the frequencies). 


```{r causals-endorsement-fig, fig.width = 4, fig.height = 3, eval = F}

ggplot(d.caus.endorse.summary, aes( x = factor(frequency), y = MAP_h, ymin = low, ymax = high, fill = distribution,
                        group = distribution))+
  geom_bar(stat='identity', position = position_dodge(), width = 0.3, color = 'black')+
  geom_errorbar(width = 0.2, position = position_dodge(0.3), color = 'black')+
  theme(axis.text.x = element_text(angle = 0))+
  geom_hline(yintercept = 0.5, lty = 3, color = 'black')+
  scale_y_continuous(limits = c(0, 1), breaks = c(0,0.5, 1))+
  scale_fill_solarized()+
  xlab("Target frequency")+
  ylab("Proportion Causal Endorsement")
```


```{r causals-fullmodel}
n_chains <- 1
n_samples <- 5000
burn <- n_samples / 2
lg <- 5
i <- 1

model_prefix <- "pilot-results-causals-jointModel-S1-"

m.cas.samp <- data.frame()
for (i in seq(1, n_chains)){
  mi <- fread(paste(project.path,  "models/causals/results/", 
                    model_prefix, "smntcs_causal-",
                    n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
  m.cas.samp <- bind_rows(m.cas.samp, mi %>% mutate(chain = i))
}

# n_chains <- 1
# n_samples <- 1000
# burn <- n_samples
# lg <- 5
# i <- 1
# m.hab.fixed.samp <- data.frame()
# for (i in seq(1, n_chains)){
#   mi <- fread(paste(project.path,  "models/habituals/results/", 
#                     model_prefix, "smtncs_some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.samp.i <- mi %>% mutate(chain = i)
#   m.hab.fixed.samp <- bind_rows(m.hab.fixed.samp, m.samp.i)
# }

# m.hab.somemodel.endorsement <- m.hab.fixed.samp %>%
#   filter(type == 'predictive') %>%
#   rename(habitual = B, time_period = D, binned_freq = E) %>%
#   group_by(habitual, time_period, binned_freq) %>%
#   summarize(MAP = estimate_mode(val),
#             cred_upper = hdi_upper(val),
#             cred_lower = hdi_lower(val))

m.cas.fullmodel.endorsement <- m.cas.samp %>%
  filter(type == 'predictive') %>%
  group_by(dist, frequency) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))
```


```{r causal-model-insets, fig.width = 8, fig.height = 3.5, cache = T}
m.cas.fullmodel.prior.parameters <- m.cas.samp %>%
  filter(type == "prior") %>%
  rename(variable = item, parameter = roundedFreq) %>%
  group_by(dist, variable, parameter) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

# use MAP estimates to generate L(h | causal) & L(h | silence) predictions

m.cas.fullmodel.prior.parameters.tidy <- m.cas.fullmodel.prior.parameters %>%
  ungroup() %>%
  select(dist, variable, parameter, MAP) %>%
  mutate(param = paste(variable, parameter, sep = "_")) %>%
  select(-variable, -parameter) %>%
  spread(param, MAP) %>%
  rename(mix = mixture_NA, 
         stable_mean = stableFreq_mean, 
         stable_concentration = stableFreq_sampleSize) %>%
  mutate( a = stable_mean * stable_concentration, 
          b = (1 - stable_mean) * stable_concentration)

cas.listener.predictions <- data.frame()
  
for (p in m.cas.fullmodel.prior.parameters.tidy$dist){
 priorParams <- m.cas.fullmodel.prior.parameters.tidy %>% filter(dist == p) 
 inputData = list(prior = list(params = data.frame(a = priorParams[["a"]],
                                                   b = priorParams[["b"]]),
                               mix = priorParams[["mix"]]), 
                  utt = "generic")
 l0.rs <- webppl(l0.model, data = inputData, data_var = "data")
 cas.listener.predictions <- bind_rows(
   cas.listener.predictions, 
   l0.rs %>% select(Parameter,value) %>% mutate(distribution = p)
   )
}

inset.color.order <- c( "#268bd2", "#dc322f","#d33682","#859900")


causals.endorsement.insets <- cas.listener.predictions %>% 
    mutate(Parameter = factor(Parameter, 
                              levels = c("state_Prior","state_Posterior"),
                            labels = c("Listener Prior (Posterior given Silence)",
                                       "Listener Posterior given Causal")),
    Distribution = factor(distribution, 
                               levels = c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak"),
         labels = c("Common Deterministic", "Rare Deterministic",
                    "Common Weak", "Rare Weak"))) %>%
  ggplot(., aes( x = value, fill = Distribution, color = Distribution, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 4, size = 1)+
  facet_wrap(~Distribution, nrow = 1)+
  scale_fill_manual(values =inset.color.order )+
  scale_color_manual(values = inset.color.order)+
  scale_alpha_manual(values = c(0.1, 0.8))+
  scale_linetype_manual(values = c(2, 1))+
  scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
  scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
  xlab("Prevalence") +
  ylab("Scaled probability density")+
  theme(legend.position = "bottom", legend.title = element_blank())+
  guides(fill = F, color = F)
```

```{r}
md.caus.endorse <- left_join(
  m.cas.fullmodel.endorsement,
    d.caus.endorse.summary %>%
    select(distribution, frequency, MAP_h, low, high) %>%
    rename(dist = distribution)
)

n.caus.endorse <- length(md.caus.endorse$MAP_h)
r2.caus.rsa <- compute_r2(md.caus.endorse, "MAP_h", "MAP")
mse.caus.rsa <- compute_mse(md.caus.endorse, "MAP_h", "MAP")

```



```{r figure-causals-endorsement, fig.width=7, fig.height=3.5, cache = T}
figure.causals.endorsement <- bind_rows(
  m.cas.fullmodel.endorsement %>%
    mutate(src = "model"),
  d.caus.endorse.summary %>%
    select(distribution, frequency, MAP_h, low, high) %>%
    rename(dist = distribution, MAP = MAP_h, cred_lower = low, cred_upper = high) %>%
    mutate(src = 'data')
) %>%
  ungroup() %>%
  mutate(
    src = factor(src, levels = c( "data", "model")),
    dist = factor(dist, 
           levels = c("rare_weak", "common_weak", "rare_deterministic","common_deterministic"),
           labels = c(
                       "Rare Weak", "Common Weak","Rare Deterministic","Common Deterministic"))
    ) %>%
  ggplot(., aes( x = dist, y = MAP, ymin = cred_lower, ymax = cred_upper, fill = dist,
                        group = src, alpha = src))+
  geom_bar(stat='identity', position = position_dodge(), width = 0.3, color = 'black')+
  geom_linerange(position = position_dodge(0.3), alpha = 1)+
  #geom_line(position = position_dodge(0.3))+
  #theme(axis.text.x = element_text(angle = 90))+
  geom_hline(yintercept = 0.5, lty = 2, color = 'black', alpha = 0.4)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  scale_alpha_manual(values = c(1, 0.3))+
  #scale_fill_solarized()+
  scale_fill_manual(values = c("#859900","#d33682","#dc322f", "#268bd2"))+
  xlab("")+
  ylab("Proportion Causal Endorsement")+
  facet_grid(.~frequency)+
  coord_flip()+
  guides(fill = F)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal', legend.title = element_blank())
```

```{r figure-causals, fig.width=11, fig.height=8, cache = T}
grid.arrange(figure.causals.endorsement, causals.endorsement.insets, ncol = 1,
             layout_matrix = cbind(c(1,1,2)))

# ,
#              habituals.endorsement.insets, ncol = 2,
#              layout_matrix = cbind(c(1,1,3), c(2,2,3)))
```

### Results

`r n.subj.cas.endorse.failed` participants were excluded from the analysis for failing to answer both of the attention check questions correctly, leaving a total of `r n.subj.cas.endorse.failed-n.subj.cas.endorse.failed` responses for analysis.
As in our other analyses of endorsement responses, we computed the Bayesian Maximum A-Posteriori (MAP) estimate and 95\% highest probability density interval of the true population probability of endorsing the statement, assuming a uniform prior. 
These are shown for the different experimentally-manipulated priors and frequencies in Figure \ref{fig:causals-endorsement}.

As predicted by our model, endorsements for a causal statement were sensitive to the background distribution of other causes and the causal power of the target cause.
When many other causes produced the effect very reliably (*common deterministic* condition), very few participants endorsed the causal statement for a cause with causal power of 0.2, and were at chance when the causal power was 0.7 (Figure \ref{fig:figure-causals-endorsement; top, blue).
By contrast, when many other causes failed to produce the effect and those that did were not very reliable (*rare weak* condition; green in figure), at least half of participants endorsed the causal statement for a cause with causal power of 0.2, and were at ceiling when the causal power was 0.7.
The other two conditions (*rare deterministic* and *common weak*) led to endorsements intermediate between these two conditions.
Our model predicted these effects again with strong quantitative accuracy ($r^2(`r n.caus.endorse`) = `r r2.caus.rsa`$; MSE = $`r mse.caus.rsa`$).

## Discussion

In our third case study, we applied our same model to generalizations about causal events.
In this domain, we succesfully manipulated participants' beliefs about the expected causal power in a domain (Expt. 3a).
We did this for both unstructured distributions (*common weak*, *common strong*) and structured distributions (*rare weak*, *rare strong*). 
In Expt. 3b, we showed that these manipulated priors influenced endorsements of the corresponding causal statements.
In addition to further demonstrating the generality of this theory, these experiments show that the *prevalence prior* $P(h)$ is causally related to endorsements of generalizations in language.

In these experiments, we used two cover stories that described plausible causal events: herbs making animals sleepy and fertilizers making plants grow tall. 
In addition to their plausibility, these causal events could have ambiguous causal power associated with them (e.g., it's plausible that there are herbs that only weakly make animals sleepy and it's also plausible that there are herbs that almost deterministically make animals sleepy).
These two features of the domains make them particularly amenable to manipulation.
That is, people's abstract theories about these domains are flexible enough to permit such manipulations.

It's likely that there exist domains for which abstract, intuitive theories might interfere with the experimentally-supplied "experimental data" to form a hybrid belief distribution.^[
If this were happening in our domains, we would expect this to show up in the results of Expt. 3a. Participants' predictions about the likely causal power of new causes would be expected to show a mixture of their abstract, intuitive theories and the experimentally supplied data.
]
For example, physical causal systems (e.g., billiard balls hitting each other) could strongly induce near-deterministic notions of causal power, analagous to our "deterministic" priors conditions. 
Causal systems that demonstrate surprising or *a priori* unlikely effects (e.g., liquids melting concrete) could induce rarity about the existence of a non-zero causal power, analgous to our "rare" prior conditions. 
Our theory would predict in these cases that differences in endorsement would be attributable to differences in the belief distribution over causal power. 


# General Discussion

It is a remarkable fact that so much is learned from ideas expressed vaguely in words.
Generalizations in language (e.g., *John runs.*, *Dogs are friendly.*, *The block makes the machine play music*) are a premier example of how simple statements --- statements understood by even the youngest language users --- can display complex sensitivity to context.
We have argued that the core meaning of such linguistic expressions is, in fact, simple, but underspecified. 
To our knowledge, this is the first formal theory of genericity in language that makes precise quantitative predictions about human endorsement data. 

As a formal theory of genericity in language, we aim to unify significant swaths of language that are seemingly quite different from one another: generalizations about categories, events, and causes.
What these expressions have in common is not surface-level features but *genericity*, the fact that these statements convey generalizations. 
Indeed we have been able to capture the context-sensivity of these statements by positing a single underlying scale defined by the *propensity*, or probability (Expt. 1, 2a \& 2b, 3).
This scale is defined by a subjective, predictive probability, not mere frequency (Expts. 2c \& 2d).
We have shown that listeners' prior knowledge about this scale is causally related to their endorsement of statements of genericity (Expt. 3).
Finally, our framework naturally accounts for a number of well-documented examples from linguistics of generalizations about categories, or *generic language* (Expt. 1).
In addition to unifying seemingly disparate parts of language, this paper provides a theoretical framework for asking further questions about genericity, which we outline below.

The theory we present here has a number of desirable features that go beyond alternative accounts.
The first is that our theory is accompanied by a formal model, which makes precise quantitative predictions about endorsments of statements that convey generalizations.
The formal Bayesian model provides a clean separation of the semantics of a generalization from background, world knowledge.
This is critical to understanding the contribution of a generalization in language above and beyond background knowledge.
We believe this has been a stumbling block for formal semantic models that try to argue in one or another way for a statistical or probabilty based approach [@Cohen1999; @Nickel2008].
As @Nickel2016 notes, genericity is particularly difficult to study because it intersects with many other phenomena in language (e.g., gradable adjectives in *Giraffes are tall*).
We argue that it also intersects with world knowledge in subtle ways, and it is critical to accurately measure and describe world knowledge before making claims about the implications of a generalization *per se*.

Our separation of world knowledge from the semantics of generics provides a substantial, but well-circumscribed role for conceptual structure to influence generic production and comprehension.
We do this without having to posit that conceptual structure and semantics are one in the same.
This is important for applied Natural Language Processing research that seeks to draw inferences about particular entities from information conveyed in generalizations in text [e.g., @Herbelot2011].
At the same time, this paves the way for articulating formal theories about conceptual structure and provides a route for influence on generic language [c.f., @Leslie2008].

<!-- We used the most basic tests to validate our model: Production was examined in terms of *endorsement*, which can be seen as a special case of production when the only available alterantive utterances are the assertion or its negation. -->
<!-- Comprehension was examined in terms of *implied prevalence*, which in our theory is most basic inference a hearer can make upon hearing a generalization. -->
<!-- Though these are highly abstracted tests of the model, the modeling framework in principle allows for elaboration to a more complete production and comprehension model.  -->
<!-- The model is a communicative model, which includes a component corresponding to *comprehension* of genericity (i.e., a listener model).  -->

In the rest of this discussion, we discuss some key features of the theory that warrant future investigation and the relationship of our theories to extant theories of genericity.
We conclude by sketching out arguments for other known philosophical puzzles in genericity.

## The Comparison Class and the Question Under Discussion

In this paper, we proposed a model for understanding generalizations in language that relies upon interlocutors' shared beliefs over the statistics of the event or property under discussion.
We constructed the prior belief distribution for the propensity of a property, event, or cause by considering other possible categories having the property, people doing the action, or causes producing the effect.
These other kinds, people, and causes form *comparison classes* against which the target is evaluated. 
$P(h)$ is always relative to a class of other entities or events, a comparison class $C$: $P_C(h)$.

The existence of comparison classes is uncontroversial in the study of *vague language* [e.g., gradable adjectives like *tall* and vague quantifiers like *many*; @Bale2011; @Solt2009].
Adult judgments of the felicity of gradable adjectives like *tall* or *dark* depend upon fine-grained details of the statistics of the comparison class [@Qing2014; @Schmidt2009; @Solt2012].
We have shown similar sensitivity to the statistics of comparison class in the language of generalizations, thus incorporating generic, habitual, and causal language to the study of *vague language*. 

In this work, we made the design decision to construct the comparison class with respect to other kinds, people, or causes.
In this way, the statement of genericity is addressing the implicit Question Under Discussions (QUDs): *who \textsc{does action}?*, *what causes \textsc{effect}?*, or *what \textsc{has feature}?*, respectively.
In the minimal contexts employed in our experiments, this is a reasonable assumption.
There is a different assumption we could have made, however: Another clear way to construct the comparison class, with respect to other features. 
This *feature-wise* comparison class would correspond to addressing the implicit QUDs: *what does \textsc{person} do?*, *what effects does \textsc{cause} bring about?*, and *what features does \textsc{kind} have?*

Statements of genericity can be used to address either QUD, and this difference can even be observed the same sentence.
"Lawyers care about the law." can be interpreted with a category-wise comparison class (i.e., *Lawyers (as opposed to doctors, firefighters, ...) care about the law.*; say, in a pedagogical context) or a feature-wise comparison class (i.e., *Lawyers care about the law (as opposed to justice, ethics, ...)* e.g., said sardonically).
We chose to focus in this work on the category-wise comparison class for methodological convenience. 
In our three case studies, the *category-wise* comparison classes are fixed to *other people*, *other causes*, or *other kinds*, even as we vary the features.
*Feature-wise* comparison classes are likely to be modulated by the feature itself.
Future work should address the property-wise reading of generalizations, and explore what cues lead listeners to one or the other interpretation.


<!-- Throughout our experiments in this work, we have focused on sentences about animals.  -->
<!-- In addition to being the main focus of past theoretical and empirical work, focusing on animals is methodologically convenient as the comparison class for generics about animals is quite naturally \emph{animals}. -->
<!-- When we look beyond generics about animals, deciding what goes into a comparison class becomes less clear. -->
<!-- There are some hints that the comparison class can be derived with respect to the property [@Keil1979], but may involve pragmatic reasoning as well. -->
<!-- For example, the statement "iPhones are useful" could be in comparison to other forms of technology (like a desktop computer), while "iPhones are heavy" could really only be informative relative to other handheld devices. -->

<!-- The incorporation of a comparison class into the study of generic language might help elucidate other puzzles concerning generics. -->
<!-- Recent work in philosophy and linguistics, for instance, suggest generic language is context-sensitive [@Nickel2008; Sterken2015]. -->
<!-- @Nickel2008 points out that \emph{Dobermans have floppy ears} may be true in the context of a discussion of evolutionary biology but that \emph{Dobermans have pointy ears} is true in a discussion of dog breeding. -->
<!-- Our theory provides a hint from where to begin to understand this context sensitivity: the comparison class. -->
<!-- Different conversational contexts could bring to mind different comparison classes, in a way analogous to the context-sensitivity of  gradable adjectives (e.g., \emph{tall}).  -->
<!-- Hearing that "Abigail is tall" means different things is Abigail is 20 years old or if she is 4.  -->
<!-- Future work will be needed to explore whether a pragmatic inference approach is also relevant to establishing the comparison class, and what background knowledge about properties, categories, and context is relevant. -->

## Structured prior knowledge and Implications for Conceptual Structure

<!-- -- add tomasello about learning about how to take turns, as a generalization about an event type which is the event of how the game is supposed to be played -->

Previous psychological and philosophical work on generics has looked beyond prevalence and focused on conceptual distinctions and relations [@Gelman2003; @Prasada2013; @Leslie2007; @Leslie2008]. 
Prasada has argued for a distinction between \emph{characteristic} properties (e.g., \emph{Diapers are absorbent.}) and \emph{statistical} properties (e.g., \emph{Diapers are white.}).
Leslie suggests information that is striking (e.g., \emph{Tigers eat people.}) is useful and thus permitted to be a generic.
Gelman outlines how generics tend to express \emph{essential} qualities that are relatively timeless and enduring. 
Where in the prevalence-based semantics could such conceptual distinctions come into play?

Our approach makes the strong claim that beliefs about predicted prevalence are the connective tissue between conceptual knowledge and the semantics of generic language.
That is, the effect of conceptually meaningful differences on generic language is predicted to be mediated by differences in corresponding prevalence distributions.
We found in Expts. 1 \& 2 that conceptually different kinds of properties and events gave rise to different prevalence priors. 
In Expt. 3 we showed that these prevalence distributions are causally related to endorsement.

It is natural to ask how subjective probabilities might reflect conceptual knowledge.
We found that empirical prevalence distributions are structured in a way that reflects intuitions about causal mechanisms underlying different properties; the differences in shape of these distributions in turn led to variable endorsements of generalizations. 
It is plausible that richer conceptual knowledge also influences these distributions, such as higher-order conceptual knowledge about the nature of properties and categories [@Gelman2003; @Keil1992].
Indeed, it has been argued that conceptual structure in general, including higher-order abstractions, can be captured by probabilisitic causal models [@pearl1988probabilistic; @Gopnik2003theory] and their generalization in probabilistic programs [@Goodmanconcepts].
Future work will be needed to explore whether probabilistic representations of conceptual knowledge can capture the relations identified in other accounts of generics (such as principled, essential, and striking properties), and whether the effect of these relations can then be adequately captured via their impact on subjective prevalence.


## Communicating Predictive Probabilities

It is important to note that our approach is based on \emph{subjective} probability, and not mere frequency.
Indeed, we elucidated in Expts. 2c \& 2d that using participants' \emph{predictions} of probability in our formal model perfectly track generic endorsement, when the present frequency would make the wrong prediction.

The focus on subjective, predictive probability casts new light on puzzles surrounding accidentally-true situations.
Consider a classic thought experiment in the linguistics and philosophy literature: Imagine that currently every Supreme Court Justice's social security number is an even number. 
Linguists predict the statement "Supreme Court Justices have even social security numbers" would still be considered false [@Cohen1999].
Our explanation of this phenomenon is that abstract intuitive theories lead us to reject observed frequencies in forming our subjective probabilities.
That is, because we may believe selection for the Supreme Court is not influenced by one's social security number, we would assign a roughly 50\% subjective probability that the \emph{next} justice would have an even social security number.
Thus, given that the all professions would have roughly the same probability of having employees with even social security numbers, the statement \emph{Supreme Court Justices have even social security numbers} would be similar to *Birds are female*. 
Thus, our model would not endorse the statement \emph{Supreme Court Justices have even social security numbers}, because the predictive prevalence would not be any different for Supreme Court Justices than any other profession.
This perspective makes the intriguing prediction that if we learned much more surprising information, we might be compelled to revise our theory of the domain, update our subjective probability of future instances, and then accept the generic. 
For instance, if every justice in history had \emph{prime numbered} social security numbers (a more suspicious coincidence), one might appeal to a conspiracy, which \emph{would} have predictive consequences.

This paper puts forth the theory that the simplest language of abstractions (i.e., the language of generalizations) communicates predictive probabilities from speaker to hearer. 
This might seem contrary to a basic tenet in cognitive psychology, that human cognition falters when reasoning about probabilities [@Tversky1974].
In a multitude of demonstrations, Kahneman and Tversky observed that human reasoners perform poorly when reasoning about explicit probabilities.
This has been taken to imply that people are bad with probabilties.

We suggest that problems with communicating probabilities are a problem of the language used to convey *explicit* probabilties [cf., @Levinson1995]. 
A host of developmental evidence suggests that even the youngest learners are actually quite good about reasoning about probabilities, but in ways that are not explicit or tied to language [@Gweon2010; @Dewar2010].
In this work, we have argued that probabilities are central to even the most basic linguistic expressions.
We convey probabilities to each other using language, but the most basic ways we have of conveying them are *vague*. 

<!-- One might also question the feature of our computational model that the speaker is explicitly trying to communicate $h$, her subjective probability.  -->
<!-- We note that if the prevalence prior is structured, as we have argued throughout, then the Question Under Discussion that our speaker model is addressing can also be formalized to be "What component of the prior does the target category belong?". This model with a modified QUD makes almost identical quantitative predictions as the simple model without this abstract QUD.  -->

<!-- ### The Problems with Communicating Generalizations -->

<!-- Thus, in order to define a generalization about a category, there must be some corresponding concrete particular instance of that category from which the generalization is formed.  -->
<!-- For example, if an agent observes $n$ instances of \textsc{dog} ($d_1, d_2, ..., d_n$) -->
<!-- For example, if an observer forms a generalization about \textsc{dogs}, she must some way of determining when she is in an instance of the category \textsc{dogs}: She must be able to *individuate*. -->
<!-- In Hume's words, generalizations are predictions about "instances of which we have had no experience resemble those of which we have had experience" [@HumeTHN]. -->

<!-- Generalizations can be made about almost anything. -->
<!-- Here, we restrict our focus to generalizations about categories (whose linguistic expression is known as *generic language*), which have been the primary focus of psychologists, linguists, and philosophers. -->
<!-- We posit that our analysis should expect to generalizations about events (habitual language) as well as generalizations about causal forces (causal language). -->

<!-- Generics express a relation between a kind K (e.g., \textsc{robins}) and a property F (e.g., \textsc{lays eggs}), such that the property can also be said to be applicable of an individual (i.e., the bird in my backyard lays eggs). -->
<!-- Bare plural statements (e.g., \emph{Robins lay eggs}) tend strongly to yield a generic meaning [@Carlson1977], though other forms can express such a meaning sometimes (e.g., \emph{A mongoose eats snakes.}). -->

<!-- Given that generics express a property that can be applied to individuals, it would seem intuitive that the number of individuals with the property would be what makes the statement true or false. -->
<!-- Counter-examples like \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} v. \emph{Birds are female} stifle such intuition.  -->

<!-- ### Statistical Accounts of Generics -->

<!-- Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}.  -->
<!-- Of course, birds do not in general lay eggs (it's only the adult, female ones that do). -->
<!-- The primary way of dealing with such issues is to posit domain restrictions ("implicitly, we are only talking about the females") when there are "salient partitions" [@Carlson1995]. -->
<!-- The most fully-developed theory on this front is due to @Cohen1999.  -->
<!-- Let's first introduce some notation: -->

<!-- ### Conceptual Accounts of Generics -->



## Acquiring the Language of Generalizations

The linguistic outlet for generalizations about categories---so called, *generic language*---has received tremendous attention from psychologists, linguists, and philosophers.
Generic language is one of the earliest emerging forms of complex, compositional language.
Somewhere between 2 and 3 years of age, children recognize that generics convey a generalization about a category, not directly tied to concrete instances in a scene [@Cimpian2008].
Generics are ubiqiutous in child-directed and child-produced speech [@Gelman1998; @GelmanEtAl2004] and are believed to be central to the growth of conceptual knowledge [@Gelman2004], 

What is perhaps surprising from a formal perspective is that generic language is far from trivial to characterize. 
If generics convey something about the prevalence of feature, we would expect them to be in some way comparable to quantifier statments (e.g., "some", "most", "all", ...).
The extreme flexibility of generic meaning (e.g., *Birds lay eggs* vs. *Birds are female*) stands in stark contrast to its early emergence in development.
In fact, quantified statements, whose formal meaning is much more straight-forward (e.g., "Some" means more than zero), emerge much *later* in development [@Brandone2014; @Gelman2015].
This has led some to conclude that the normal tools for describing the semantics of quantified utterances (i.e., a truth-functional threshold) is not approrpriate for generic language [@Leslie2008].

We think this would be throwing out the baby with the bathwater.
When we consider the acquisition problem, there are two aspects of meaning that a language learner must acquire for the semantics of a quantified utterance (e.g., "some", "most"): (1) that the meaning is a threshold-function (i.e., $\denote{u}(h, \theta) = \{h :h > \theta\}$ and (2) the fixed value of the threshold (e.g., $\theta = 0$ for "some", $\theta = 0.5$ for "most").
Upon learning the meaning function is a threshold-function (1), a Bayesian language learner would be well-suited to adopt a prior distribution over possible thresholds $\theta$.
This prior could then be updated with more data to acquire its fixed meaning.
For the generic statement, the language learner is done once she acquires (1). 
The rest of the meaning is inferred in context.
With that lens, generic meaning would be primary, while quantified utterances would be refinements.

There is an alternative reason why our uncertain threshold model makes the acquisition problem easier than quantifiers.
A uniform prior over the threshold in the truth-functional threshold semantics (as we have assumed in this work) is mathematically equivalent to a *soft semantics* wherein the degree to which the utterance is true is proportional to the probability degree $h$ *itself*: ^[It is equivalent to the probability degree $h$ normalized by the prior distribution over the degree (i.e., the comparison class)]
$$
\int_{0}^{1} \delta_{h > \theta} \diff \theta =  h
$$

This *soft semantics* (corresponding intuitively to something like "the higher the probability, the better the utterance", or just "more is better") is perhaps the simplest semantics one could imagine.
<!-- Given this mathematical equivalence, we can imagine the *more is better* soft semantics is what is acquired early. -->
The difficulty in acquiring the meaning of quantifiers is then a difficulty in recognizing a fixed-threshold semantics as a special case of this *more is better* semantics.
This special case involves learning the value of the threshold (aspect 2) and not in recognizing that the utterance conveys something about the quantity (e.g., aspect 1).

## Relationship to Other Theories

To our knowledge, this is the first formal theory of genericity in language that makes accurate and precise quantitative predictions about human behavioral data. 
A number of other theories of genericity have been proposed in the linguistics and psychological literatures. 
By and large, these theories are concerned with explaining *generics* or generalizations about categories, so in keeping with the spirit of these authors, we will refer to the object of explanation, without loss of generality, as *generics*.

Semantic theories fall into one of two broad camps: Those that appeal to the statistics of the world (e.g., how many Ks have F) and those that appeal to structured, conceptual representations (e.g., there is something about being K which causes it to F).
Statistical and conceptual theories express the major contrasting views of the truth conditions of generic statements [@Carlson1995essay].^[We use the terms statistical and conceptual to refer to what @Carlson1995essay referred to as "inductive" and "rules and regulations" views, respectively.]

### Statistical Accounts

Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}. 
Our probabilistic account has antecedents in @Cohen1999 's theory of generics as a frequency adverb (e.g., "generally").
Similar to our formulation, Cohen looks to *prevalence* as a metric against which generics get evaluated.
For Cohen, there are two kinds of generic statements: "Absolute" and "Relative".
"Absolute generics" have a meaning with a fixed threshold on prevalence and that threshold is taken to be 0.5: $P(F\mid K)>0.5$. 
Roughly speaking, "Dogs have four legs" is true because a given dog is more likely than not to have four legs. 
"Relative generics" on the other hand depend upon an alternative set of kinds (his notation: $Alt(K)$), similar to our comparison class.
These statements (e.g., *Mosquitos carry malaria*) are true if an arbitrary member of the target kind is more likely than an arbitrary member of an alternative kind to have the feature.

There are two main points of deviation of our theory from Cohen's.
First, though his theory is framed in terms of probabilities, it still relies upon a fixed threshold and only makes predictions about what is "true" or "false".
Our theory posits that the meaning (i.e., threshold) is underspecified in the semantics and is inferred in context. 
It is a probabilistic model and makes predictions about what is strongly endorsed, what is strongly rejected, and all of the shades in between.
Second, Cohen draws the distinction between "Absolute" and "Relative" generics, providing different mechanisms to explain each; our underspecified threshold is able to handle both kinds of examples with the same basic machinery.

It should be noted that another mechanism used in Cohen's theory is to contextually restrict the entities that go into the computation of prevalence (i.e., which robins do we look at to compute the prevalence of *laying eggs* among *robins*?).
These entities are restricted to those that *could have some feature* in a (contextually-specified) alternative set of features (so called, $Alt(F)$). 
For example, the property \emph{lays eggs} could induce a set of alternatives that all have to do with procreation (e.g., \emph{gives birth to live young}, \emph{undergoes mitosis}, ...).
That is, *Robins lay eggs* can be evaluated as an "absolute generic" because the only individuals under consideration are the female members of kinds as only the female members can plausibly satisfy one or another of the alternatives having to do with procreation (i.e., the features in $Alt(F)$).
<!-- Of course, the inferential machinery behind this sort of domain restriction (i.e., what is $Alt(F)$?) requires further theorizing to explain.  -->
The inferential machinery behind this sort of domain restriction (i.e., what is $Alt(F)$?) relies upon conceptual information, but the details remain obscure [@Carlson1995essay].
^[However, see @Cohen2004 for a discussion of how his semantic constraints relate to different kinds of generics and different kinds of conceptual representational frameworks found in cognitive science.]
Because the mechanisms behind domain restriction plausibly require yet another pragmatic inference relying heavily on world knowledge, the theory we present here doesn't explicitly rely upon domain restriction.
However, it's an open question whether there exists a reformulation of the uncertain threshold model to a fixed threshold where the listener has uncertainty about the relevant domain restriction. 

In the computational linguistics literature, a similar proposal about underspecification of generics has been made [@Herbelot2011].
In their model, generics express a quantified relation, specifically either "Some", "Most", or "All".
The task of the computational linguist, then, is to construct a set of features that accurately predicts (relative to human judgments) the quantified relationship expressed by the generic. 
Though similar in spirit, our theory does not posit that quantified relations corresponding to the words "Some", "Most", or "All" are in any way primitive; rather, we use a continuous distribution over possible thresholds to represent the inherenent vagueness in quantificational meaning in generics.
Additionally, by using a vague, underspecified threshold, our formulation naturally extends to habitual language, where quantified relations like "Most" or "All" are not directly applicable. 

Our theory bears some conceptual similarity to a recent proposal in philosophy by @Sterken2015. 
Sterken treats the generic as a kind of *indexical* (e.g., "this" or "I").
She argues that kind of truth-conditional variability exhibited by generics is best accounted for by generics as indexicals where both domain restriction (described above) and "quantificational force" vary with context.
We think our uncertain threshold model is a way of formalizing Sterken's "quantificational force", though we remain agnostic as to relationship between generics and indexicals. 
As noted above, we have not yet formalized uncertainty about domain restriction but think this is a promising avenue for future research.

Other theorists under the quantificational or statistical banner draw on the intuition that generics seem to express something about what is *normal* in the world [@Asher1995; @Pelletier1997; @Nickel2008].
For example, it is a regrettable but true fact that due to unfortunate circumstances, not all dogs have four legs.
However, we might want to hold onto the idea that were the world to work *normally*, then *all* dogs would have four legs. 
This idea has intuitive appeal for rejecting accidentally true generalizations (e.g., *Supreme Court justices have even social security numbers*; described above).
It also may underly what is problematic about stereotyped language (e.g., *Boys are good at math*).
Though we do not directly address "what is normal" with our theory, we argue that a speaker's beliefs about what is *probable* (which may relate to what is *normal*) plays a role in endorsing and interpreting generalizations.
<!-- is central to the formulation of our theory of generics in a Bayesian cognitive model. -->
Indeed, we have shown in Expt. 3 that speaker's beliefs about what is likely to be the case in the near future matters for endorsing generalizations above and beyond what is currently true in the world.
Prevalence or propensity in our theory is a posterior predictive probability, which incorporates background knowledge in order to make predictions about the future. 
The intersection of what is "normal" and an agent's predictive probability is another target for future research [though see @Icard2017 for some recent progress].

### Conceptual Accounts

Conceptual accounts of generics emphasize the structure of generic knowledge [@Prasada2000], and view generic utterances as the way of expressing special mental relationships between kinds and properties [@Leslie2008; @Prasada2012].
This perspective draws intuition from the fact that generics can express rich relationships between kinds and properties.
Examples like *Lions roar.*  and *Bishops move diagonally.* seem to express something beyond a statistical relationship.
If a lion lacked the capacity to roar, we might consider a less good example of being a lion; 
if you start moving the bishop other ways besides diagonally, you cease to be playing chess.
\emph{Bishops move diagonally} because those are the rules of the game, not because they \emph{tend to} move diagonally.

Perhaps the most influential conceptual account of generics is credited to @Leslie2007.
For Leslie, generics are tied to the cognitive system's "default mode of generalization".
The intuition comes from the fact that infants seem ready to make strong generalizations from small amounts of data [e.g., @Baldwin1993].
This "default mode" comes equipped with the ability to single-out \emph{striking properties} (e.g., properties which are dangerous or appalling) as particularly useful aspects of the world to know about.
Hence, \emph{Mosquitos carry malaria} is true because carrying malaria will kill you, which is a striking and a useful bit of information to convey.

This theory thus predicted that generics that convey striking properties would be endorsed even when they are relatively rare (e.g., *Sharks attack swimmers*).
This prediction was borne out experimentally when participants are supplied information about the dangerousness of a feature, even when the actual prevalence is quite low [e.g., "30\% of lorches have purple feathers that are as sharp as needles and can easily get lodged in you, causing massive bleeding."; @Cimpian2010].
In pilot work, using paradigms similar to the manipulation check paradigm employed in the causal language experiments (Expt. 4a) and the structured elictation used in habituals (Expt. 2a), we have observed that the prior distribution over the prevalence of the feature $P(h)$ also changes when supplied information about the dangerousness of the feature.
That is, our pilot work suggests that experiments that experimentally induce beliefs about the dangerousness of a feature also manipulate the prevalence priors. 
Thus, our model would also predict these effects, but by way of the prevalence prior rather than something special about dangerous features per se.
However, we do not dispute that extra-linguistic factors can play a role in endorsements of generalizations (e.g., a speaker may not *want* a hearer to believe the generalization, which she herself knows to be true, or visa versa; more on this below).

Of course, not all generics convey striking or appalling information (e.g., *Birds lay eggs*).
Leslie's default mode can also distinguish "negative counter-instances" of a property  (e.g., a bird that doesn't lay eggs, like male birds) from "positive counter-instances" (e.g., a hypothetical bird that bears live young).
Generics are much less reasonable when positive counter-instances exist. Hence, \emph{Birds are female} seems weird because "being male" is a positive counter-instance of "being female", but since there are no birds that bear live young,  \emph{Birds lay eggs} is fine.

@Prasada2006 and later @Prasada2013 distinguish between \emph{principled}, \emph{statistical}, and \emph{causal} relations within concepts.
Striking generics (e.g. \emph{Mosquitos carry malaria}) show characteristics of \emph{causal connections} (operationalized using the phrase \emph{There is something about Ks that cause them to F}).
Generics like \emph{Birds lay eggs} (in which only a minority have the property) exhibit *principled* connections (operationalized by endorsement of the phrase \emph{In general, Ks have F}).
The fact that generics about different properties license different kinds of inferences is taken as evidence that the generics themselves represent different kinds of relations.
Statistical information takes a backseat to the conceptual structure.

Generics can arbitrary relationships too: *Barns are red* because most barns are red.
If farmers around the world decided to paint their barns green, they would be no less barns, but barns would be green.
*Ravens are bigger than toasters* is a true statement, but doesn't tell us anything principled or causal about ravens.
These sorts of conceptual accounts must always accomodate completely arbirtary, statistical relations.

In the last fifteen years, there has been tremendous progress in formalizing rich, structural knowledge and the inferences derived from that knowledge [@Tenenbaum2011]. 
For example, @Kemp2008 introduce a probabilistic model for learning different kinds of structural relationships between objects and properties (e.g., taxonomies, social cliques, dominance relations).
@Goodmanconcepts argue that concepts and intuitive theories can be constructed from elementary random primitives using insights from *probabilistic programmming*. 
In this paper, we take a minimalist approach in modeling conceptual structure in our prior on prevalence $P(h)$, and incorporate only what is necessary in our model of world knowledge. 
In Expt. 1a, we discovered that a mixture model is necessary to account for intuitions about the prevalence of properties across different categories.
This minimally structured model reflects basic knowledge about kinds and properties (e.g., that most kinds don't have most properites). 
It's plausible that more richly structured knowledge of the kind described by @Prasada2013 and @Leslie2007 can be incorporating into the general probabilsitic framework that we use here.

The key point divergence between their and our view concerns the core meaning of a generic statement.
For us, the semantics concerns a probability, which could be the byproduct of a conceptual structure, but it is not the conceptual structure itself. 
This provides a more unifying view of generics, since the basic currency of generic meaning is a single metric: probability. 

<!-- It is also the first attempt in psychology to try to unify significant swaths of language that are seemingly quite different from one another: generalizations about events, causes, and categories. -->

<!-- -- Leslie (2008): Default inferences, could be similar to the cognitive model described, but we don't make strong metaphysical commitments (e.g., to "negative" properties) -->

<!-- -- "default generalization" -- very close, the listener is doing a generalization, but not generalization based on evidence, but by reasoning about the threshold -->



<!-- Generic language is the simple and ubiquitous way by which generalizations are conveyed between people. -->
<!-- Yet the dramatic flexibility of generic language has confounded psychologists, linguists and philosophers who have tried to articulate what exactly generic statements mean.  -->
<!-- We evaluated a theory of generic language derived from general principles of language understanding using a simple, but uncertain, basic meaning---a threshold on property prevalence. -->
<!-- Our formal model is a minimal extension of the RSA theory of language understanding, together with an underspecified threshold semantics. -->
<!-- The model was able to explain two major puzzles of generics: their extreme flexibility in truth conditions and the contrastingly strong interpretation of many novel generics. -->
<!-- Both of these phenomena were revealed to depend in systematic ways on prior knowledge about properties. -->
<!-- This prior knowledge was revealed through Bayesian model analysis to be structured, providing a promising bridge to conceptual accounts of generic language. -->
<!-- To understand the nature of the underlying prevalence scale, we showed that generic language is about speakers' expectations of future prevalence, and not necessarily what the current state of the world is like.  -->
<!-- Across all experiments, the formal model predicted the quantitative details of participants' judgments with high accuracy. -->

<!-- There have been numerous demonstrations arguing that statistics (e.g., property prevalence) are insufficient to explain generic meaning [@Gelman2002; @Gelman2007; @Cimpian2010; @Cimpian2010c; @Khemlani2012; @Prasada2013]. -->
<!-- In these experiments, the prevalence considered is only the prevalence of the property for the target category [e.g., the percentage of birds that lay eggs; @Khemlani2012; @Prasada2013], what we have referred to as \emph{within-kind prevalence}.  -->
<!-- Indeed, this simple statistic also fails to explain our data (Figure \ref{fig:commongenerics}b, right). -->
<!-- Our formal model of pragmatics, by contrast, considers not only within-kind prevalence, but a listener's prior beliefs about prevalence across kinds in order to arrive at a meaning for a generic utterance. -->
<!-- By establishing the validity of a semantics based on prevalence alone, we provide a formalism to learn about categories from generic statements.  -->
<!-- Further, since prevalence is a probability, our model can take information conveyed with a generic and be naturally extended to make predictions about entities in the world or support explanations of events or behavior. -->

<!-- ## Generic Identification -->

<!-- Throughout this paper we treated the bare plural construction as a generic utterance with a threshold semantics: $\denote{\text{K F}}(x, \theta)=x>\theta$. -->
<!-- The bare plural construction can also indicate a specific plural predication. -->
<!-- For example, "Dogs are on my lawn" picks out a specific group of dogs, while "Dogs have fur"  does not [@Carlson1977]. -->
<!-- The problem of \emph{identifying} a generic meaning from a bare plural construction is itself a challenging problem because generic meaning can be signaled using a diverse array of morphosyntactic cues. -->

<!-- @Declerck1991 suggests that generic and non-generic bare plurals can be treated in the same way, and that pragmatic considerations alone may resolve interpretative differences.  -->
<!-- Indeed it does appear that knowledge of the properties under discussion (e.g., the state of being on a front lawn; the state of having fur) could facilitate the generic identification process. -->
<!-- Other pragmatic factors, like knowledge of the identity of the speaker (e.g., a teacher vs. a veterinarian), can also disambiguate generic and non-generic meaning [@Cimpian2008]. -->
<!-- Recent work suggests that utterances that fail to refer to specific entities or events could pragmatically imply generic meaning [@Crone2016cogsci]. -->
<!-- Incorporating these insights about generic identification into an information-theoretic, communicative perspective is a natural extension of this work. -->

## Other philosophical puzzles

In our experiments, we have tried to include major known philosophically puzzling examples of generalizations in language. 
In this section, we consider other previously-discussed troubling examples of generic statements and sketch out arguments rooted in the theoretical framework presented in this paper.

1. Books are paperbacks.
2. Mary handles the mail from Antarctica.
3. Elephants live in Africa and Asia.
4. Women are submissive.

(1) is often used to demonstrate that the property prevalence is not sufficient for a generic to be true.
The vast majority of books that exist are paperbacks; nonetheless *Books are paperbacks* sounds weird.
This is not an isolated example.
The same features can be observed in *Pastas are spaghetti*, *Animals are humans*.
These examples are generated by pairing a superordinate category with a property that describes a strictly subordinate category. 
We posit that the weirdness of these examples is the result of a failure of the comparison class.
<!-- Specifically, the target category K in a generic statement "Ks F" is not expected to be in the comparison class.  -->

Though we have not introduced a theory of how a comparison class is contructed, relevant ideas may be found in @Keil1979.
His notion of *predicability* might relate to the comparison class, insofar as both may be derived with respect the property (e.g., the kinds of categories can be felicitiously predicated by "is paperback" may form a comparison class).
Note that this idea is similar to @Cohen1999's category of "Relative generics", which have truth conditions that are determined by a comparison to an alternative set of Ks (his Alt(K), our comparison class) which is composed of Ks that could satisfy some F in an an alternative set of Fs.
For example, being a paperback can only be true or false (in the predicability sense of @Keil1979) of individual books.
Aggregations of entities that can be predicated by the feature are, without further supporting context, categories of books.
Such categories may include books of different of different genres or eras, among plausibly many other such sub-categories.
From the taxonomic relation of the subordinate categories (e.g., types of books) to the target category (i.e., books), the true prevalence of the feature among the target category must be the mean of the prevalence of all subcategories, assuming an equal partitioning of the prior probability of subcategories.
Thus, if we were to construct a speaker model of the statement "Books are paperbacks", the model would predict a 0.5 endorsement: neither true nor untrue, in the same way as "Birds are female". 

(2) is often used to argue that no actual instances need have the property to make a generalization true. 
In this thought experiment, part (perhaps all) of Mary's job is to handle the mail from Antarctica; to date, however, there has not been any mail from Antarctica [@Cohen1999]. 
Thus, it is not the case that Mary *has handled* any mail from Antarctica before.
This example is an extreme case of the phenomenon highlighted by Expts. 2c \& 2d.
In this pair of experiments, we manipulated predictive probability, while keeping fixed past frequency, and saw the endorsements of habitual statements indeed tracked predictive probability (with the linking function being the RSA speaker model). 
Our general cognitive modeling framework takes the fundamental currency of a generalization in language to be a subjective predictive probability. 
Past frequency and actual prevalence in the world often tracks predictive probabilities, but language users' internal models of how the world works can lead them in certain situations to expect something different.

(3) shows a subtle interaction of generic language with another linguistic device: conjunction. ^[
This example was first introduced, to our knowledge, by @Nickel2008.
]
@Nickel2008 suggests this statement is semantically equivalent to: *Elephants live in Africa and elephants live in Asia*.
This example is challenging for many statistical based approaches, since it cannot be the case that most elephants live in Africa and most elephants live in Asia. In addition, we should not take the statement to imply that there are international elephants (i.e., individual elephants who live in Africa part time and Asia part time); but that is the only possible interpretation if the generic statement is given a fixed, majority based quantificational account (e.g., if the generic means *most*).

<!-- Instead, the statement should be glossed as *some elephants live in Africa* and *some elephants live in Asia*, and perhaps taken collectively, *most elephants live in either Africa or Asia* [@Nickel2008].  -->
<!-- This reading of (3) gives it the same semantic content as two statements: "Elephants live in Africa. Elephants live in Asia." -->
<!-- Our model is able to flexibly accomodate this statement by having uncertainty about the truth-conditional threshold.  -->

With reasonable assumptions about the prior (e.g., that *lives in* is similar to other biological properties from Expt. 1a and assuming international elephants are unlikely *a priori*), upon the hearing "Elephants live in Africa", our model will infer that most elephants live in Africa.
When the model then hears "Elephants live in Asia", the model updates its beliefs to accomodate this second statement, inferring that some live in Africa and other lives in Asia. 
This is possible because our model has uncertainty over the semantic thresholds in the generic claim.
Our model can flexibly accomodate new evidence (e.g., learning that elephants live in Asia) that might otherwise contradict previous statements (e.g., knowing that elephants live in Africa).
A runnable version of the model for this example can be found at [http://forestdb.org/models/generics-conjunction.html](http://forestdb.org/models/generics-conjunction.html).

Finally, (4) is used to argue that even if the property is widespread (or perhaps in a softer sense, as in our model, widespread relative to other kinds or groups), speakers might find it avervise to actually assert the generic.
That is, even if it were the case that, because of a variety of socio-cultural reasons, women tend to be submissive (more so than men, and many other social categories), we might still not want to assert *Women are submissive* [@Haslanger2011]. 
In our Model Simulations section, we showed that the generic often can imply the property is widespread, even with uninformed prior beliefs about the property. 
If the generic conveys the property is in fact more widespread than the speaker herself might believe, this can lead to a distortion of the truth. ^[
Note that this can naturally capture the central phenoman described in @Cimpian2010.
].
The current formulation of the model is relatively ambivalent about the deviations of the listener's beliefs from the speaker's belief (i.e., the speaker doesn't mind that the prevalence that she believes to be true will be exaggerated from the generic statement).
However, if states of the world (or, listener's beliefs about states of the world) correspond to different subjective utilities, speakers who take into the account the subjective utilities of the listener may produce language that deviates from what a pure informational transmission speaker might say.
This kind of extension to the RSA framework has already accounted for a variety of behaviors corresponding to polite speech understanding and production [@Yoon2016; @Yoon2017].
If our speaker model believed that states of the world where the prevalence of the feature is high (e.g., a world where many women were submissive) were undesirable (either in absolute terms or as a belief), then a speaker who considered both the informational content of the utterance and the utility in the states of the world implied by that utterance would too find it reprehensible to make such generic claims. 
We find this possibility intruiging for future work.

In sum, we've tried to argue that our information-theoretic communicative model couched in general cognitive framework can be used to formalize a number of interesting and heretofore puzzling avenues in genericity. 
The precise implementation of these hypotheses we leave for future work.

<!-- -- Books are paperbacks. (failure of the constrast class) -->
<!-- -- Birds lay eggs and are female vs. Elephants live in Asia and Africa -->
<!-- can use pragmatics to resolve if the property is conjunctive or not.  -->
<!-- -- Women are submissive. (Haslanger, 2011; generics + politeness mechanism, some states have lower value than others) -->
<!-- -- Muslims are terrorists (vs. Mosquitos carry malaria) -->
<!-- -- Mary handles the mail from antarctica (predictive propensity; also highlights domain restriction on events) -->
<!-- - Glass breaks when struck. (so-called "disposition", but here: a generalization about the event of a glass being struck [already we are domain restricted]) -->


# Conclusion

It might seem paradoxical that a part of language that is so common in communication and central to learning should be vague. 
Shouldn't speakers and teachers want to express their ideas as crisply as possible?
To the contrary, underspecification can be efficient, given that context can be used to resolve the uncertainty [@Piantadosi2012].
In our work, context takes the form of a listener and speaker's shared beliefs about the property in question. 
By leveraging this common ground, genericity provide a powerful way to communicate and learn generalizations,
which would otherwise be difficult or costly information to learn through direct experience.

The dark side of this flexibility is the potential for miscommunication or deceit: A speaker might assert a generic utterance that he himself would not accept, conveying a too-strong generalization to a na\"{i}ve listener.  
Our model predicts this potential particularly for properties which, when present, are widespread in a category---we showed that biological properties are believed to have this distribution, but many properties of social categories may as well [@Cimpian2011a; @Cimpian2012b; @Rhodes2012].
Disagreements are also predicted when interlocutors fail to share background assumptions, which in framework could be differences in the target prevalence, the prior distributions on prevalence, or the comparison class.
<!-- It's an open question whether or not politically charged statements (e.g., "Black lives matter") can be understood by divergent beliefs about the these constituent factors.  -->
For example, there is considerable disagreement as to whether or not "Humans cause global warming".
Our theory predicts this disagreement may be the result of differences in the estimated \emph{causal power} of humans influencing global warming as well as the causal power of \emph{other forces} (e.g., plate tectonics) on climate change.
This is a promising area for future research.

Categories are inherently unobservable. 
You cannot see the category \textsc{dog}, only some number of instances of it.
Yet we easily talk about these abstractions, conveying hard-won generalizations to each other and down through generations.
The theory presented here gives one explanation of how we do so, providing a computational perspective on how category generalizations are conveyed and how beliefs play a central role in understanding language.


# References 

<!-- \setlength{\parindent}{-0.1in}  -->
<!-- \setlength{\leftskip}{0.125in} -->
<!-- \noindent -->

<div id = 'refs'></div>

```{r child = 'appendix-cueValidity.Rmd'}
```


```{r child = 'appendix-alternativeRSAmodels.Rmd'}
```
